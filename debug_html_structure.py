#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é’ç©ºæ–‡åº«HTMLã®æ§‹é€ èª¿æŸ»
"""

import requests
from bs4 import BeautifulSoup
import re

def analyze_html_structure():
    """HTMLã®æ§‹é€ ã‚’åˆ†æ"""
    print("ğŸ” é’ç©ºæ–‡åº«HTMLã®æ§‹é€ èª¿æŸ»")
    print("=" * 60)
    
    try:
        # ãƒšãƒ¼ã‚¸å–å¾—
        url = "https://www.aozora.gr.jp/index_pages/person_all.html"
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        
        # é’ç©ºæ–‡åº«ã®æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç†ã‚’æ”¹å–„
        if response.encoding is None or response.encoding.lower() in ['iso-8859-1', 'ascii']:
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãƒã‚¤ãƒˆå†…å®¹ã‚’Shift_JISã¨ã—ã¦ãƒ‡ã‚³ãƒ¼ãƒ‰
            try:
                content = response.content.decode('shift_jis')
            except UnicodeDecodeError:
                # Shift_JISã§ãƒ‡ã‚³ãƒ¼ãƒ‰ã§ããªã„å ´åˆã¯EUC-JPã‚’è©¦ã™
                try:
                    content = response.content.decode('euc-jp')
                except UnicodeDecodeError:
                    # ãã‚Œã§ã‚‚ãƒ€ãƒ¡ãªå ´åˆã¯UTF-8ã‚’è©¦ã™
                    content = response.content.decode('utf-8', errors='ignore')
        else:
            content = response.text
        
        soup = BeautifulSoup(content, 'html.parser')
        
        print(f"âœ… ãƒšãƒ¼ã‚¸å–å¾—æˆåŠŸ")
        print(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {response.encoding}")
        print(f"ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚µã‚¤ã‚º: {len(response.text):,} æ–‡å­—")
        
        # åŸºæœ¬æ§‹é€ ã®èª¿æŸ»
        print(f"\nğŸ“‹ åŸºæœ¬æ§‹é€ :")
        print(f"h1ã‚¿ã‚°æ•°: {len(soup.find_all('h1'))}")
        print(f"h2ã‚¿ã‚°æ•°: {len(soup.find_all('h2'))}")
        print(f"h3ã‚¿ã‚°æ•°: {len(soup.find_all('h3'))}")
        print(f"olã‚¿ã‚°æ•°: {len(soup.find_all('ol'))}")
        print(f"liã‚¿ã‚°æ•°: {len(soup.find_all('li'))}")
        
        # h2ã‚¿ã‚°ã®å†…å®¹ç¢ºèª
        h2_tags = soup.find_all('h2')
        print(f"\nğŸ“š h2ã‚¿ã‚°ã®å†…å®¹:")
        for i, h2 in enumerate(h2_tags):
            text = h2.get_text().strip()
            print(f"  {i+1}. '{text}'")
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥ã®æ§‹é€ ç¢ºèª
        print(f"\nğŸ” æœ€åˆã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®è©³ç´°èª¿æŸ»:")
        
        # ã€Œã‚¢ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¢ã™
        for element in soup.find_all(['h2', 'h3']):
            text = element.get_text().strip()
            if text == 'ã‚¢':
                print(f"âœ… ã€Œã‚¢ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ç™ºè¦‹: {element.name}ã‚¿ã‚°")
                
                # æ¬¡ã®è¦ç´ ã‚’èª¿æŸ»
                next_elements = []
                current = element.next_sibling
                count = 0
                while current and count < 10:
                    if hasattr(current, 'name') and current.name:
                        next_elements.append((current.name, current.get_text()[:100] if current.get_text() else ""))
                    current = current.next_sibling
                    count += 1
                
                print(f"æ¬¡ã®è¦ç´ :")
                for name, text in next_elements:
                    print(f"  {name}: {text[:50]}...")
                break
        
        # olè¦ç´ ã®è©³ç´°èª¿æŸ»
        ol_elements = soup.find_all('ol')
        if ol_elements:
            print(f"\nğŸ“ olè¦ç´ ã®è©³ç´°:")
            for i, ol in enumerate(ol_elements[:3]):  # æœ€åˆã®3ã¤ã®ã¿
                li_elements = ol.find_all('li')
                print(f"  ol#{i+1}: {len(li_elements)}å€‹ã®liè¦ç´ ")
                
                # æœ€åˆã®æ•°å€‹ã®liè¦ç´ ã‚’è¡¨ç¤º
                for j, li in enumerate(li_elements[:5]):
                    text = li.get_text().strip()
                    print(f"    li#{j+1}: {text}")
        
        # ä½œå®¶é …ç›®ã®ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        print(f"\nğŸ¯ ä½œå®¶é …ç›®ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ:")
        all_li = soup.find_all('li')
        
        # ã€Œ(å…¬é–‹ä¸­ï¼šæ•°å­—)ã€ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å«ã‚€liè¦ç´ ã‚’æ¢ã™
        import re
        author_pattern = re.compile(r'\(å…¬é–‹ä¸­ï¼š\d+\)')
        author_items = []
        
        for li in all_li:
            text = li.get_text().strip()
            if author_pattern.search(text):
                author_items.append(text)
        
        print(f"ä½œå®¶é …ç›®ã¨æ€ã‚ã‚Œã‚‹liè¦ç´ : {len(author_items)}å€‹")
        
        # æœ€åˆã®10å€‹ã‚’è¡¨ç¤º
        print(f"\nğŸ“‹ ä½œå®¶é …ç›®ã‚µãƒ³ãƒ—ãƒ«:")
        for i, item in enumerate(author_items[:10], 1):
            print(f"  {i:2}. {item}")
        
        # æ­£ã—ã„æ—¥æœ¬èªãƒ‘ã‚¿ãƒ¼ãƒ³ã®è©³ç´°åˆ†æ
        if author_items:
            print(f"\nğŸ” ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ:")
            sample_text = author_items[0] if author_items else ""
            print(f"ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ: {sample_text}")
            
            # ã€Œå…¬é–‹ä¸­ã€ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç¢ºèª
            if 'å…¬é–‹ä¸­ï¼š' in sample_text:
                print(f"âœ… æ­£å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º: 'å…¬é–‹ä¸­ï¼š'")
            
            # æ•°å­—æŠ½å‡ºãƒ†ã‚¹ãƒˆ
            numbers = re.findall(r'\d+', sample_text)
            if numbers:
                print(f"âœ… æ•°å­—ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º: {numbers}")
        
        return author_items
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return []

if __name__ == "__main__":
    author_items = analyze_html_structure()
    print(f"\nğŸ“Š èª¿æŸ»å®Œäº†: {len(author_items)}å€‹ã®ä½œå®¶é …ç›®ã‚’ç™ºè¦‹") 