#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ¤– AIæ–‡è„ˆåˆ¤æ–­å‹Geocodingã‚·ã‚¹ãƒ†ãƒ  (Legacyçµ±åˆç‰ˆ)
æ–‡è„ˆã‚’ç†è§£ã—ã¦åœ°åã®å¦¥å½“æ€§ã¨åº§æ¨™ã‚’é«˜ç²¾åº¦ã§æ¨å®š

Features:
- æ–‡è„ˆåˆ†æã«ã‚ˆã‚‹åœ°å/äººåã®åˆ¤åˆ¥
- æ­´å²çš„æ–‡è„ˆã§ã®åœ°åŸŸç‰¹å®š
- æ›–æ˜§åœ°åã®è§£æ±º
- è¤‡åˆåœ°åã®åˆ†æ
- å®Ÿè¨¼æ¸ˆã¿é«˜ä¿¡é ¼åº¦geocodingï¼ˆ0.92-0.98ï¼‰
"""

import re
import sqlite3
import logging
import openai
import os
import json
import time
import hashlib
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime, timedelta
from dotenv import load_dotenv
from threading import Lock

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

logger = logging.getLogger(__name__)

# APIåˆ¶é™ç®¡ç†ç”¨ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
_api_cache = {}
_api_rate_limiter = {
    'openai': {'last_call': 0, 'min_interval': 1.0},  # 1ç§’é–“éš”
    'google_maps': {'last_call': 0, 'min_interval': 0.1}  # 0.1ç§’é–“éš”
}
_cache_lock = Lock()

def _rate_limit_api(api_name: str, min_interval: float = 1.0):
    """API ãƒ¬ãƒ¼ãƒˆåˆ¶é™ç®¡ç†"""
    current_time = time.time()
    last_call = _api_rate_limiter.get(api_name, {}).get('last_call', 0)
    time_since_last = current_time - last_call
    
    if time_since_last < min_interval:
        sleep_time = min_interval - time_since_last
        logger.info(f"ğŸ•’ {api_name} ãƒ¬ãƒ¼ãƒˆåˆ¶é™: {sleep_time:.2f}ç§’å¾…æ©Ÿ")
        time.sleep(sleep_time)
    
    _api_rate_limiter[api_name]['last_call'] = time.time()

def _get_cache_key(text: str, api_type: str) -> str:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ"""
    content = f"{api_type}:{text}"
    return hashlib.md5(content.encode('utf-8')).hexdigest()

def _load_api_cache() -> Dict:
    """APIã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿"""
    cache_file = "data/api_cache.json"
    try:
        if os.path.exists(cache_file):
            with open(cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
    except Exception as e:
        logger.warning(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    return {}

def _save_api_cache(cache: Dict):
    """APIã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    cache_file = "data/api_cache.json"
    try:
        with _cache_lock:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.warning(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆæœŸåŒ–
_api_cache = _load_api_cache()

@dataclass
class ContextAnalysisResult:
    """æ–‡è„ˆåˆ†æçµæœ"""
    is_place_name: bool  # åœ°åã¨ã—ã¦ä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹
    confidence: float    # ä¿¡é ¼åº¦
    place_type: str     # åœ°åã®ç¨®é¡
    historical_context: str  # æ­´å²çš„æ–‡è„ˆ
    geographic_context: str  # åœ°ç†çš„æ–‡è„ˆ
    reasoning: str      # åˆ¤æ–­ç†ç”±
    suggested_location: Optional[str] = None  # æ¨å®šåœ°åŸŸ

@dataclass
class GeocodingResult:
    """Geocodingçµæœ"""
    place_name: str
    latitude: Optional[float]
    longitude: Optional[float]
    confidence: float
    source: str
    prefecture: Optional[str] = None
    city: Optional[str] = None
    context_analysis: Optional[ContextAnalysisResult] = None
    fallback_used: bool = False

class ContextAwareGeocoder:
    """AIæ–‡è„ˆåˆ¤æ–­å‹Geocodingã‚µãƒ¼ãƒ“ã‚¹ï¼ˆLegacyçµ±åˆç‰ˆï¼‰"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self._init_knowledge_base()
        self._init_openai_client()
        logger.info("ğŸ¤– AIæ–‡è„ˆåˆ¤æ–­å‹Geocodingã‚µãƒ¼ãƒ“ã‚¹ï¼ˆLegacyçµ±åˆç‰ˆï¼‰åˆæœŸåŒ–å®Œäº†")
    
    def _init_openai_client(self):
        """OpenAI APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–"""
        api_key = os.getenv('OPENAI_API_KEY')
        if api_key:
            self.openai_client = openai.OpenAI(api_key=api_key)
            self.openai_enabled = True
            logger.info("âœ… OpenAI APIæ¥ç¶šæº–å‚™å®Œäº†")
        else:
            self.openai_client = None
            self.openai_enabled = False
            logger.warning("âš ï¸ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆChatGPTæ©Ÿèƒ½ç„¡åŠ¹ï¼‰")
    
    def _init_knowledge_base(self):
        """æ–‡è„ˆåˆ¤æ–­ç”¨çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–"""
        
        # æ–‡è„ˆåˆ¤æ–­ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.context_patterns = {
            # åœ°åã‚’ç¤ºå”†ã™ã‚‹æ–‡è„ˆãƒ‘ã‚¿ãƒ¼ãƒ³
            "place_indicators": [
                r"[ã¸ã«]è¡Œ", r"[ã‚’ã«]å‡º", r"[ã«]ä½", r"[ã‚’]é€š", r"[ã‹ã‚‰]æ¥",
                r"[ã«]ç€", r"[ã‚’]è¨ª", r"[ã«]å‘", r"[ã§]ç”Ÿ", r"[ã‚’]ç™º",
                r"è¡—", r"ç”º", r"æ‘", r"é‡Œ", r"å›½", r"çœŒ", r"å¸‚", r"åŒº",
                r"æ»åœ¨", r"æ—…è¡Œ", r"å‚æ‹", r"è¦‹ç‰©", r"è¦³å…‰", r"æ•£æ­©",
                r"å‡ºèº«", r"åœ¨ä½", r"ç§»ä½", r"å¼•è¶Š", r"å¸°éƒ·", r"æ•…éƒ·",
                r"æ™¯è‰²", r"é¢¨æ™¯", r"åæ‰€", r"éºè·¡", r"å¯º", r"ç¥ç¤¾",
                r"é§…", r"æ¸¯", r"æ©‹", r"å·", r"å±±", r"æµ·", r"æ¹–",
                r"ã‹ã‚‰.*ã¾ã§", r"ã‚’çµŒç”±", r"çµŒç”±ã—ã¦", r"é€šé", r"ç«‹ã¡å¯„"
            ],
            
            # äººåã‚’ç¤ºå”†ã™ã‚‹æ–‡è„ˆãƒ‘ã‚¿ãƒ¼ãƒ³
            "person_indicators": [
                r"ã•ã‚“$", r"å›$", r"æ°$", r"å…ˆç”Ÿ$", r"æ§˜$", r"æ®¿$",
                r"ã¯è©±", r"ãŒè¨€", r"ã¨ä¼š", r"ã«è", r"ã¨è©±", r"ã‚’å‘¼",
                r"ã®é¡”", r"ã®æ€§æ ¼", r"ã®å®¶æ—", r"ã®äºº", r"ã¨ã„ã†äºº",
                r"åå‰", r"åå‰ã¯", r"ã¨ã„ã†å", r"å‘¼ã°ã‚Œ", r"å‘¼ã‚“ã§",
                r"æ©Ÿå«Œ", r"æ€’", r"ç¬‘", r"æ³£", r"æ‚²ã—", r"å–œ", r"æ†¤",
                r"ã¯.*æ‰“ã¤", r"ã¯.*å©", r"ã¯.*æ®´", r"ã¯.*æ€’é³´",
                r"ã¯.*è¨€ã£ãŸ", r"ã¯.*æ€ã£ãŸ", r"ã¯.*æ„Ÿã˜ãŸ"
            ],
            
            # æ­´å²çš„æ–‡è„ˆãƒ‘ã‚¿ãƒ¼ãƒ³
            "historical_indicators": [
                r"å›½$", r"è—©$", r"åŸ$", r"å®¿å ´", r"è¡—é“",
                r"å¤ã", r"æ˜”", r"æ±Ÿæˆ¸æ™‚ä»£", r"å¹³å®‰", r"éŒå€‰",
                r"æ™‚ä»£", r"å½“æ™‚", r"æ˜”ã®", r"å¤ã„", r"æ­´å²"
            ]
        }
        
        # æ›–æ˜§åœ°åãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆäººåã®å¯èƒ½æ€§ãŒã‚ã‚‹åœ°åï¼‰
        self.ambiguous_places = {
            "æŸ": {"äººåå¯èƒ½æ€§": 0.8, "åœ°å": "åƒè‘‰çœŒæŸå¸‚"},
            "æ¸…æ°´": {"äººåå¯èƒ½æ€§": 0.7, "åœ°å": "é™å²¡çœŒæ¸…æ°´åŒº"},
            "æœ¬éƒ·": {"åœ°åŸŸæ€§": "æ±äº¬", "åœ°å": "æ±äº¬éƒ½æ–‡äº¬åŒºæœ¬éƒ·"},
            "ç¥ç”°": {"åœ°åŸŸæ€§": "æ±äº¬", "åœ°å": "æ±äº¬éƒ½åƒä»£ç”°åŒºç¥ç”°"},
            "é’å±±": {"äººåå¯èƒ½æ€§": 0.6, "åœ°å": "æ±äº¬éƒ½æ¸¯åŒºé’å±±"},
            "éº»å¸ƒ": {"åœ°åŸŸæ€§": "æ±äº¬", "åœ°å": "æ±äº¬éƒ½æ¸¯åŒºéº»å¸ƒ"},
            "ä¸¡å›½": {"åœ°åŸŸæ€§": "æ±äº¬", "åœ°å": "æ±äº¬éƒ½å¢¨ç”°åŒºä¸¡å›½"},
            "ä¼è¦‹": {"åœ°åŸŸæ€§": "äº¬éƒ½", "åœ°å": "äº¬éƒ½åºœäº¬éƒ½å¸‚ä¼è¦‹åŒº"},
            "åµå±±": {"åœ°åŸŸæ€§": "äº¬éƒ½", "åœ°å": "äº¬éƒ½åºœäº¬éƒ½å¸‚å³äº¬åŒºåµå±±"},
        }
        
        # äººåãƒ»å­¦è€…åãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆé«˜ç²¾åº¦ãƒ•ã‚£ãƒ«ã‚¿ç”¨ï¼‰
        self.known_person_names = {
            # æ¤ç‰©å­¦è€…ãƒ»å­¦è€…
            'æ¾æ‘', 'æ¾æ‘ä»»ä¸‰', 'ç‰§é‡', 'ç‰§é‡å¯Œå¤ªéƒ', 'æ¹¯å·', 'æ¹¯å·ç§€æ¨¹',
            'æœæ¯”å¥ˆ', 'æœæ¯”å¥ˆæ³°å½¦', 'æœ¨æ‘', 'æœ¨æ‘é™½äºŒéƒ', 'åŸ', 'åŸå¯›',
            'æœéƒ¨', 'æœéƒ¨åºƒå¤ªéƒ', 'ä¸­äº•', 'ä¸­äº•çŒ›ä¹‹é€²', 'å°æ³‰', 'å°æ³‰æºä¸€',
            
            # æ–‡è±ªãƒ»ä½œå®¶ï¼ˆä½œå“ä¸­ã«è¨€åŠã•ã‚Œã‚‹å¯èƒ½æ€§ï¼‰
            'å¤ç›®', 'å¤ç›®æ¼±çŸ³', 'èŠ¥å·', 'èŠ¥å·é¾ä¹‹ä»‹', 'å¤ªå®°', 'å¤ªå®°æ²»',
            'å·ç«¯', 'å·ç«¯åº·æˆ', 'ä¸‰å³¶', 'ä¸‰å³¶ç”±ç´€å¤«', 'è°·å´', 'è°·å´æ½¤ä¸€éƒ',
            
            # ä¸€èˆ¬çš„ãªå§“
            'ç”°ä¸­', 'ä½è—¤', 'éˆ´æœ¨', 'é«˜æ©‹', 'æ¸¡è¾º', 'ä¼Šè—¤', 'å±±ç”°',
            'ä¸­æ‘', 'å°æ—', 'åŠ è—¤', 'å‰ç”°', 'å±±æœ¬', 'ä½ã€…æœ¨', 'å±±å£',
            'æ¾æœ¬', 'äº•ä¸Š', 'æœ¨æ‘', 'æ—', 'æ¸…æ°´', 'å±±å´', 'æ± ç”°',
        }
        
        # å­¦è¡“ãƒ»å°‚é–€ç”¨èªãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
        self.academic_terms = {
            # æ¤ç‰©å­¦ç”¨èª
            'èªåŸ', 'èªæº', 'å­¦å', 'åˆ†é¡', 'æ¨™æœ¬', 'è¦³å¯Ÿ', 'è¨˜éŒ²',
            'å›³é‘‘', 'æ¤ç‰©', 'èŠ±', 'è‘‰', 'èŒ', 'æ ¹', 'ç¨®å­', 'æœå®Ÿ',
            'é–‹èŠ±', 'çµå®Ÿ', 'ç™ºèŠ½', 'ç”Ÿè‚²', 'åˆ†å¸ƒ', 'ç”Ÿæ¯', 'è‡ªç”Ÿ',
            
            # ä¸€èˆ¬å­¦è¡“ç”¨èª
            'ç ”ç©¶', 'èª¿æŸ»', 'å®Ÿé¨“', 'è¦³æ¸¬', 'åˆ†æ', 'è€ƒå¯Ÿ', 'è«–æ–‡',
            'å ±å‘Š', 'ç™ºè¡¨', 'è¬›æ¼”', 'ä¼šè­°', 'å­¦ä¼š', 'å”ä¼š', 'å›£ä½“',
            
            # åŒ»å­¦ãƒ»ç§‘å­¦ç”¨èª
            'ç—…åŸ', 'ç—‡çŠ¶', 'æ²»ç™‚', 'è¨ºæ–­', 'åŒ»å­¦', 'è–¬å­¦',
            
            # ç¥è©±ãƒ»å®—æ•™ç”¨èª
            'ç‰§ç¾Šç¥', 'å¤šåˆ†ç¥',
            
            # æŠ½è±¡æ¦‚å¿µãƒ»æˆå¥
            'ç†è«–', 'æ¦‚å¿µ', 'æ€æƒ³', 'å“²å­¦', 'ä¸»ç¾©', 'æ–¹æ³•', 'æŠ€è¡“',
            'é€²é€€ç¶­è°·', 'æœ€ä¸Š',  # æœ€ä¸Šç´šãƒ»æˆå¥ã¨ã—ã¦
        }
        
        # ä¸€èˆ¬åè©ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
        self.general_nouns = {
            # æ™‚é–“ãƒ»çŠ¶æ…‹ãƒ»é‡è©
            'ä»Šæ—¥', 'æ˜¨æ—¥', 'æ˜æ—¥', 'ä»Šæœ', 'å¤•æ–¹', 'æ·±å¤œ', 'æ—©æœ',
            'æ²¢å±±', 'å¤§å‹¢', 'å°‘æ•°', 'å¤šæ•°', 'å…¨éƒ¨', 'ä¸€éƒ¨', 'åŠåˆ†',
            'æ™‚é–“', 'æ™‚æœŸ', 'æœŸé–“', 'ç¬é–“', 'æ°¸ä¹…', 'ä¸€æ™‚',
            
            # å‹•ä½œãƒ»è¡Œç‚º
            'ç§»å‹•', 'åˆ°ç€', 'å‡ºç™º', 'å¸°å®…', 'å¤–å‡º', 'æ•£æ­©', 'æ—…è¡Œ',
            'è¡Œå‹•', 'å‹•ä½œ', 'ä½œæ¥­', 'æ´»å‹•', 'é‹å‹•', 'åŠ´åƒ',
            
            # å»ºç‰©ãƒ»æ–½è¨­ã®ä¸€èˆ¬åç§°
            'ç¾å®¹é™¢', 'ç†é«ªåº—', 'ç—…é™¢', 'å­¦æ ¡', 'å›³æ›¸é¤¨', 'éƒµä¾¿å±€',
            'éŠ€è¡Œ', 'ä¼šç¤¾', 'å·¥å ´', 'åº—èˆ—', 'å•†åº—', 'å¸‚å ´',
            
            # è‡ªç„¶ãƒ»åœ°å½¢ã®ä¸€èˆ¬åç§°
            'å±±é‡', 'ç”°é‡', 'é‡åŸ', 'è‰åŸ', 'è’é‡', 'å¹³é‡',
            'æ°´è¾º', 'å·è¾º', 'æµ·è¾º', 'å²¸è¾º', 'æ¹–ç•”', 'æ± ç•”',
            'æ£®æ—', 'ç«¹æ—', 'æ¾æ—', 'é›‘æœ¨æ—',
            
            # æŠ½è±¡çš„æ¦‚å¿µ
            'å•é¡Œ', 'èª²é¡Œ', 'å›°é›£', 'éšœå®³', 'åŸå› ', 'çµæœ', 'å½±éŸ¿',
            'ç›®çš„', 'æ‰‹æ®µ', 'æ–¹æ³•', 'æ–¹å¼', 'æ–¹å‘', 'çŠ¶æ³', 'çŠ¶æ…‹',
            'æ€§è³ª', 'ç‰¹å¾´', 'æ€§æ ¼', 'å€‹æ€§', 'äººæ ¼', 'å“æ ¼',
            
            # æ„Ÿæƒ…ãƒ»å¿ƒç†
            'æ°—æŒ', 'æ„Ÿæƒ…', 'å¿ƒæƒ…', 'æ°—åˆ†', 'å¿ƒå¢ƒ', 'æ„Ÿè¦š',
            'å°è±¡', 'æ„Ÿæƒ³', 'æ„è¦‹', 'è€ƒãˆ', 'æ€è€ƒ', 'åˆ¤æ–­',
            
            # å½¢å®¹ãƒ»ç¨‹åº¦
            'ç¨‹åº¦', 'å…·åˆ', 'èª¿å­', 'æ§˜å­', 'æ§˜ç›¸', 'æ¨¡æ§˜',
            'æœ€ä¸Š', 'æœ€é«˜', 'æœ€ä½', 'æœ€å¤§', 'æœ€å°', 'æœ€å¤š',
        }
        
        # å®Ÿè¨¼æ¸ˆã¿é«˜ä¿¡é ¼åº¦éƒ½å¸‚ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆ0.92-0.98ä¿¡é ¼åº¦ï¼‰
        self.high_confidence_cities = {
            # æ±äº¬è©³ç´°åœ°åï¼ˆä¿¡é ¼åº¦0.95ï¼‰
            "æœ¬éƒ·": (35.7081, 139.7619, "æ±äº¬éƒ½æ–‡äº¬åŒº", 0.95),
            "ç¥ç”°": (35.6918, 139.7648, "æ±äº¬éƒ½åƒä»£ç”°åŒº", 0.95),
            "é’å±±": (35.6736, 139.7263, "æ±äº¬éƒ½æ¸¯åŒº", 0.95),
            "éº»å¸ƒ": (35.6581, 139.7414, "æ±äº¬éƒ½æ¸¯åŒº", 0.95),
            "ä¸¡å›½": (35.6967, 139.7933, "æ±äº¬éƒ½å¢¨ç”°åŒº", 0.95),
            "èµ¤å‚": (35.6745, 139.7378, "æ±äº¬éƒ½æ¸¯åŒº", 0.95),
            "æ—¥æœ¬æ©‹": (35.6813, 139.7744, "æ±äº¬éƒ½ä¸­å¤®åŒº", 0.95),
            "ç¯‰åœ°": (35.6654, 139.7707, "æ±äº¬éƒ½ä¸­å¤®åŒº", 0.95),
            "æ–°æ©‹": (35.6665, 139.7580, "æ±äº¬éƒ½æ¸¯åŒº", 0.95),
            "ä¸Šé‡": (35.7136, 139.7772, "æ±äº¬éƒ½å°æ±åŒº", 0.95),
            
            # äº¬éƒ½è©³ç´°åœ°åï¼ˆä¿¡é ¼åº¦0.92-0.98ï¼‰
            "ä¼è¦‹": (34.9393, 135.7578, "äº¬éƒ½åºœäº¬éƒ½å¸‚ä¼è¦‹åŒº", 0.98),
            "åµå±±": (35.0088, 135.6761, "äº¬éƒ½åºœäº¬éƒ½å¸‚å³äº¬åŒº", 0.98),
            "æ¸…æ°´": (34.9948, 135.7849, "äº¬éƒ½åºœäº¬éƒ½å¸‚æ±å±±åŒº", 0.92),
            "ç¥‡åœ’": (35.0037, 135.7744, "äº¬éƒ½åºœäº¬éƒ½å¸‚æ±å±±åŒº", 0.98),
            "å®‡æ²»": (34.8842, 135.7991, "äº¬éƒ½åºœå®‡æ²»å¸‚", 0.95),
            
            # å¤§é˜ªä¸»è¦åœ°åï¼ˆä¿¡é ¼åº¦0.92ï¼‰
            "é›£æ³¢": (34.6659, 135.5020, "å¤§é˜ªåºœå¤§é˜ªå¸‚æµªé€ŸåŒº", 0.92),
            "æ¢…ç”°": (34.7010, 135.4962, "å¤§é˜ªåºœå¤§é˜ªå¸‚åŒ—åŒº", 0.92),
            "å¿ƒæ–æ©‹": (34.6723, 135.5002, "å¤§é˜ªåºœå¤§é˜ªå¸‚ä¸­å¤®åŒº", 0.92),
            
            # ç¥å¥ˆå·çœŒä¸»è¦åœ°åï¼ˆä¿¡é ¼åº¦0.95ï¼‰
            "æ¨ªæµœ": (35.4478, 139.6425, "ç¥å¥ˆå·çœŒæ¨ªæµœå¸‚", 0.95),
            "éŒå€‰": (35.3197, 139.5468, "ç¥å¥ˆå·çœŒéŒå€‰å¸‚", 0.95),
            "ç®±æ ¹": (35.2322, 139.1069, "ç¥å¥ˆå·çœŒè¶³æŸ„ä¸‹éƒ¡ç®±æ ¹ç”º", 0.95),
            
            # ä¹å·åœ°åï¼ˆä¿¡é ¼åº¦0.95ï¼‰
            "é¹¿å…å³¶": (31.5966, 130.5571, "é¹¿å…å³¶çœŒé¹¿å…å³¶å¸‚", 0.95),
            
            # é–¢æ±ãƒ»ä¸­éƒ¨è¦³å…‰åœ°ï¼ˆä¿¡é ¼åº¦0.93ï¼‰
            "æ—¥å…‰": (36.7581, 139.6014, "æ ƒæœ¨çœŒæ—¥å…‰å¸‚", 0.93),
            
            # åŒ—æµ·é“åœ°åï¼ˆä¿¡é ¼åº¦0.95ï¼‰
            "å°æ¨½": (43.1907, 140.9947, "åŒ—æµ·é“å°æ¨½å¸‚", 0.95),
            "å‡½é¤¨": (41.7687, 140.7291, "åŒ—æµ·é“å‡½é¤¨å¸‚", 0.95),
            "æœ­å¹Œ": (43.0642, 141.3469, "åŒ—æµ·é“æœ­å¹Œå¸‚", 0.95),
        }
        
        # æ­´å²åœ°åãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆä¿¡é ¼åº¦0.85ï¼‰
        self.historical_places = {
            "æ±Ÿæˆ¸": (35.6762, 139.6503, "æ±äº¬éƒ½", 0.85),
            "å¹³å®‰äº¬": (35.0116, 135.7681, "äº¬éƒ½åºœ", 0.85),
            "ä¼Šå‹¢": (34.4900, 136.7056, "ä¸‰é‡çœŒä¼Šå‹¢å¸‚", 0.85),
            "å¤§å’Œ": (34.6851, 135.8325, "å¥ˆè‰¯çœŒ", 0.85),
            "ç¾æ¿ƒ": (35.3912, 136.7223, "å²é˜œçœŒ", 0.85),
            "å°¾å¼µ": (35.1802, 136.9066, "æ„›çŸ¥çœŒè¥¿éƒ¨", 0.85),
            "è–©æ‘©": (31.5966, 130.5571, "é¹¿å…å³¶çœŒ", 0.85),
            "ä¼Šè±†": (34.9756, 138.9462, "é™å²¡çœŒä¼Šè±†åŠå³¶", 0.85),
            "ç”²æ–": (35.6635, 138.5681, "å±±æ¢¨çœŒ", 0.85),
            "ä¿¡æ¿ƒ": (36.2048, 137.9677, "é•·é‡çœŒ", 0.85),
            "è¶Šå¾Œ": (37.9026, 139.0235, "æ–°æ½ŸçœŒ", 0.85),
            "è¿‘æ±Ÿ": (35.0045, 135.8686, "æ»‹è³€çœŒ", 0.85),
        }
        
        # éƒ½é“åºœçœŒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆä¿¡é ¼åº¦0.95ï¼‰
        self.prefecture_coords = {
            "åŒ—æµ·é“": (43.0642, 141.3469, 0.95),
            "é’æ£®çœŒ": (40.8244, 140.7400, 0.95),
            "å²©æ‰‹çœŒ": (39.7036, 141.1527, 0.95),
            "å®®åŸçœŒ": (38.2682, 140.8721, 0.95),
            "ç§‹ç”°çœŒ": (39.7186, 140.1024, 0.95),
            "å±±å½¢çœŒ": (38.2404, 140.3633, 0.95),
            "ç¦å³¶çœŒ": (37.7503, 140.4677, 0.95),
            "èŒ¨åŸçœŒ": (36.3417, 140.4468, 0.95),
            "æ ƒæœ¨çœŒ": (36.5657, 139.8836, 0.95),
            "ç¾¤é¦¬çœŒ": (36.3911, 139.0608, 0.95),
            "åŸ¼ç‰çœŒ": (35.8572, 139.6489, 0.95),
            "åƒè‘‰çœŒ": (35.6047, 140.1233, 0.95),
            "æ±äº¬éƒ½": (35.6762, 139.6503, 0.95),
            "ç¥å¥ˆå·çœŒ": (35.4478, 139.6425, 0.95),
            "æ–°æ½ŸçœŒ": (37.9026, 139.0235, 0.95),
            "å¯Œå±±çœŒ": (36.6953, 137.2113, 0.95),
            "çŸ³å·çœŒ": (36.5945, 136.6256, 0.95),
            "ç¦äº•çœŒ": (36.0652, 136.2216, 0.95),
            "å±±æ¢¨çœŒ": (35.6635, 138.5681, 0.95),
            "é•·é‡çœŒ": (36.2048, 137.9677, 0.95),
            "å²é˜œçœŒ": (35.3912, 136.7223, 0.95),
            "é™å²¡çœŒ": (34.9766, 138.3831, 0.95),
            "æ„›çŸ¥çœŒ": (35.1802, 136.9066, 0.95),
            "ä¸‰é‡çœŒ": (34.7303, 136.5086, 0.95),
            "æ»‹è³€çœŒ": (35.0045, 135.8686, 0.95),
            "äº¬éƒ½åºœ": (35.0116, 135.7681, 0.95),
            "å¤§é˜ªåºœ": (34.6937, 135.5023, 0.95),
            "å…µåº«çœŒ": (34.6913, 135.1830, 0.95),
            "å¥ˆè‰¯çœŒ": (34.6851, 135.8325, 0.95),
            "å’Œæ­Œå±±çœŒ": (34.2261, 135.1675, 0.95),
            "é³¥å–çœŒ": (35.5038, 134.2381, 0.95),
            "å³¶æ ¹çœŒ": (35.4722, 133.0505, 0.95),
            "å²¡å±±çœŒ": (34.6617, 133.9345, 0.95),
            "åºƒå³¶çœŒ": (34.3966, 132.4596, 0.95),
            "å±±å£çœŒ": (34.1861, 131.4706, 0.95),
            "å¾³å³¶çœŒ": (34.0658, 134.5590, 0.95),
            "é¦™å·çœŒ": (34.3401, 134.0434, 0.95),
            "æ„›åª›çœŒ": (33.8416, 132.7658, 0.95),
            "é«˜çŸ¥çœŒ": (33.5597, 133.5311, 0.95),
            "ç¦å²¡çœŒ": (33.6064, 130.4181, 0.95),
            "ä½è³€çœŒ": (33.2494, 130.2989, 0.95),
            "é•·å´çœŒ": (32.7448, 129.8737, 0.95),
            "ç†Šæœ¬çœŒ": (32.7898, 130.7417, 0.95),
            "å¤§åˆ†çœŒ": (33.2382, 131.6126, 0.95),
            "å®®å´çœŒ": (31.9111, 131.4239, 0.95),
            "é¹¿å…å³¶çœŒ": (31.5966, 130.5571, 0.95),
            "æ²–ç¸„çœŒ": (26.2124, 127.6792, 0.95),
        }
        
        # æµ·å¤–åœ°åãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆæ–‡å­¦ä½œå“é »å‡ºï¼‰
        self.foreign_places = {
            "ãƒ­ãƒ¼ãƒ": (41.9028, 12.4964, "ã‚¤ã‚¿ãƒªã‚¢", 0.90),
            "ãƒ‘ãƒª": (48.8566, 2.3522, "ãƒ•ãƒ©ãƒ³ã‚¹", 0.90),
            "ãƒ­ãƒ³ãƒ‰ãƒ³": (51.5074, -0.1278, "ã‚¤ã‚®ãƒªã‚¹", 0.90),
            "ãƒ™ãƒ«ãƒªãƒ³": (52.5200, 13.4050, "ãƒ‰ã‚¤ãƒ„", 0.90),
            "ãƒ‹ãƒ¥ãƒ¼ãƒ¨ãƒ¼ã‚¯": (40.7128, -74.0060, "ã‚¢ãƒ¡ãƒªã‚«", 0.90),
            "ä¸Šæµ·": (31.2304, 121.4737, "ä¸­å›½", 0.90),
            "ãƒšã‚­ãƒ³": (39.9042, 116.4074, "ä¸­å›½", 0.90),
            "åŒ—äº¬": (39.9042, 116.4074, "ä¸­å›½", 0.90),
            "ãƒ¢ã‚¹ã‚¯ãƒ¯": (55.7558, 37.6176, "ãƒ­ã‚·ã‚¢", 0.90),
            "ã‚¦ã‚£ãƒ¼ãƒ³": (48.2082, 16.3738, "ã‚ªãƒ¼ã‚¹ãƒˆãƒªã‚¢", 0.90),
        }
    
    def analyze_context(self, place_name: str, sentence: str, before_text: str = "", after_text: str = "") -> ContextAnalysisResult:
        """æ–‡è„ˆåˆ†æã‚’å®Ÿè¡Œï¼ˆé«˜ç²¾åº¦ãƒ•ã‚£ãƒ«ã‚¿çµ±åˆç‰ˆï¼‰"""
        full_context = f"{before_text} {sentence} {after_text}"
        
        # ğŸš€ é«˜ç²¾åº¦ãƒ•ã‚£ãƒ«ã‚¿: äººåãƒ»å­¦è¡“ç”¨èªãƒ»ä¸€èˆ¬åè©ãƒã‚§ãƒƒã‚¯
        if place_name in self.known_person_names:
            return ContextAnalysisResult(
                is_place_name=False,
                confidence=0.95,
                place_type="äººå",
                historical_context="",
                geographic_context="",
                reasoning=f"äººåãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç™»éŒ²æ¸ˆã¿: {place_name}",
                suggested_location=None
            )
        
        if place_name in self.academic_terms:
            return ContextAnalysisResult(
                is_place_name=False,
                confidence=0.90,
                place_type="å­¦è¡“ç”¨èª",
                historical_context="",
                geographic_context="",
                reasoning=f"å­¦è¡“ãƒ»å°‚é–€ç”¨èªã¨ã—ã¦è­˜åˆ¥: {place_name}",
                suggested_location=None
            )
        
        if place_name in self.general_nouns:
            return ContextAnalysisResult(
                is_place_name=False,
                confidence=0.85,
                place_type="ä¸€èˆ¬åè©",
                historical_context="",
                geographic_context="",
                reasoning=f"ä¸€èˆ¬åè©ã¨ã—ã¦è­˜åˆ¥: {place_name}",
                suggested_location=None
            )
        
        # åœ°åæŒ‡æ¨™ã®ã‚¹ã‚³ã‚¢
        place_score = 0
        for pattern in self.context_patterns["place_indicators"]:
            if re.search(pattern, full_context):
                place_score += 1
        
        # äººåæŒ‡æ¨™ã®ã‚¹ã‚³ã‚¢
        person_score = 0
        for pattern in self.context_patterns["person_indicators"]:
            if re.search(pattern, full_context):
                person_score += 1
        
        # æ­´å²çš„æ–‡è„ˆã®ã‚¹ã‚³ã‚¢
        historical_score = 0
        for pattern in self.context_patterns["historical_indicators"]:
            if re.search(pattern, full_context):
                historical_score += 1
        
        # æ›–æ˜§åœ°åã®ç‰¹æ®Šå‡¦ç†
        if place_name in self.ambiguous_places:
            ambiguous_info = self.ambiguous_places[place_name]
            person_possibility = ambiguous_info.get("äººåå¯èƒ½æ€§", 0)
            
            if person_score > place_score and person_possibility > 0.5:
                return ContextAnalysisResult(
                    is_place_name=False,
                    confidence=0.8,
                    place_type="äººåå¯èƒ½æ€§",
                    historical_context="",
                    geographic_context=ambiguous_info.get("åœ°å", ""),
                    reasoning=f"äººåæŒ‡æ¨™ãŒå¼·ãã€äººåå¯èƒ½æ€§{person_possibility}",
                    suggested_location=None
                )
        
        # ç·åˆåˆ¤æ–­
        total_place_indicators = place_score + historical_score
        is_place = total_place_indicators > person_score
        
        confidence = 0.5
        if total_place_indicators > 0:
            confidence = min(0.95, 0.5 + (total_place_indicators * 0.15))
        
        place_type = "ä¸€èˆ¬åœ°å"
        if historical_score > 0:
            place_type = "æ­´å²åœ°å"
        elif place_name in self.high_confidence_cities:
            place_type = "éƒ½å¸‚éƒ¨"
        elif any(place_name.endswith(suffix) for suffix in ["éƒ½", "åºœ", "çœŒ"]):
            place_type = "éƒ½é“åºœçœŒ"
        
        # reasoningå¤‰æ•°ã®åˆæœŸåŒ–
        reasoning = f"åœ°åæŒ‡æ¨™{total_place_indicators}ä»¶ vs äººåæŒ‡æ¨™{person_score}ä»¶"
        
        # ChatGPTã«ã‚ˆã‚‹è¿½åŠ åˆ†æï¼ˆå¯èƒ½ãªå ´åˆï¼‰
        if self.openai_enabled:
            try:
                chatgpt_analysis = self._analyze_context_with_llm(place_name, sentence)
                if chatgpt_analysis:
                    # ChatGPTåˆ†æçµæœã‚’çµ±åˆ
                    confidence = max(confidence, chatgpt_analysis.get('confidence', 0.0))
                    if chatgpt_analysis.get('is_place_name'):
                        is_place = True
                    
                    # åˆ†æçµæœã‚’reasoningã«è¿½åŠ 
                    chatgpt_reasoning = chatgpt_analysis.get('reasoning', '')
                    if chatgpt_reasoning:
                        reasoning += f" | ChatGPT: {chatgpt_reasoning}"
                        
                    logger.info(f"ğŸ¤– ChatGPTåˆ†æçµ±åˆ: {place_name} -> ä¿¡é ¼åº¦{confidence:.2f}")
                        
            except Exception as e:
                logger.warning(f"ChatGPTåˆ†æã‚¨ãƒ©ãƒ¼ ({place_name}): {str(e)}")

        return ContextAnalysisResult(
            is_place_name=is_place,
            confidence=confidence,
            place_type=place_type,
            historical_context="æ­´å²çš„æ–‡è„ˆã‚ã‚Š" if historical_score > 0 else "",
            geographic_context=f"åœ°åæŒ‡æ¨™{total_place_indicators}ä»¶",
            reasoning=reasoning,
            suggested_location=self.ambiguous_places.get(place_name, {}).get("åœ°å")
        )
    
    def geocode_place(self, place_name: str, sentence: str = "", before_text: str = "", after_text: str = "") -> Optional[GeocodingResult]:
        """åœ°åã‚’Geocodeï¼ˆåº§æ¨™å¤‰æ›ï¼‰"""
        
        # æ–‡è„ˆåˆ†æ
        context_analysis = self.analyze_context(place_name, sentence, before_text, after_text)
        
        # äººåã¨åˆ¤æ–­ã•ã‚ŒãŸå ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if not context_analysis.is_place_name:
            return None
        
        # 1. é«˜ä¿¡é ¼åº¦éƒ½å¸‚ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æ¤œç´¢
        if place_name in self.high_confidence_cities:
            lat, lon, location, confidence = self.high_confidence_cities[place_name]
            return GeocodingResult(
                place_name=place_name,
                latitude=lat,
                longitude=lon,
                confidence=confidence,
                source="high_confidence_cities",
                prefecture=location.split('éƒ½')[0] + 'éƒ½' if 'éƒ½' in location else 
                          location.split('åºœ')[0] + 'åºœ' if 'åºœ' in location else
                          location.split('çœŒ')[0] + 'çœŒ' if 'çœŒ' in location else None,
                city=location,
                context_analysis=context_analysis
            )
        
        # 2. æ­´å²åœ°åãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æ¤œç´¢
        if place_name in self.historical_places:
            lat, lon, location, confidence = self.historical_places[place_name]
            return GeocodingResult(
                place_name=place_name,
                latitude=lat,
                longitude=lon,
                confidence=confidence,
                source="historical_places",
                prefecture=location if location.endswith(('éƒ½', 'åºœ', 'çœŒ')) else None,
                city=location,
                context_analysis=context_analysis
            )
        
        # 3. éƒ½é“åºœçœŒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æ¤œç´¢
        for pref_name, (lat, lon, confidence) in self.prefecture_coords.items():
            if place_name in pref_name or pref_name.replace('éƒ½', '').replace('åºœ', '').replace('çœŒ', '') == place_name:
                return GeocodingResult(
                    place_name=place_name,
                    latitude=lat,
                    longitude=lon,
                    confidence=confidence,
                    source="prefecture_coords",
                    prefecture=pref_name,
                    city=pref_name,
                    context_analysis=context_analysis
                )
        
        # 4. æµ·å¤–åœ°åãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æ¤œç´¢
        if place_name in self.foreign_places:
            lat, lon, country, confidence = self.foreign_places[place_name]
            return GeocodingResult(
                place_name=place_name,
                latitude=lat,
                longitude=lon,
                confidence=confidence,
                source="foreign_places",
                prefecture=country,
                city=place_name,
                context_analysis=context_analysis
            )
        
        # 5. ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šéƒ¨åˆ†ãƒãƒƒãƒãƒ³ã‚°
        fallback_result = self._fallback_geocoding(place_name, context_analysis)
        if fallback_result:
            fallback_result.fallback_used = True
            return fallback_result
        
        return None
    
    def _fallback_geocoding(self, place_name: str, context_analysis: ContextAnalysisResult) -> Optional[GeocodingResult]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯Geocodingï¼ˆéƒ¨åˆ†ãƒãƒƒãƒãƒ³ã‚° + Google Maps APIï¼‰"""
        
        # 1. éƒ¨åˆ†ãƒãƒƒãƒãƒ³ã‚°ï¼ˆéƒ½é“åºœçœŒï¼‰
        for pref_name, (lat, lon, confidence) in self.prefecture_coords.items():
            pref_base = pref_name.replace('éƒ½', '').replace('åºœ', '').replace('çœŒ', '')
            if pref_base in place_name or place_name in pref_base:
                return GeocodingResult(
                    place_name=place_name,
                    latitude=lat,
                    longitude=lon,
                    confidence=max(0.3, confidence - 0.3),  # ä¿¡é ¼åº¦ã‚’ä¸‹ã’ã‚‹
                    source="fallback_prefecture",
                    prefecture=pref_name,
                    city=f"{pref_name}å†…ã®åœ°å",
                    context_analysis=context_analysis
                )
        
        # 2. éƒ¨åˆ†ãƒãƒƒãƒãƒ³ã‚°ï¼ˆé«˜ä¿¡é ¼åº¦éƒ½å¸‚ï¼‰
        for city_name, (lat, lon, location, confidence) in self.high_confidence_cities.items():
            if city_name in place_name or place_name in city_name:
                return GeocodingResult(
                    place_name=place_name,
                    latitude=lat,
                    longitude=lon,
                    confidence=max(0.4, confidence - 0.2),
                    source="fallback_city",
                    prefecture=location.split('éƒ½')[0] + 'éƒ½' if 'éƒ½' in location else 
                              location.split('åºœ')[0] + 'åºœ' if 'åºœ' in location else
                              location.split('çœŒ')[0] + 'çœŒ' if 'çœŒ' in location else None,
                    city=location,
                    context_analysis=context_analysis
                )
        
        # 3. Google Maps API Geocodingï¼ˆæ–°æ©Ÿèƒ½ï¼‰
        google_result = self._google_maps_geocoding(place_name, context_analysis)
        if google_result:
            return google_result
        
        return None

    def _google_maps_geocoding(self, place_name: str, context_analysis: ContextAnalysisResult) -> Optional[GeocodingResult]:
        """Google Maps API Geocoding"""
        google_api_key = os.getenv('GOOGLE_MAPS_API_KEY')
        
        if not google_api_key:
            logger.debug("Google Maps APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return None
        
        try:
            import googlemaps
            gmaps = googlemaps.Client(key=google_api_key)
            
            # æ—¥æœ¬å›½å†…ã«é™å®šã—ã¦Geocoding
            search_query = f"{place_name}, æ—¥æœ¬"
            
            geocode_result = gmaps.geocode(
                search_query,
                region='jp',
                language='ja'
            )
            
            if geocode_result:
                location = geocode_result[0]['geometry']['location']
                lat = location['lat']
                lng = location['lng']
                
                # æ—¥æœ¬å›½å†…ã®åº§æ¨™ã‹ãƒã‚§ãƒƒã‚¯
                if self._is_japan_coordinate(lat, lng):
                    # ä½æ‰€ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‹ã‚‰è©³ç´°æƒ…å ±ã‚’æŠ½å‡º
                    prefecture = None
                    city = None
                    formatted_address = geocode_result[0].get('formatted_address', '')
                    
                    for component in geocode_result[0].get('address_components', []):
                        types = component.get('types', [])
                        if 'administrative_area_level_1' in types:
                            prefecture = component['long_name']
                        elif 'locality' in types or 'administrative_area_level_2' in types:
                            city = component['long_name']
                    
                    # ä¿¡é ¼åº¦è¨ˆç®—ï¼ˆGoogle Maps APIã¯é«˜ä¿¡é ¼åº¦ã ãŒã€æ–‡è„ˆåˆ†æçµæœã‚‚è€ƒæ…®ï¼‰
                    confidence = 0.8  # Google Maps APIãƒ™ãƒ¼ã‚¹ä¿¡é ¼åº¦
                    if context_analysis.confidence > 0.8:
                        confidence = min(0.9, confidence + 0.1)
                    
                    logger.info(f"ğŸŒ Google MapsæˆåŠŸ: {place_name} â†’ ({lat:.4f}, {lng:.4f})")
                    
                    return GeocodingResult(
                        place_name=place_name,
                        latitude=lat,
                        longitude=lng,
                        confidence=confidence,
                        source="google_maps_api",
                        prefecture=prefecture,
                        city=city or formatted_address,
                        context_analysis=context_analysis,
                        fallback_used=True
                    )
                else:
                    logger.debug(f"Google MapsçµæœãŒæ—¥æœ¬å›½å¤–: {place_name} â†’ ({lat:.4f}, {lng:.4f})")
            
        except ImportError:
            logger.warning("googlemapsãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        except Exception as e:
            logger.error(f"Google Maps APIã‚¨ãƒ©ãƒ¼ ({place_name}): {str(e)}")
        
        return None

    def _is_japan_coordinate(self, lat: float, lng: float) -> bool:
        """åº§æ¨™ãŒæ—¥æœ¬å›½å†…ã‹ãƒã‚§ãƒƒã‚¯"""
        # æ—¥æœ¬ã®å¤§ã¾ã‹ãªåº§æ¨™ç¯„å›²
        japan_bounds = {
            'north': 45.5,
            'south': 24.0,
            'east': 146.0,
            'west': 122.0
        }
        
        return (japan_bounds['south'] <= lat <= japan_bounds['north'] and
                japan_bounds['west'] <= lng <= japan_bounds['east'])
    
    def _analyze_context_with_llm(self, place_name: str, sentence: str) -> Optional[Dict[str, any]]:
        """ChatGPTã«ã‚ˆã‚‹æ–‡è„ˆåˆ†æï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œï¼‰"""
        if not self.openai_enabled:
            return None
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        cache_key = _get_cache_key(f"{place_name}:{sentence}", "openai_context")
        if cache_key in _api_cache:
            logger.info(f"ğŸ¯ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {place_name}")
            return _api_cache[cache_key]
            
        try:
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            _rate_limit_api('openai', 1.0)
            
            prompt = f"""
ä»¥ä¸‹ã®æ–‡ç« ã§ä½¿ã‚ã‚Œã¦ã„ã‚‹ã€Œ{place_name}ã€ã«ã¤ã„ã¦åˆ†æã—ã¦ãã ã•ã„ã€‚

æ–‡ç« : {sentence}

ä»¥ä¸‹ã®è¦³ç‚¹ã‹ã‚‰åˆ†æã—ã€JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š
{{
    "is_place_name": true/false,
    "confidence": 0.0-1.0,
    "place_type": "éƒ½å¸‚/åœ°åŸŸ/æ­´å²åœ°å/è‡ªç„¶åœ°å/äººåãªã©",
    "reasoning": "åˆ¤æ–­ç†ç”±"
}}

åˆ¤æ–­åŸºæº–ï¼š
- åœ°åã¨ã—ã¦ä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹ã€äººåã¨ã—ã¦ä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹
- æ–‡è±ªä½œå“ã§ã®æ–‡è„ˆçš„æ„å‘³
- æ­´å²çš„ãƒ»æ–‡å­¦çš„ãªèƒŒæ™¯
"""

            response = self.openai_client.chat.completions.create(
                model=os.getenv('OPENAI_MODEL', 'gpt-3.5-turbo'),
                messages=[
                    {'role': 'system', 'content': 'ã‚ãªãŸã¯æ–‡è±ªä½œå“ã®åœ°ååˆ†æå°‚é–€å®¶ã§ã™ã€‚æ–‡è„ˆã‚’ç†è§£ã—ã¦åœ°å/äººåã‚’æ­£ç¢ºã«åˆ¤åˆ¥ã—ã¦ãã ã•ã„ã€‚'},
                    {'role': 'user', 'content': prompt}
                ],
                max_tokens=int(os.getenv('OPENAI_MAX_TOKENS', '300')),
                temperature=float(os.getenv('OPENAI_TEMPERATURE', '0.1'))
            )
            
            response_text = response.choices[0].message.content.strip()
            
            # JSONè§£æè©¦è¡Œ
            try:
                # JSONãƒ–ãƒ­ãƒƒã‚¯ã®æŠ½å‡ºï¼ˆ```json ``` ã«å›²ã¾ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
                if '```json' in response_text:
                    json_start = response_text.find('```json') + 7
                    json_end = response_text.find('```', json_start)
                    response_text = response_text[json_start:json_end].strip()
                elif '```' in response_text:
                    json_start = response_text.find('```') + 3
                    json_end = response_text.find('```', json_start)
                    response_text = response_text[json_start:json_end].strip()
                    
                result = json.loads(response_text)
                
                # çµæœã®æ¤œè¨¼
                if isinstance(result, dict) and 'is_place_name' in result:
                    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                    _api_cache[cache_key] = result
                    _save_api_cache(_api_cache)
                    logger.info(f"ğŸ’¾ çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥: {place_name}")
                    return result
                else:
                    logger.warning(f"ChatGPTå¿œç­”ã®å½¢å¼ãŒä¸æ­£: {response_text}")
                    return None
                    
            except json.JSONDecodeError as e:
                logger.warning(f"ChatGPTå¿œç­”ã®JSONè§£æã‚¨ãƒ©ãƒ¼: {response_text}")
                return None
            
        except Exception as e:
            logger.error(f"ChatGPT APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None
    
    def geocode_places_batch(self, limit: Optional[int] = None) -> Dict[str, int]:
        """ä¸€æ‹¬Geocodingå‡¦ç†"""
        conn = sqlite3.connect('data/bungo_map.db')
        cursor = conn.cursor()
        
        try:
            # æœªå‡¦ç†ã®åœ°å-ã‚»ãƒ³ãƒ†ãƒ³ã‚¹é–¢é€£ã‚’å–å¾—ï¼ˆæ–°ã‚¹ã‚­ãƒ¼ãƒå¯¾å¿œï¼‰
            query = """
                SELECT sp.sentence_id, sp.place_id, p.place_name, s.sentence_text,
                       sp.prev_sentence_1, sp.next_sentence_1
                FROM sentence_places sp
                JOIN places p ON sp.place_id = p.place_id
                JOIN sentences s ON sp.sentence_id = s.sentence_id
                WHERE p.latitude IS NULL OR p.longitude IS NULL
            """
            
            if limit:
                query += f" LIMIT {limit}"
                
            cursor.execute(query)
            places_to_geocode = cursor.fetchall()
            
            logger.info(f"ğŸ¯ Geocodingå¯¾è±¡: {len(places_to_geocode)}ä»¶")
            
            stats = {
                'processed_places': 0,
                'geocoded_places': 0,
                'skipped_places': 0,
                'errors': 0
            }
            
            for sentence_id, place_id, place_name, sentence_text, prev_sentence, next_sentence in places_to_geocode:
                try:
                    # Geocodingå®Ÿè¡Œ
                    result = self.geocode_place(place_name, sentence_text, prev_sentence or "", next_sentence or "")
                    
                    if result:
                        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°
                        self._update_place_coordinates(place_id, result)
                        stats['geocoded_places'] += 1
                        
                        logger.info(f"âœ… Geocoding: {place_name} â†’ {result.latitude:.4f}, {result.longitude:.4f} ({result.confidence:.2f})")
                    else:
                        stats['skipped_places'] += 1
                        logger.debug(f"â­ï¸ ã‚¹ã‚­ãƒƒãƒ—: {place_name} (æ–‡è„ˆåˆ¤æ–­ã§é™¤å¤–)")
                    
                    stats['processed_places'] += 1
                    
                except Exception as e:
                    stats['errors'] += 1
                    logger.error(f"âŒ Geocodingã‚¨ãƒ©ãƒ¼: {place_name} - {e}")
            
            return stats
            
        finally:
            conn.close()
    
    def _update_place_coordinates(self, place_id: int, result: GeocodingResult):
        """åœ°åã®åº§æ¨™æƒ…å ±ã‚’æ›´æ–°"""
        conn = sqlite3.connect('data/bungo_map.db')
        cursor = conn.cursor()
        
        try:
            # placesãƒ†ãƒ¼ãƒ–ãƒ«æ›´æ–°
            cursor.execute("""
                UPDATE places SET
                    latitude = ?, longitude = ?, confidence = ?,
                    source_system = ?, verification_status = 'auto_geocoded',
                    updated_at = ?
                WHERE place_id = ?
            """, (
                result.latitude, result.longitude, result.confidence,
                f"context_aware_geocoder_{result.source}", datetime.now(),
                place_id
            ))
            
            # sentence_placesãƒ†ãƒ¼ãƒ–ãƒ«ã«ã‚‚åº§æ¨™æƒ…å ±ã‚’æ›´æ–°
            cursor.execute("""
                UPDATE sentence_places SET
                    latitude = ?, longitude = ?,
                    geocoding_source = ?, geocoding_confidence = ?
                WHERE place_id = ?
            """, (
                result.latitude, result.longitude,
                f"context_aware_geocoder_{result.source}", result.confidence,
                place_id
            ))
            
            conn.commit()
            
        finally:
            conn.close()
    
    def get_geocoding_statistics(self) -> Dict[str, any]:
        """Geocodingçµ±è¨ˆã®å–å¾—"""
        conn = sqlite3.connect('data/bungo_map.db')
        cursor = conn.cursor()
        
        # åŸºæœ¬çµ±è¨ˆ
        cursor.execute('SELECT COUNT(*) FROM places')
        total_places = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(*) FROM places WHERE latitude IS NOT NULL AND longitude IS NOT NULL')
        geocoded_places = cursor.fetchone()[0]
        
        # ã‚½ãƒ¼ã‚¹åˆ¥çµ±è¨ˆ
        cursor.execute('SELECT source_system, COUNT(*) FROM places WHERE source_system IS NOT NULL GROUP BY source_system')
        source_stats = {source: count for source, count in cursor.fetchall()}
        
        # ä¿¡é ¼åº¦åˆ†å¸ƒ
        cursor.execute('SELECT COUNT(*) FROM places WHERE confidence >= 0.9 AND latitude IS NOT NULL')
        high_confidence = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(*) FROM places WHERE confidence >= 0.7 AND confidence < 0.9 AND latitude IS NOT NULL')
        medium_confidence = cursor.fetchone()[0]
        
        conn.close()
        
        return {
            "total_places": total_places,
            "geocoded_places": geocoded_places,
            "geocoding_rate": geocoded_places / total_places * 100 if total_places > 0 else 0,
            "source_stats": source_stats,
            "confidence_distribution": {
                "high_confidence_count": high_confidence,
                "medium_confidence_count": medium_confidence
            }
        }

    def delete_invalid_places(self, place_names: List[str], reason: str = "ç®¡ç†è€…åˆ¤æ–­") -> Dict[str, any]:
        """ç„¡åŠ¹ãªåœ°åã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å‰Šé™¤"""
        conn = sqlite3.connect('data/bungo_map.db')
        cursor = conn.cursor()
        
        deleted_places = []
        not_found_places = []
        
        for place_name in place_names:
            try:
                # place_mastersã‹ã‚‰åœ°åã®å­˜åœ¨ç¢ºèª
                cursor.execute('SELECT master_id, display_name FROM place_masters WHERE display_name = ?', (place_name,))
                place_data = cursor.fetchone()
                
                if not place_data:
                    not_found_places.append(place_name)
                    continue
                
                master_id = place_data[0]
                
                # é–¢é€£ã™ã‚‹sentence_placesãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å‰Šé™¤
                cursor.execute('DELETE FROM sentence_places WHERE master_id = ?', (master_id,))
                deleted_relations = cursor.rowcount
                
                # place_mastersã®validation_statusã‚’'rejected'ã«è¨­å®šï¼ˆè«–ç†å‰Šé™¤ï¼‰
                cursor.execute('UPDATE place_masters SET validation_status = ?, updated_at = CURRENT_TIMESTAMP WHERE master_id = ?', 
                             ('rejected', master_id))
                
                deleted_places.append({
                    "place_name": place_name,
                    "master_id": master_id,
                    "deleted_relations": deleted_relations,
                    "reason": reason
                })
                
                logger.info(f"ğŸ—‘ï¸ åœ°åå‰Šé™¤å®Œäº†: {place_name} (ID: {master_id}, é–¢é€£: {deleted_relations}ä»¶)")
                
            except Exception as e:
                logger.error(f"åœ°åå‰Šé™¤ã‚¨ãƒ©ãƒ¼ ({place_name}): {str(e)}")
                not_found_places.append(place_name)
        
        conn.commit()
        conn.close()
        
        return {
            "deleted_places": deleted_places,
            "not_found_places": not_found_places,
            "total_deleted": len(deleted_places),
            "reason": reason
        }

    def cleanup_invalid_places(self, auto_confirm: bool = False) -> Dict[str, any]:
        """ç„¡åŠ¹åœ°åã®è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        conn = sqlite3.connect('data/bungo_map.db')
        cursor = conn.cursor()
        
        # æœªGeocodingã®åœ°åã‚’å–å¾—
        cursor.execute('''
            SELECT p.place_id, p.place_name, p.place_type, p.confidence, p.source_system,
                   COUNT(sp.sentence_id) as usage_count,
                   MIN(s.sentence_text) as sample_sentence
            FROM places p
            LEFT JOIN sentence_places sp ON p.place_id = sp.place_id
            LEFT JOIN sentences s ON sp.sentence_id = s.sentence_id
            WHERE p.latitude IS NULL AND p.longitude IS NULL
            GROUP BY p.place_id, p.place_name
            ORDER BY usage_count ASC
        ''')
        
        ungeocoded_places = cursor.fetchall()
        conn.close()
        
        # å‰Šé™¤å€™è£œã®åˆ†æ
        candidates_for_deletion = []
        
        for place_id, place_name, place_type, confidence, source_system, usage_count, sample_sentence in ungeocoded_places:
            # æ–‡è„ˆåˆ†æã«ã‚ˆã‚‹å‰Šé™¤åˆ¤å®š
            if sample_sentence:
                context_analysis = self.analyze_context(place_name, sample_sentence)
                
                # äººåã¨åˆ¤å®šã•ã‚ŒãŸå ´åˆ
                if not context_analysis.is_place_name and "äººå" in context_analysis.reasoning:
                    candidates_for_deletion.append({
                        "place_id": place_id,
                        "place_name": place_name,
                        "reason": "äººåã¨ã—ã¦åˆ¤å®š",
                        "confidence": context_analysis.confidence,
                        "usage_count": usage_count,
                        "sample": sample_sentence[:50] + "..." if len(sample_sentence) > 50 else sample_sentence
                    })
                
                # æ¶ç©ºåœ°åã‚„æŠ½è±¡è¡¨ç¾ã®åˆ¤å®š
                elif place_name in ["è‡ªç„¶ç”º", "æ¯æ—¥æµ·"] or any(keyword in place_name for keyword in ["æ¯æ—¥", "è‡ªç„¶"]):
                    candidates_for_deletion.append({
                        "place_id": place_id,
                        "place_name": place_name,
                        "reason": "æ¶ç©ºåœ°åã¾ãŸã¯æŠ½è±¡è¡¨ç¾",
                        "confidence": confidence or 0.0,
                        "usage_count": usage_count,
                        "sample": sample_sentence[:50] + "..." if len(sample_sentence) > 50 else sample_sentence
                    })
        
        # è‡ªå‹•å‰Šé™¤ã¾ãŸã¯ç¢ºèª
        if auto_confirm:
            place_names_to_delete = [candidate["place_name"] for candidate in candidates_for_deletion]
            deletion_result = self.delete_invalid_places(place_names_to_delete, "è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—")
            
            return {
                "candidates": candidates_for_deletion,
                "deletion_result": deletion_result,
                "auto_deleted": True
            }
        else:
            return {
                "candidates": candidates_for_deletion,
                "auto_deleted": False,
                "message": "å‰Šé™¤å€™è£œãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚æ‰‹å‹•ã§ç¢ºèªãƒ»å‰Šé™¤ã—ã¦ãã ã•ã„ã€‚"
            }

    def get_place_usage_analysis(self, place_name: str) -> Dict[str, any]:
        """ç‰¹å®šåœ°åã®ä½¿ç”¨çŠ¶æ³è©³ç´°åˆ†æ"""
        conn = sqlite3.connect('data/bungo_map.db')
        cursor = conn.cursor()
        
        # åŸºæœ¬æƒ…å ±
        cursor.execute('SELECT * FROM places WHERE place_name = ?', (place_name,))
        place_data = cursor.fetchone()
        
        if not place_data:
            return {"error": f"åœ°å '{place_name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}
        
        # ä½¿ç”¨æ–‡ä¸€è¦§
        cursor.execute('''
            SELECT s.sentence_text, w.title, a.name
            FROM sentences s
            JOIN sentence_places sp ON s.sentence_id = sp.sentence_id
            JOIN places p ON sp.place_id = p.place_id
            LEFT JOIN works w ON s.work_id = w.work_id
            LEFT JOIN authors a ON w.author_id = a.author_id
            WHERE p.place_name = ?
            ORDER BY s.sentence_id
        ''', (place_name,))
        
        usage_sentences = cursor.fetchall()
        conn.close()
        
        # æ–‡è„ˆåˆ†æ
        context_analyses = []
        for sentence_text, work_title, author_name in usage_sentences:
            analysis = self.analyze_context(place_name, sentence_text)
            context_analyses.append({
                "sentence": sentence_text,
                "work_title": work_title,
                "author_name": author_name,
                "is_place_name": analysis.is_place_name,
                "confidence": analysis.confidence,
                "reasoning": analysis.reasoning
            })
        
        return {
            "place_data": {
                "place_id": place_data[0],
                "place_name": place_data[1],
                "place_type": place_data[2],
                "latitude": place_data[3],
                "longitude": place_data[4],
                "confidence": place_data[5],
                "source_system": place_data[6]
            },
            "usage_count": len(usage_sentences),
            "context_analyses": context_analyses,
            "recommended_action": self._get_recommended_action(context_analyses)
        }

    def _get_recommended_action(self, context_analyses: List[Dict]) -> str:
        """æ–‡è„ˆåˆ†æçµæœã«åŸºã¥ãæ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³"""
        if not context_analyses:
            return "å‰Šé™¤æ¨å¥¨: ä½¿ç”¨ä¾‹ãªã—"
        
        place_name_count = sum(1 for analysis in context_analyses if analysis["is_place_name"])
        total_count = len(context_analyses)
        
        if place_name_count == 0:
            return "å‰Šé™¤æ¨å¥¨: å…¨ã¦éåœ°åã¨ã—ã¦åˆ¤å®š"
        elif place_name_count / total_count < 0.5:
            return "å‰Šé™¤æ¤œè¨: åœ°åä½¿ç”¨ç‡ãŒä½ã„"
        else:
            return "ä¿æŒæ¨å¥¨: åœ°åã¨ã—ã¦é©åˆ‡ã«ä½¿ç”¨"

    def ai_mass_verification(self, limit: Optional[int] = None, confidence_threshold: float = 0.7) -> Dict[str, any]:
        """AIå¤§é‡æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ  - æ—¢å­˜åœ°åã®å†è©•ä¾¡"""
        if not self.openai_enabled:
            return {"error": "OpenAI APIãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"}
        
        conn = sqlite3.connect('data/bungo_map.db')
        cursor = conn.cursor()
        
        # æ¤œè¨¼å¯¾è±¡ã®åœ°åã‚’å–å¾—ï¼ˆä½¿ç”¨é »åº¦ãŒä½ã„ãƒ»æœªæ¤œè¨¼ã®åœ°åã‚’å„ªå…ˆï¼‰
        query = '''
            SELECT pm.master_id, pm.display_name, pm.geocoding_confidence, pm.geocoding_source,
                   COUNT(sp.sentence_id) as usage_count,
                   GROUP_CONCAT(s.sentence_text, '|||') as all_sentences
            FROM place_masters pm
            JOIN sentence_places sp ON pm.master_id = sp.master_id
            JOIN sentences s ON sp.sentence_id = s.sentence_id
            WHERE pm.verification_status IS NULL OR pm.verification_status != 'ai_verified'
            GROUP BY pm.master_id, pm.display_name
            ORDER BY usage_count ASC, pm.geocoding_confidence ASC
        '''
        
        if limit:
            query += f' LIMIT {limit}'
            
        cursor.execute(query)
        places_to_verify = cursor.fetchall()
        
        verification_results = {
            'verified_places': [],
            'deletion_candidates': [],
            'total_processed': 0,
            'ai_errors': 0
        }
        
        logger.info(f"ğŸ¤– AIå¤§é‡æ¤œè¨¼é–‹å§‹: {len(places_to_verify)}ä»¶")
        
        for master_id, place_name, confidence, source_system, usage_count, all_sentences in places_to_verify:
            try:
                sentences = all_sentences.split('|||') if all_sentences else []
                
                # è¤‡æ•°æ–‡è„ˆã§ã®AIåˆ†æ
                ai_analyses = []
                for sentence in sentences[:5]:  # æœ€å¤§5æ–‡ã¾ã§
                    ai_result = self._enhanced_ai_analysis(place_name, sentence)
                    if ai_result:
                        ai_analyses.append(ai_result)
                
                if not ai_analyses:
                    verification_results['ai_errors'] += 1
                    continue
                
                # ç·åˆåˆ¤å®š
                overall_verdict = self._calculate_overall_verdict(ai_analyses)
                
                place_result = {
                    'master_id': master_id,
                    'place_name': place_name,
                    'usage_count': usage_count,
                    'current_confidence': confidence,
                    'ai_analyses': ai_analyses,
                    'overall_verdict': overall_verdict,
                    'recommendation': overall_verdict['recommendation']
                }
                
                # å‰Šé™¤å€™è£œã®åˆ¤å®š
                if overall_verdict['is_valid'] == False and overall_verdict['confidence'] >= confidence_threshold:
                    verification_results['deletion_candidates'].append(place_result)
                    logger.info(f"âŒ å‰Šé™¤å€™è£œ: {place_name} (AIç¢ºä¿¡åº¦: {overall_verdict['confidence']:.2f})")
                else:
                    verification_results['verified_places'].append(place_result)
                    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ¤œè¨¼æ¸ˆã¿ãƒãƒ¼ã‚¯ã‚’ä»˜ä¸
                    cursor.execute(
                        "UPDATE place_masters SET verification_status = 'ai_verified', ai_confidence = ? WHERE master_id = ?",
                        (overall_verdict['confidence'], master_id)
                    )
                    logger.info(f"âœ… æ¤œè¨¼æ¸ˆã¿: {place_name} (AIç¢ºä¿¡åº¦: {overall_verdict['confidence']:.2f})")
                
                verification_results['total_processed'] += 1
                
            except Exception as e:
                logger.error(f"AIæ¤œè¨¼ã‚¨ãƒ©ãƒ¼ ({place_name}): {str(e)}")
                verification_results['ai_errors'] += 1
        
        conn.commit()
        conn.close()
        
        return verification_results

    def _enhanced_ai_analysis(self, place_name: str, sentence: str) -> Optional[Dict[str, any]]:
        """å¼·åŒ–ã•ã‚ŒãŸAIåˆ†æ - ã‚ˆã‚Šè©³ç´°ãªåˆ¤å®š"""
        if not self.openai_enabled:
            return None
            
        try:
            prompt = f"""
ä»¥ä¸‹ã®æ–‡ç« ä¸­ã®ã€Œ{place_name}ã€ã«ã¤ã„ã¦ã€åœ°åã¨ã—ã¦ã®å¦¥å½“æ€§ã‚’è©³ç´°ã«åˆ†æã—ã¦ãã ã•ã„ã€‚

æ–‡ç« : {sentence}

ä»¥ä¸‹ã®è¦³ç‚¹ã‹ã‚‰ç·åˆçš„ã«åˆ¤æ–­ã—ã€JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š

{{
    "is_place_name": true/false,
    "confidence": 0.0-1.0,
    "place_type": "éƒ½å¸‚å/åœ°åŸŸå/æ­´å²åœ°å/è‡ªç„¶åœ°å/äººå/å­¦è¡“ç”¨èª/ä¸€èˆ¬åè©/ãã®ä»–",
    "reasoning": "è©³ç´°ãªåˆ¤æ–­ç†ç”±",
    "context_clues": ["æ–‡è„ˆæ‰‹ãŒã‹ã‚Šã®ãƒªã‚¹ãƒˆ"],
    "alternative_interpretation": "ä»–ã®è§£é‡ˆã®å¯èƒ½æ€§",
    "literary_context": "æ–‡å­¦ä½œå“ã§ã®ä½¿ç”¨æ–‡è„ˆ"
}}

åˆ¤æ–­åŸºæº–ï¼š
1. æ–‡ä¸­ã§ã®æ–‡æ³•çš„å½¹å‰²ï¼ˆä¸»èª/ç›®çš„èª/ä¿®é£¾èªç­‰ï¼‰
2. å‘¨è¾ºèªå¥ã¨ã®é–¢ä¿‚æ€§
3. æ–‡è±ªä½œå“ã§ã®å…¸å‹çš„ãªä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³
4. åœ°åã¨ã—ã¦ä½¿ã‚ã‚Œã‚‹éš›ã®æ–‡è„ˆçš„ç‰¹å¾´
5. äººåãƒ»ä¸€èˆ¬åè©ã¨ã®åŒºåˆ¥

ç‰¹ã«æ³¨æ„ç‚¹ï¼š
- æ¤ç‰©å­¦è€…ãƒ»æ–‡è±ªã®äººåã¯åœ°åã§ã¯ãªã„
- ã€Œæ²¢å±±ã€ã€Œæ§˜å­ã€ç­‰ã®ä¸€èˆ¬åè©ã¯åœ°åã§ã¯ãªã„
- ã€ŒèªåŸã€ã€Œç—…åŸã€ç­‰ã®å­¦è¡“ç”¨èªã¯åœ°åã§ã¯ãªã„
- æ–‡è„ˆä¸Šæ˜ã‚‰ã‹ã«äººç‰©ã‚’æŒ‡ã™å ´åˆã¯äººååˆ¤å®š
"""

            response = self.openai_client.chat.completions.create(
                model=os.getenv('OPENAI_MODEL', 'gpt-4'),  # ã‚ˆã‚Šé«˜æ€§èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
                messages=[
                    {'role': 'system', 'content': 'æ—¥æœ¬æ–‡å­¦ãƒ»åœ°ç†ãƒ»è¨€èªå­¦ã®å°‚é–€å®¶ã¨ã—ã¦ã€æ–‡è±ªä½œå“ä¸­ã®åœ°åã‚’æ­£ç¢ºã«åˆ¤åˆ¥ã—ã¦ãã ã•ã„ã€‚æ–‡è„ˆã‚’æ·±ãç†è§£ã—ã€èª¤åˆ¤å®šã‚’é¿ã‘ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚'},
                    {'role': 'user', 'content': prompt}
                ],
                max_tokens=500,
                temperature=0.1
            )
            
            response_text = response.choices[0].message.content.strip()
            
            # JSONè§£æ
            try:
                if '```json' in response_text:
                    json_start = response_text.find('```json') + 7
                    json_end = response_text.find('```', json_start)
                    response_text = response_text[json_start:json_end].strip()
                elif '```' in response_text:
                    json_start = response_text.find('```') + 3
                    json_end = response_text.find('```', json_start)
                    response_text = response_text[json_start:json_end].strip()
                    
                result = json.loads(response_text)
                
                if isinstance(result, dict) and 'is_place_name' in result:
                    return result
                else:
                    logger.warning(f"AIå¿œç­”å½¢å¼ã‚¨ãƒ©ãƒ¼: {response_text}")
                    return None
                    
            except json.JSONDecodeError:
                logger.warning(f"AIå¿œç­”JSONè§£æã‚¨ãƒ©ãƒ¼: {response_text}")
                return None
            
        except Exception as e:
            logger.error(f"å¼·åŒ–AIåˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None

    def _calculate_overall_verdict(self, ai_analyses: List[Dict[str, any]]) -> Dict[str, any]:
        """è¤‡æ•°ã®AIåˆ†æçµæœã‹ã‚‰ç·åˆåˆ¤å®šã‚’è¨ˆç®—"""
        if not ai_analyses:
            return {'is_valid': False, 'confidence': 0.0, 'recommendation': 'ä¸æ˜'}
        
        # åœ°ååˆ¤å®šã®é›†è¨ˆ
        place_votes = sum(1 for analysis in ai_analyses if analysis.get('is_place_name', False))
        total_votes = len(ai_analyses)
        
        # ä¿¡é ¼åº¦ã®å¹³å‡
        avg_confidence = sum(analysis.get('confidence', 0.0) for analysis in ai_analyses) / total_votes  
        
        # ç·åˆåˆ¤å®š
        is_valid = place_votes / total_votes >= 0.5
        
        if place_votes == 0:
            recommendation = "å‰Šé™¤æ¨å¥¨"
            final_confidence = avg_confidence
        elif place_votes / total_votes < 0.3:
            recommendation = "å‰Šé™¤æ¤œè¨"
            final_confidence = avg_confidence * 0.8
        elif place_votes / total_votes < 0.7:
            recommendation = "è¦å†æ¤œè¨"
            final_confidence = avg_confidence * 0.9
        else:
            recommendation = "ä¿æŒæ¨å¥¨"
            final_confidence = avg_confidence
        
        # åˆ¤å®šç†ç”±ã®çµ±åˆ
        reasoning_summary = []
        place_types = [analysis.get('place_type', '') for analysis in ai_analyses]
        most_common_type = max(set(place_types), key=place_types.count) if place_types else 'ä¸æ˜'
        
        return {
            'is_valid': is_valid,
            'confidence': final_confidence,
            'recommendation': recommendation,
            'place_name_ratio': place_votes / total_votes,
            'most_common_type': most_common_type,
            'analysis_count': total_votes,
            'detailed_analyses': ai_analyses
        }

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    import argparse
    
    parser = argparse.ArgumentParser(description='AIæ–‡è„ˆåˆ¤æ–­å‹Geocodingã‚·ã‚¹ãƒ†ãƒ ï¼ˆLegacyçµ±åˆç‰ˆï¼‰')
    parser.add_argument('--limit', type=int, help='å‡¦ç†ã™ã‚‹åœ°åæ•°ã®ä¸Šé™')
    parser.add_argument('--stats-only', action='store_true', help='çµ±è¨ˆæƒ…å ±ã®ã¿è¡¨ç¤º')
    parser.add_argument('--cleanup', action='store_true', help='ç„¡åŠ¹åœ°åã®è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ')
    parser.add_argument('--cleanup-preview', action='store_true', help='ç„¡åŠ¹åœ°åã®å‰Šé™¤å€™è£œã‚’è¡¨ç¤ºï¼ˆå®Ÿè¡Œãªã—ï¼‰')
    parser.add_argument('--delete', nargs='+', help='æŒ‡å®šã—ãŸåœ°åã‚’å‰Šé™¤')
    parser.add_argument('--analyze', type=str, help='æŒ‡å®šã—ãŸåœ°åã®ä½¿ç”¨çŠ¶æ³ã‚’è©³ç´°åˆ†æ')
    parser.add_argument('--ai-verify', action='store_true', help='AIå¤§é‡æ¤œè¨¼ã‚’å®Ÿè¡Œ')
    parser.add_argument('--ai-verify-limit', type=int, default=20, help='AIæ¤œè¨¼ã™ã‚‹åœ°åæ•°ã®ä¸Šé™ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 20ï¼‰')
    parser.add_argument('--confidence-threshold', type=float, default=0.7, help='å‰Šé™¤å€™è£œã¨ã™ã‚‹ä¿¡é ¼åº¦ã®é–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.7ï¼‰')
    
    args = parser.parse_args()
    
    geocoder = ContextAwareGeocoder()
    
    # çµ±è¨ˆæƒ…å ±è¡¨ç¤º
    print("=== ğŸ¤– AIæ–‡è„ˆåˆ¤æ–­å‹Geocodingã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆï¼ˆLegacyçµ±åˆç‰ˆï¼‰===")
    stats = geocoder.get_geocoding_statistics()
    print(f"ç·åœ°åæ•°: {stats['total_places']:,}")
    print(f"Geocodingæ¸ˆã¿åœ°åæ•°: {stats['geocoded_places']:,}")
    print(f"æœªGeocodingåœ°åæ•°: {stats['geocoded_places'] - stats['total_places']:,}")
    print(f"Geocodingç‡: {stats['geocoding_rate']:.1f}%")
    
    if stats.get('source_stats'):
        print("\nğŸ”§ Geocodingã‚½ãƒ¼ã‚¹:")
        for source, count in stats['source_stats'].items():
            print(f"  {source}: {count}ä»¶")
    
    if stats.get('confidence_distribution'):
        print("\nğŸ“Š ä¿¡é ¼åº¦åˆ†å¸ƒ:")
        for confidence_level, count in stats['confidence_distribution'].items():
            print(f"  {confidence_level}: {count}ä»¶")
    
    # åœ°åä½¿ç”¨çŠ¶æ³åˆ†æ
    if args.analyze:
        print(f"\n=== ğŸ” åœ°åä½¿ç”¨çŠ¶æ³åˆ†æ: {args.analyze} ===")
        analysis_result = geocoder.get_place_usage_analysis(args.analyze)
        
        if "error" in analysis_result:
            print(f"âŒ {analysis_result['error']}")
            return
        
        place_data = analysis_result["place_data"]
        print(f"ğŸ“ åœ°åæƒ…å ±:")
        print(f"   ID: {place_data['place_id']}")
        print(f"   åœ°å: {place_data['place_name']}")
        print(f"   ç¨®åˆ¥: {place_data['place_type']}")
        print(f"   åº§æ¨™: ({place_data['latitude']}, {place_data['longitude']})")
        print(f"   ä¿¡é ¼åº¦: {place_data['confidence']}")
        print(f"   ã‚½ãƒ¼ã‚¹: {place_data['source_system']}")
        
        print(f"\nğŸ“Š ä½¿ç”¨çµ±è¨ˆ:")
        print(f"   ä½¿ç”¨å›æ•°: {analysis_result['usage_count']}å›")
        print(f"   æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {analysis_result['recommended_action']}")
        
        print(f"\nğŸ“ ä½¿ç”¨ä¾‹:")
        for i, context in enumerate(analysis_result["context_analyses"][:3]):
            print(f"   ä¾‹{i+1}: {context['sentence'][:100]}...")
            print(f"        åœ°ååˆ¤å®š: {context['is_place_name']} (ä¿¡é ¼åº¦: {context['confidence']:.2f})")
            print(f"        ç†ç”±: {context['reasoning']}")
            print()
        
        return
    
    # å‰Šé™¤å€™è£œãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
    if args.cleanup_preview:
        print("\n=== ğŸ” ç„¡åŠ¹åœ°åå‰Šé™¤å€™è£œãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ ===")
        cleanup_result = geocoder.cleanup_invalid_places(auto_confirm=False)
        
        if not cleanup_result["candidates"]:
            print("âœ… å‰Šé™¤å€™è£œã®ç„¡åŠ¹åœ°åã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        print(f"ğŸ“Š å‰Šé™¤å€™è£œ: {len(cleanup_result['candidates'])}ä»¶")
        for candidate in cleanup_result["candidates"]:
            print(f"   ğŸ—‘ï¸ {candidate['place_name']}")
            print(f"      ç†ç”±: {candidate['reason']}")
            print(f"      ä½¿ç”¨å›æ•°: {candidate['usage_count']}å›")
            print(f"      ä¾‹: {candidate['sample']}")
            print()
        
        print("ğŸ’¡ å‰Šé™¤ã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ --cleanup ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„")
        return
    
    # è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ
    if args.cleanup:
        print("\n=== ğŸ—‘ï¸ ç„¡åŠ¹åœ°åè‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ ===")
        cleanup_result = geocoder.cleanup_invalid_places(auto_confirm=True)
        
        if cleanup_result["deletion_result"]["total_deleted"] > 0:
            print(f"âœ… {cleanup_result['deletion_result']['total_deleted']}ä»¶ã®ç„¡åŠ¹åœ°åã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
            for deleted in cleanup_result["deletion_result"]["deleted_places"]:
                print(f"   ğŸ—‘ï¸ {deleted['place_name']} (ç†ç”±: {deleted['reason']})")
        else:
            print("âœ… å‰Šé™¤å¯¾è±¡ã®ç„¡åŠ¹åœ°åã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        
        return
    
    # æŒ‡å®šåœ°åå‰Šé™¤
    if args.delete:
        print(f"\n=== ğŸ—‘ï¸ æŒ‡å®šåœ°åå‰Šé™¤: {', '.join(args.delete)} ===")
        deletion_result = geocoder.delete_invalid_places(args.delete, "æ‰‹å‹•å‰Šé™¤")
        
        if deletion_result["total_deleted"] > 0:
            print(f"âœ… {deletion_result['total_deleted']}ä»¶ã®åœ°åã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
            for deleted in deletion_result["deleted_places"]:
                print(f"   ğŸ—‘ï¸ {deleted['place_name']} (é–¢é€£: {deleted['deleted_relations']}ä»¶)")
        
        if deletion_result["not_found_places"]:
            print(f"âš ï¸ è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸåœ°å: {', '.join(deletion_result['not_found_places'])}")
        
        return
    
    # AIå¤§é‡æ¤œè¨¼
    if args.ai_verify:
        print(f"\n=== ğŸ¤– AIå¤§é‡æ¤œè¨¼é–‹å§‹ (ä¸Šé™: {args.ai_verify_limit}ä»¶, ä¿¡é ¼åº¦é–¾å€¤: {args.confidence_threshold}) ===")
        verification_result = geocoder.ai_mass_verification(
            limit=args.ai_verify_limit, 
            confidence_threshold=args.confidence_threshold
        )
        
        if "error" in verification_result:
            print(f"âŒ {verification_result['error']}")
            return
        
        print(f"\nğŸ“Š AIæ¤œè¨¼çµæœ:")
        print(f"å‡¦ç†æ¸ˆã¿: {verification_result['total_processed']}ä»¶")
        print(f"æ¤œè¨¼æ¸ˆã¿: {len(verification_result['verified_places'])}ä»¶")
        print(f"å‰Šé™¤å€™è£œ: {len(verification_result['deletion_candidates'])}ä»¶")
        print(f"AIã‚¨ãƒ©ãƒ¼: {verification_result['ai_errors']}ä»¶")
        
        if verification_result['deletion_candidates']:
            print(f"\nğŸ—‘ï¸ å‰Šé™¤å€™è£œåœ°å:")
            for candidate in verification_result['deletion_candidates'][:10]:  # ä¸Šä½10ä»¶è¡¨ç¤º
                verdict = candidate['overall_verdict']
                print(f"   âŒ {candidate['place_name']:12} (ä½¿ç”¨{candidate['usage_count']:2d}å›)")
                print(f"      AIåˆ¤å®š: {verdict['most_common_type']} | åœ°åç‡: {verdict['place_name_ratio']:.2f}")
                print(f"      æ¨å¥¨: {verdict['recommendation']} | ç¢ºä¿¡åº¦: {verdict['confidence']:.2f}")
                if verdict['detailed_analyses']:
                    first_analysis = verdict['detailed_analyses'][0]
                    print(f"      ç†ç”±: {first_analysis.get('reasoning', 'ä¸æ˜')[:100]}...")
                print()
            
            if len(verification_result['deletion_candidates']) > 10:
                print(f"   ... ä»– {len(verification_result['deletion_candidates']) - 10}ä»¶")
            
            # å‰Šé™¤ç¢ºèª
            print("ğŸ’¡ ã“ã‚Œã‚‰ã®åœ°åã‚’å‰Šé™¤ã—ã¾ã™ã‹ï¼Ÿ (y/N): ", end="")
            try:
                user_input = input().strip().lower()
                if user_input in ['y', 'yes']:
                    delete_names = [candidate['place_name'] for candidate in verification_result['deletion_candidates']]
                    deletion_result = geocoder.delete_invalid_places(delete_names, "AIæ¤œè¨¼ã«ã‚ˆã‚‹å‰Šé™¤")
                    print(f"âœ… {deletion_result['total_deleted']}ä»¶ã®åœ°åã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
                else:
                    print("ğŸ”„ å‰Šé™¤ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
            except (KeyboardInterrupt, EOFError):
                print("\nğŸ”„ å‰Šé™¤ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
        
        return
    
    if args.stats_only:
        return
    
    # Geocodingå‡¦ç†å®Ÿè¡Œ
    print("\n=== ğŸš€ AIæ–‡è„ˆåˆ¤æ–­å‹Geocodingå‡¦ç†é–‹å§‹ ===")
    result = geocoder.geocode_places_batch(limit=args.limit)
    
    print("\n=== ğŸ“Š å‡¦ç†çµæœ ===")
    print(f"å‡¦ç†åœ°åæ•°: {result['processed_places']:,}")
    print(f"GeocodingæˆåŠŸ: {result['geocoded_places']:,}")
    print(f"ã‚¹ã‚­ãƒƒãƒ—: {result['skipped_places']:,}")
    print(f"ã‚¨ãƒ©ãƒ¼æ•°: {result['errors']:,}")
    
    # æ›´æ–°å¾Œçµ±è¨ˆ
    if result['processed_places'] > 0:
        print("\n=== ğŸ“ˆ æ›´æ–°å¾Œçµ±è¨ˆ ===")
        final_stats = geocoder.get_geocoding_statistics()
        print(f"ç·åœ°åæ•°: {final_stats['total_places']:,}")
        print(f"Geocodingæ¸ˆã¿åœ°åæ•°: {final_stats['geocoded_places']:,}")
        print(f"Geocodingç‡: {final_stats['geocoding_rate']:.1f}%")
        
        if final_stats.get('confidence_distribution'):
            print("\nğŸ“Š ä¿¡é ¼åº¦åˆ†å¸ƒï¼ˆæ›´æ–°å¾Œï¼‰:")
            for confidence_level, count in final_stats['confidence_distribution'].items():
                print(f"  {confidence_level}: {count}ä»¶")

if __name__ == "__main__":
    main() 