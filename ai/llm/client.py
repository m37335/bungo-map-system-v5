"""
æ–‡è±ªã‚†ã‹ã‚Šåœ°å›³ã‚·ã‚¹ãƒ†ãƒ  - LLM ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ

OpenAI API ã¨ã®æ¥ç¶šã€ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ã‚’æä¾›
"""

import os
import json
import time
import hashlib
import logging
from typing import Dict, List, Optional, Any
from threading import Lock
from pathlib import Path
from dataclasses import dataclass
from dotenv import load_dotenv

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

load_dotenv()
logger = logging.getLogger(__name__)

@dataclass
class LLMRequest:
    """LLM ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    prompt: str
    model: str = "gpt-3.5-turbo"
    max_tokens: int = 1000
    temperature: float = 0.1
    cache_key: Optional[str] = None

@dataclass
class LLMResponse:
    """LLM ãƒ¬ã‚¹ãƒãƒ³ã‚¹"""
    content: str
    model: str
    tokens_used: int
    cached: bool = False
    error: Optional[str] = None

class RateLimiter:
    """API ãƒ¬ãƒ¼ãƒˆåˆ¶é™ç®¡ç†"""
    
    def __init__(self):
        self._limits = {
            'openai': {'last_call': 0, 'min_interval': 1.0},
            'google_maps': {'last_call': 0, 'min_interval': 0.1}
        }
        self._lock = Lock()
    
    def wait_if_needed(self, api_name: str, min_interval: float = None) -> None:
        """å¿…è¦ã«å¿œã˜ã¦å¾…æ©Ÿ"""
        if min_interval is None:
            min_interval = self._limits.get(api_name, {}).get('min_interval', 1.0)
        
        with self._lock:
            current_time = time.time()
            last_call = self._limits.get(api_name, {}).get('last_call', 0)
            time_since_last = current_time - last_call
            
            if time_since_last < min_interval:
                sleep_time = min_interval - time_since_last
                logger.info(f"ğŸ•’ {api_name} ãƒ¬ãƒ¼ãƒˆåˆ¶é™: {sleep_time:.2f}ç§’å¾…æ©Ÿ")
                time.sleep(sleep_time)
            
            self._limits.setdefault(api_name, {})['last_call'] = time.time()

class APICache:
    """API ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†"""
    
    def __init__(self, cache_file: str = "data/api_cache.json"):
        self.cache_file = Path(cache_file)
        self._cache = {}
        self._lock = Lock()
        self._load_cache()
    
    def _load_cache(self) -> None:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿"""
        try:
            if self.cache_file.exists():
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    self._cache = json.load(f)
                logger.info(f"ğŸ“ APIã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿: {len(self._cache)}ä»¶")
        except Exception as e:
            logger.warning(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            self._cache = {}
    
    def _save_cache(self) -> None:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        try:
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
            self.cache_file.parent.mkdir(parents=True, exist_ok=True)
            
            with self._lock:
                with open(self.cache_file, 'w', encoding='utf-8') as f:
                    json.dump(self._cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.warning(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def get_cache_key(self, text: str, api_type: str) -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ"""
        content = f"{api_type}:{text}"
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def get(self, key: str) -> Optional[Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—"""
        return self._cache.get(key)
    
    def set(self, key: str, value: Any) -> None:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        self._cache[key] = value
        self._save_cache()
    
    def clear(self) -> None:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        self._cache.clear()
        self._save_cache()
    
    def size(self) -> int:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚º"""
        return len(self._cache)

class LLMClient:
    """LLM ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆOpenAI APIï¼‰"""
    
    def __init__(self, api_key: Optional[str] = None, cache_file: str = "data/api_cache.json"):
        """åˆæœŸåŒ–"""
        self.api_key = api_key or os.getenv('OPENAI_API_KEY')
        self.client = None
        self.enabled = False
        
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆæœŸåŒ–
        self.rate_limiter = RateLimiter()
        self.cache = APICache(cache_file)
        
        # OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
        self._init_client()
    
    def _init_client(self) -> None:
        """OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–"""
        if not OPENAI_AVAILABLE:
            logger.warning("âš ï¸ OpenAI ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return
        
        if not self.api_key:
            logger.warning("âš ï¸ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return
        
        try:
            self.client = openai.OpenAI(api_key=self.api_key)
            self.enabled = True
            logger.info("âœ… OpenAI APIæ¥ç¶šæº–å‚™å®Œäº†")
        except Exception as e:
            logger.error(f"âŒ OpenAI APIåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    
    def is_available(self) -> bool:
        """APIåˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯"""
        return self.enabled and self.client is not None
    
    def generate(self, request: LLMRequest) -> LLMResponse:
        """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆåŒæœŸç‰ˆï¼‰"""
        if not self.is_available():
            return LLMResponse(
                content="",
                model=request.model,
                tokens_used=0,
                error="OpenAI API ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"
            )
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        cache_key = request.cache_key or self.cache.get_cache_key(request.prompt, "openai")
        cached_result = self.cache.get(cache_key)
        
        if cached_result:
            logger.info("ğŸ“ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ")
            return LLMResponse(
                content=cached_result.get("content", ""),
                model=cached_result.get("model", request.model),
                tokens_used=cached_result.get("tokens_used", 0),
                cached=True
            )
        
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒã‚§ãƒƒã‚¯
        self.rate_limiter.wait_if_needed("openai")
        
        try:
            # API å‘¼ã³å‡ºã—
            response = self.client.chat.completions.create(
                model=request.model,
                messages=[{"role": "user", "content": request.prompt}],
                max_tokens=request.max_tokens,
                temperature=request.temperature
            )
            
            content = response.choices[0].message.content
            tokens_used = response.usage.total_tokens
            
            # çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
            cache_data = {
                "content": content,
                "model": request.model,
                "tokens_used": tokens_used,
                "timestamp": time.time()
            }
            self.cache.set(cache_key, cache_data)
            
            logger.info(f"ğŸ¤– OpenAI APIæˆåŠŸ: {tokens_used} tokens")
            
            return LLMResponse(
                content=content,
                model=request.model,
                tokens_used=tokens_used,
                cached=False
            )
            
        except Exception as e:
            logger.error(f"âŒ OpenAI API ã‚¨ãƒ©ãƒ¼: {e}")
            return LLMResponse(
                content="",
                model=request.model,
                tokens_used=0,
                error=str(e)
            )
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ"""
        return {
            "cache_size": self.cache.size(),
            "cache_file": str(self.cache.cache_file),
            "api_available": self.is_available()
        } 