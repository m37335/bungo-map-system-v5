#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡è±ªã‚†ã‹ã‚Šåœ°å›³ã‚·ã‚¹ãƒ†ãƒ  v4.0 - çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œå™¨
åœ°åãƒã‚¹ã‚¿ãƒ¼å„ªå…ˆè¨­è¨ˆã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 

ä½¿ç”¨ä¾‹:
    python3 run_pipeline.py --author "æ¢¶äº• åŸºæ¬¡éƒ"
    python3 run_pipeline.py --status "æ¢¶äº• åŸºæ¬¡éƒ"
    python3 run_pipeline.py --ai-verify
    python3 run_pipeline.py --ai-verify-delete
"""

import sys
import os
import argparse
import time
from datetime import datetime
from typing import List, Dict, Any

# ãƒ‘ã‚¹è¨­å®š
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼ã‚·ã‚¹ãƒ†ãƒ 
from extractors.processors import CompleteAuthorProcessor
from extractors.places.enhanced_place_extractor import EnhancedPlaceExtractorV3
from extractors.places.place_master_manager import PlaceMasterManagerV2
from ai.llm import LLMClient
from ai.nlp import ContextAnalyzer
from ai.geocoding import GeocodingEngine
from extractors.wikipedia import WikipediaAuthorEnricher
from database.sentence_places_enricher import SentencePlacesEnricher
from extractors.aozora import AozoraMetadataExtractor
from extractors.aozora.author_list_scraper import AuthorListScraper

class BungoPipeline:
    """æ–‡è±ªåœ°å›³ã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆåœ°åãƒã‚¹ã‚¿ãƒ¼å„ªå…ˆç‰ˆï¼‰"""
    
    def __init__(self):
        print("ğŸš€ æ–‡è±ªã‚†ã‹ã‚Šåœ°å›³ã‚·ã‚¹ãƒ†ãƒ  v4.0 - ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–ä¸­...")
        print("âœ¨ åœ°åãƒã‚¹ã‚¿ãƒ¼å„ªå…ˆè¨­è¨ˆã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªå‡¦ç†")
        
        self.author_processor = CompleteAuthorProcessor()
        self.place_extractor = EnhancedPlaceExtractorV3()
        self.place_master_manager = PlaceMasterManagerV2()
        
        # æ–°ã—ã„ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼AIã‚·ã‚¹ãƒ†ãƒ 
        self.llm_client = LLMClient()
        self.context_analyzer = ContextAnalyzer()
        self.geocoding_engine = GeocodingEngine(self.llm_client)
        
        self.wikipedia_enricher = WikipediaAuthorEnricher()
        self.sentence_places_enricher = SentencePlacesEnricher()
        self.metadata_extractor = AozoraMetadataExtractor()
        
        # é’ç©ºæ–‡åº«URLè‡ªå‹•å–å¾—æ©Ÿèƒ½
        self.author_list_scraper = AuthorListScraper()
        
        print("âœ… ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å®Œäº†")
        print("ğŸ¯ åœ°åãƒã‚¹ã‚¿ãƒ¼æ¤œç´¢ â†’ é‡è¤‡ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å›é¿ â†’ APIåŠ¹ç‡åŒ–")
    
    def check_and_set_aozora_url(self, author_name: str) -> bool:
        """é’ç©ºæ–‡åº«URLç¢ºèªãƒ»è‡ªå‹•è¨­å®š"""
        try:
            import sqlite3
            db_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'data', 'bungo_map.db')
            
            # ç¾åœ¨ã®URLçŠ¶æ³ç¢ºèª
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT author_id, author_name, aozora_author_url
                FROM authors 
                WHERE author_name = ?
            """, (author_name,))
            
            result = cursor.fetchone()
            conn.close()
            
            if not result:
                print(f"âŒ ä½œè€… {author_name} ãŒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return False
            
            author_id, db_author_name, current_url = result
            
            # é’ç©ºæ–‡åº«URLãŒæ—¢ã«è¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ
            if current_url and current_url.strip():
                print(f"âœ… é’ç©ºæ–‡åº«URLè¨­å®šæ¸ˆã¿: {current_url}")
                return True
            
            # é’ç©ºæ–‡åº«URLãŒæœªè¨­å®šã®å ´åˆã€è‡ªå‹•å–å¾—
            print(f"ğŸ” é’ç©ºæ–‡åº«URLæœªè¨­å®š - è‡ªå‹•å–å¾—é–‹å§‹: {author_name}")
            
            success = self.author_list_scraper.update_single_author_url(author_name)
            
            if success:
                print(f"âœ… é’ç©ºæ–‡åº«URLè‡ªå‹•è¨­å®šå®Œäº†")
                return True
            else:
                print(f"âš ï¸ é’ç©ºæ–‡åº«URLå–å¾—å¤±æ•— - ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç¶™ç¶šï¼ˆä¸€éƒ¨åˆ¶é™ã‚ã‚Šï¼‰")
                return False
                
        except Exception as e:
            print(f"âŒ é’ç©ºæ–‡åº«URLãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def run_full_pipeline(self, author_name: str, include_places: bool = True, 
                         include_geocoding: bool = True, include_maintenance: bool = True) -> Dict[str, Any]:
        """å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œï¼ˆåœ°åãƒã‚¹ã‚¿ãƒ¼å„ªå…ˆç‰ˆï¼‰"""
        start_time = datetime.now()
        print(f"\nğŸŒŸ æ–‡è±ªã‚†ã‹ã‚Šåœ°å›³ã‚·ã‚¹ãƒ†ãƒ  - å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹")
        print(f"ğŸ‘¤ å¯¾è±¡ä½œè€…: {author_name}")
        print(f"ğŸ¯ åœ°åãƒã‚¹ã‚¿ãƒ¼å„ªå…ˆè¨­è¨ˆã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªå‡¦ç†")
        print("=" * 80)
        
        results = {
            'author': author_name,
            'start_time': start_time,
            'success': False,
            'works_processed': 0,
            'sentences_created': 0,
            'places_extracted': 0,
            'places_saved': 0,
            'places_geocoded': 0,
            'geocoding_skipped': 0,
            'geocoding_success_rate': 0.0,
            'sentences_processed': 0,
            'master_cache_hits': 0,
            'new_masters_created': 0,
            'aozora_url_status': False,
            'errors': []
        }
        
        try:
            # äº‹å‰ãƒã‚§ãƒƒã‚¯: é’ç©ºæ–‡åº«URLç¢ºèªãƒ»è‡ªå‹•è¨­å®š
            print("ğŸ”„ äº‹å‰ãƒã‚§ãƒƒã‚¯: é’ç©ºæ–‡åº«URLç¢ºèª...")
            url_status = self.check_and_set_aozora_url(author_name)
            results['aozora_url_status'] = url_status
            
            if not url_status:
                print("âš ï¸ é’ç©ºæ–‡åº«URLå–å¾—å¤±æ•— - ä½œå“åé›†ã«åˆ¶é™ãŒã‹ã‹ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
            
            # ã‚¹ãƒ†ãƒƒãƒ—1: ä½œè€…ãƒ»ä½œå“å‡¦ç†
            print("\nğŸ”„ ã‚¹ãƒ†ãƒƒãƒ—1: ä½œè€…ä½œå“å‡¦ç†é–‹å§‹...")
            print("  ğŸ“š é’ç©ºæ–‡åº«ã‹ã‚‰ä½œå“åé›†")
            print("  ğŸ“„ æœ¬æ–‡å–å¾—ãƒ»ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†")
            print("  ğŸ“ ã‚»ãƒ³ãƒ†ãƒ³ã‚¹åˆ†å‰²ãƒ»ä¿å­˜")
            
            step1_result = self.author_processor.process_author_complete(author_name, content_processing=True)
            
            if step1_result and step1_result.get('success', False):
                results['works_processed'] = step1_result.get('works_collection', {}).get('new_works', 0)
                results['sentences_created'] = step1_result.get('content_processing', {}).get('total_sentences', 0)
                print(f"âœ… ã‚¹ãƒ†ãƒƒãƒ—1å®Œäº†: {results['works_processed']}ä½œå“ã€{results['sentences_created']:,}ã‚»ãƒ³ãƒ†ãƒ³ã‚¹")
            else:
                raise Exception("ä½œè€…ãƒ»ä½œå“å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
            # ã‚¹ãƒ†ãƒƒãƒ—2: åœ°åãƒã‚¹ã‚¿ãƒ¼å„ªå…ˆå‡¦ç†
            if include_places:
                print("\nğŸ”„ ã‚¹ãƒ†ãƒƒãƒ—2: åœ°åãƒã‚¹ã‚¿ãƒ¼å„ªå…ˆæŠ½å‡ºãƒ»å‡¦ç†é–‹å§‹...")
                print("  ğŸ” åœ°åæŠ½å‡º â†’ ãƒã‚¹ã‚¿ãƒ¼æ¤œç´¢")
                print("  âš¡ æ—¢å­˜åœ°å: ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‚ç…§ï¼ˆã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¹ã‚­ãƒƒãƒ—ï¼‰")
                print("  ğŸ†• æ–°è¦åœ°å: ãƒã‚¹ã‚¿ãƒ¼ä½œæˆ â†’ ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Ÿè¡Œ")
                print("  ğŸ¤– AIæ¤œè¨¼çµ±åˆã«ã‚ˆã‚‹å“è³ªä¿è¨¼")
                
                # ä½œè€…ã®worksã‚’å–å¾—ã—ã¦processing
                step2_result = self._process_author_places(author_name)
                
                results.update(step2_result)
                
                print(f"âœ… ã‚¹ãƒ†ãƒƒãƒ—2å®Œäº†:")
                print(f"  ğŸ“Š å‡¦ç†: {results['sentences_processed']}ã‚»ãƒ³ãƒ†ãƒ³ã‚¹")
                print(f"  ğŸ—ºï¸ æŠ½å‡º: {results['places_extracted']}åœ°å")
                print(f"  âš¡ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {results['master_cache_hits']}ä»¶")
                print(f"  ğŸ†• æ–°è¦ãƒã‚¹ã‚¿ãƒ¼: {results['new_masters_created']}ä»¶")
                print(f"  ğŸŒ ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {results['places_geocoded']}ä»¶")
                print(f"  ğŸ“ˆ æˆåŠŸç‡: {results['geocoding_success_rate']:.1f}%")
            
            # ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
            if include_maintenance:
                print("\nğŸ”„ ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼ãƒ»ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹...")
                print("  ğŸ‘¤ Wikipediaä½œè€…æƒ…å ±è‡ªå‹•è£œå®Œ")
                print("  ğŸ“ sentence_placesä½œè€…ãƒ»ä½œå“æƒ…å ±è£œå®Œ")
                print("  ğŸ“š worksãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è‡ªå‹•è£œå®Œ")
                print("  ğŸ“… å‡ºç‰ˆå¹´æƒ…å ±æ›´æ–°")
                print("  ğŸ—ï¸ åœ°åãƒã‚¹ã‚¿ãƒ¼çµ±è¨ˆæ›´æ–°")
                
                step3_result = self._run_data_quality_maintenance()
                results.update(step3_result)
                
                if step3_result['maintenance_success']:
                    print(f"âœ… ã‚¹ãƒ†ãƒƒãƒ—3å®Œäº†: ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼å‡¦ç†æ­£å¸¸å®Œäº†")
                else:
                    print(f"âš ï¸ ã‚¹ãƒ†ãƒƒãƒ—3è­¦å‘Š: ä¸€éƒ¨ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸãŒã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯ç¶™ç¶šã—ã¾ã™")
            else:
                print("\nâ­ï¸ ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼ãƒ»ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
            
            results['success'] = True
            
        except Exception as e:
            print(f"âŒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            results['errors'].append(str(e))
        
        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ
        end_time = datetime.now()
        results['end_time'] = end_time
        results['duration'] = (end_time - start_time).total_seconds()
        
        self._print_report(results)
        return results
    
    def _process_author_places(self, author_name: str) -> Dict[str, Any]:
        """ä½œè€…ã®åœ°åå‡¦ç†ï¼ˆåœ°åãƒã‚¹ã‚¿ãƒ¼å„ªå…ˆï¼‰"""
        try:
            # ä½œè€…ã®ä½œå“IDå–å¾—
            import sqlite3
            db_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'data', 'bungo_map.db')
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT w.work_id, w.title 
                FROM works w
                JOIN authors a ON w.author_id = a.author_id
                WHERE a.author_name = ?
                ORDER BY w.work_id
            """, (author_name,))
            
            works = cursor.fetchall()
            conn.close()
            
            if not works:
                print(f"âš ï¸ ä½œè€… {author_name} ã®ä½œå“ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return {}
            
            print(f"ğŸ“š å‡¦ç†å¯¾è±¡: {len(works)}ä½œå“")
            
            total_stats = {
                'sentences_processed': 0,
                'places_extracted': 0,
                'places_saved': 0,
                'places_geocoded': 0,
                'geocoding_skipped': 0,
                'geocoding_success_rate': 0.0,
                'master_cache_hits': 0,
                'new_masters_created': 0
            }
            
            # å„ä½œå“ã‚’å‡¦ç†
            for work_id, title in works:
                print(f"\nğŸ“– ä½œå“å‡¦ç†: {title}")
                
                work_stats = self.place_extractor.process_work_sentences(work_id, title)
                
                # çµ±è¨ˆçµ±åˆ
                total_stats['sentences_processed'] += work_stats.get('processed_sentences', 0)
                total_stats['places_extracted'] += work_stats.get('total_places', 0)
                
                # ãƒã‚¹ã‚¿ãƒ¼çµ±è¨ˆå–å¾—
                master_stats = self.place_master_manager.get_master_statistics()
                cache_stats = master_stats.get('cache_stats', {})
                
                total_stats['master_cache_hits'] += cache_stats.get('cache_hits', 0)
                total_stats['new_masters_created'] += cache_stats.get('new_masters', 0)
                total_stats['geocoding_skipped'] += cache_stats.get('geocoding_skipped', 0)
                total_stats['places_geocoded'] += cache_stats.get('geocoding_executed', 0)
            
            # æˆåŠŸç‡è¨ˆç®—
            if total_stats['places_extracted'] > 0:
                total_stats['geocoding_success_rate'] = (
                    total_stats['places_geocoded'] / 
                    max(1, total_stats['new_masters_created']) * 100
                )
            
            return total_stats
            
        except Exception as e:
            print(f"âŒ åœ°åå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def check_status(self, author_name: str):
        """ä½œè€…ã®å‡¦ç†çŠ¶æ³ç¢ºèªï¼ˆåœ°åãƒã‚¹ã‚¿ãƒ¼çµ±è¨ˆå«ã‚€ï¼‰"""
        print(f"ğŸ” {author_name} ã®å‡¦ç†çŠ¶æ³ç¢ºèª")
        print("=" * 60)
        try:
            status = self.author_processor.get_author_processing_status(author_name)
            
            # åŸºæœ¬çŠ¶æ³è¡¨ç¤º
            print(f"ğŸ‘¤ ä½œè€…: {status.get('author_name', 'N/A')}")
            print(f"ğŸ“š ä½œå“æ•°: {status.get('total_works', 0)}ä»¶")
            print(f"ğŸ“ ã‚»ãƒ³ãƒ†ãƒ³ã‚¹æ•°: {status.get('total_sentences', 0):,}ä»¶")
            print(f"ğŸ—ºï¸ åœ°åæ•°: {status.get('total_places', 0)}ä»¶")
            print(f"âœ… å‡¦ç†çŠ¶æ³: {status.get('status', 'N/A')}")
            
            # åœ°åãƒã‚¹ã‚¿ãƒ¼çµ±è¨ˆè¡¨ç¤º
            print(f"\nğŸ“Š åœ°åãƒã‚¹ã‚¿ãƒ¼çµ±è¨ˆ:")
            master_stats = self.place_master_manager.get_master_statistics()
            print(f"  ç·ãƒã‚¹ã‚¿ãƒ¼æ•°: {master_stats.get('total_masters', 0):,}")
            print(f"  ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¸ˆã¿: {master_stats.get('geocoded_masters', 0):,} ({master_stats.get('geocoding_rate', 0):.1f}%)")
            print(f"  ä½¿ç”¨å›æ•°è¨ˆ: {master_stats.get('total_usage', 0):,}")
            
        except Exception as e:
            print(f"âŒ çŠ¶æ³ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
    
    def ai_verify_places(self, limit: int = 20, confidence_threshold: float = 0.7, auto_delete: bool = False) -> Dict[str, Any]:
        """AIå¤§é‡æ¤œè¨¼å®Ÿè¡Œï¼ˆæ–°ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼ã‚·ã‚¹ãƒ†ãƒ å¯¾å¿œï¼‰"""
        print(f"ğŸ¤– AIå¤§é‡æ¤œè¨¼é–‹å§‹ (ä¸Šé™: {limit}ä»¶, ä¿¡é ¼åº¦é–¾å€¤: {confidence_threshold})")
        print("ğŸ¯ æ–°ã—ã„ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼AIã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªæ¤œè¨¼")
        
        # æ–°ã—ã„ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã§AIæ¤œè¨¼ã‚’å®Ÿè£…
        # æš«å®šçš„ã«åŸºæœ¬çš„ãªçµæœã‚’è¿”ã™
        result = {
            'processed_count': 0,
            'verified_count': 0,
            'delete_candidates': [],
            'statistics': {}
        }
        
        try:
            # TODO: æ–°ã—ã„AIãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½¿ã£ãŸæ¤œè¨¼ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…
            print("â„¹ï¸ AIæ¤œè¨¼æ©Ÿèƒ½ã¯æ–°ã—ã„ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã«ç§»è¡Œä¸­ã§ã™")
            print("   ç¾åœ¨ã¯åŸºæœ¬çš„ãªå‹•ä½œç¢ºèªã®ã¿å®Ÿè¡Œã•ã‚Œã¾ã™")
            
            if auto_delete:
                print("âœ… è‡ªå‹•å‰Šé™¤å¯¾è±¡ãªã—ï¼ˆç§»è¡Œä¸­ï¼‰")
        
        except Exception as e:
            print(f"âŒ AIæ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
            result['error'] = str(e)
        
        return result
    
    def get_master_statistics(self) -> Dict[str, Any]:
        """åœ°åãƒã‚¹ã‚¿ãƒ¼çµ±è¨ˆå–å¾—"""
        return self.place_master_manager.get_master_statistics()
    
    def print_master_statistics(self):
        """åœ°åãƒã‚¹ã‚¿ãƒ¼çµ±è¨ˆè¡¨ç¤º"""
        self.place_master_manager.print_statistics()
    
    def _run_data_quality_maintenance(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼ãƒ»ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹å‡¦ç†ï¼ˆåœ°åãƒã‚¹ã‚¿ãƒ¼å¯¾å¿œç‰ˆï¼‰"""
        maintenance_results = {
            'maintenance_success': True,
            'wikipedia_enriched_authors': 0,
            'enriched_sentence_places': 0,
            'enriched_works': 0,
            'updated_publication_years': 0,
            'master_statistics_updated': True,
            'maintenance_errors': []
        }
        
        try:
            # 1. Wikipediaä½œè€…æƒ…å ±è£œå®Œ
            try:
                wikipedia_result = self._enrich_wikipedia_author_info()
                maintenance_results['wikipedia_enriched_authors'] = wikipedia_result.get('enriched_count', 0)
            except Exception as e:
                maintenance_results['maintenance_errors'].append(f"Wikipediaä½œè€…æƒ…å ±è£œå®Œã‚¨ãƒ©ãƒ¼: {e}")
                maintenance_results['maintenance_success'] = False
            
            # 2. sentence_placesè£œå®Œ
            try:
                enrichment_result = self.sentence_places_enricher.run_full_enrichment()
                maintenance_results['enriched_sentence_places'] = enrichment_result.get('total_updates', 0)
            except Exception as e:
                maintenance_results['maintenance_errors'].append(f"sentence_placesè£œå®Œã‚¨ãƒ©ãƒ¼: {e}")
                maintenance_results['maintenance_success'] = False
            
            # 3. worksãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è£œå®Œ
            try:
                works_result = self._enrich_works_metadata()
                maintenance_results['enriched_works'] = works_result.get('enriched_count', 0)
            except Exception as e:
                maintenance_results['maintenance_errors'].append(f"worksãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è£œå®Œã‚¨ãƒ©ãƒ¼: {e}")
                maintenance_results['maintenance_success'] = False
            
            # 4. work_publication_yearæ›´æ–°
            try:
                publication_result = self._update_work_publication_years()
                maintenance_results['updated_publication_years'] = publication_result.get('updated_count', 0)
            except Exception as e:
                maintenance_results['maintenance_errors'].append(f"å‡ºç‰ˆå¹´æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
                maintenance_results['maintenance_success'] = False
                
        except Exception as e:
            maintenance_results['maintenance_errors'].append(f"ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹å‡¦ç†å…¨ä½“ã‚¨ãƒ©ãƒ¼: {e}")
            maintenance_results['maintenance_success'] = False
        
        return maintenance_results
    
    def _enrich_works_metadata(self) -> Dict[str, Any]:
        """worksãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è£œå®Œ"""
        import subprocess
        
        result = subprocess.run([
            'python3', 'extractors/enrich_works_metadata.py'
        ], capture_output=True, text=True, cwd=os.path.dirname(os.path.abspath(__file__)))
        
        if result.returncode == 0:
            enriched_count = 0
            output_lines = result.stdout.split('\n')
            for line in output_lines:
                if 'ä»¶å‡¦ç†å®Œäº†' in line:
                    try:
                        enriched_count = int(line.split('ä»¶å‡¦ç†å®Œäº†')[0].split()[-1])
                    except:
                        pass
            
            return {'enriched_count': enriched_count}
        else:
            raise Exception(f"worksãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è£œå®Œå¤±æ•—: {result.stderr}")
    
    def _update_work_publication_years(self) -> Dict[str, Any]:
        """work_publication_yearæ›´æ–°"""
        import subprocess
        
        result = subprocess.run([
            'python3', 'extractors/update_work_publication_year.py'
        ], capture_output=True, text=True, cwd=os.path.dirname(os.path.abspath(__file__)))
        
        if result.returncode == 0:
            updated_count = 0
            output_lines = result.stdout.split('\n')
            for line in output_lines:
                if 'ä»¶æ›´æ–°' in line:
                    try:
                        updated_count = int(line.split('ä»¶æ›´æ–°')[0].split()[-1])
                    except:
                        pass
            
            return {'updated_count': updated_count}
        else:
            raise Exception(f"å‡ºç‰ˆå¹´æ›´æ–°å¤±æ•—: {result.stderr}")
    
    def _enrich_wikipedia_author_info(self) -> Dict[str, Any]:
        """Wikipediaä½œè€…æƒ…å ±è£œå®Œ"""
        try:
            missing_info = self.wikipedia_enricher.preview_missing_info()
            recent_authors = missing_info.get('missing_authors', [])
            
            if not recent_authors:
                return {'enriched_count': 0, 'errors': []}
            
            target_authors = [author['author_name'] for author in recent_authors[:3]]
            enrichment_result = self.wikipedia_enricher.enrich_specific_authors(target_authors)
            
            return {
                'enriched_count': enrichment_result.get('success_count', 0),
                'errors': enrichment_result.get('errors', [])
            }
        except Exception as e:
            raise Exception(f"Wikipediaä½œè€…æƒ…å ±è£œå®Œå¤±æ•—: {e}")
    
    def _print_report(self, results: Dict[str, Any]):
        """ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤ºï¼ˆåœ°åãƒã‚¹ã‚¿ãƒ¼çµ±è¨ˆå«ã‚€ï¼‰"""
        print(f"\nğŸ‰ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ")
        print("=" * 80)
        print(f"ğŸ‘¤ ä½œè€…: {results['author']}")
        print(f"â±ï¸  å®Ÿè¡Œæ™‚é–“: {results['duration']:.1f}ç§’")
        print(f"ğŸ† çµæœ: {'æˆåŠŸ' if results['success'] else 'å¤±æ•—'}")
        print(f"ğŸ”— é’ç©ºæ–‡åº«URL: {'è¨­å®šæ¸ˆã¿' if results.get('aozora_url_status', False) else 'æœªè¨­å®š/å–å¾—å¤±æ•—'}")
        print(f"ğŸ“š å‡¦ç†ä½œå“: {results['works_processed']}ä»¶")
        print(f"ğŸ“ ç”Ÿæˆã‚»ãƒ³ãƒ†ãƒ³ã‚¹: {results['sentences_created']:,}ä»¶")
        print(f"ğŸ“„ åœ°åå‡¦ç†ã‚»ãƒ³ãƒ†ãƒ³ã‚¹: {results.get('sentences_processed', 0)}ä»¶")
        print(f"ğŸ—ºï¸  æŠ½å‡ºåœ°å: {results['places_extracted']}ä»¶")
        
        # åœ°åãƒã‚¹ã‚¿ãƒ¼çµ±è¨ˆ
        print(f"\nâš¡ åœ°åãƒã‚¹ã‚¿ãƒ¼åŠ¹ç‡æ€§:")
        print(f"  ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {results.get('master_cache_hits', 0)}ä»¶")
        print(f"  æ–°è¦ãƒã‚¹ã‚¿ãƒ¼ä½œæˆ: {results.get('new_masters_created', 0)}ä»¶")
        print(f"  ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¹ã‚­ãƒƒãƒ—: {results.get('geocoding_skipped', 0)}ä»¶")
        print(f"  ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Ÿè¡Œ: {results.get('places_geocoded', 0)}ä»¶")
        print(f"  ğŸ“ˆ æˆåŠŸç‡: {results.get('geocoding_success_rate', 0):.1f}%")
        
        # ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹çµæœ
        if 'maintenance_success' in results:
            print(f"\nğŸ”§ ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼çµæœ:")
            print(f"  ğŸ‘¤ Wikipediaä½œè€…æƒ…å ±è£œå®Œ: {results.get('wikipedia_enriched_authors', 0)}ä»¶")
            print(f"  ğŸ“ sentence_placesè£œå®Œ: {results.get('enriched_sentence_places', 0)}ä»¶")
            print(f"  ğŸ“š worksãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è£œå®Œ: {results.get('enriched_works', 0)}ä»¶")
            print(f"  ğŸ“… å‡ºç‰ˆå¹´æ›´æ–°: {results.get('updated_publication_years', 0)}ä»¶")
            print(f"  ğŸ† ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹çµæœ: {'æˆåŠŸ' if results.get('maintenance_success', False) else 'ä¸€éƒ¨ã‚¨ãƒ©ãƒ¼'}")
        
        # ã‚¨ãƒ©ãƒ¼è¡¨ç¤º
        if results['errors']:
            print(f"\nâŒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼: {len(results['errors'])}ä»¶")
            for error in results['errors']:
                print(f"  - {error}")
        
        if results.get('maintenance_errors'):
            print(f"\nâš ï¸ ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã‚¨ãƒ©ãƒ¼: {len(results['maintenance_errors'])}ä»¶")
            for error in results['maintenance_errors']:
                print(f"  - {error}")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(
        description='æ–‡è±ªã‚†ã‹ã‚Šåœ°å›³ã‚·ã‚¹ãƒ†ãƒ  v4.0 - çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # åŸºæœ¬å‡¦ç†
  python3 run_pipeline.py --author "æ¢¶äº• åŸºæ¬¡éƒ"
  python3 run_pipeline.py --author "å¤ç›® æ¼±çŸ³" --works-only
  python3 run_pipeline.py --author "èŠ¥å· é¾ä¹‹ä»‹" --no-geocoding
  python3 run_pipeline.py --author "å¤ªå®° æ²»" --no-maintenance
  python3 run_pipeline.py --status "æ¢¶äº• åŸºæ¬¡éƒ"
  
  # åœ°åç®¡ç†
  python3 run_pipeline.py --stats
  python3 run_pipeline.py --analyze "å±±é“"
  python3 run_pipeline.py --cleanup-preview
  python3 run_pipeline.py --cleanup
  python3 run_pipeline.py --delete "å…ˆæ—¥é£¯å³¶" "ä»Šé£¯å³¶" "å¤•æ–¹å±±"
  
  # Wikipediaä½œè€…æƒ…å ±è£œå®Œ
  python3 run_pipeline.py --enrich-preview
  python3 run_pipeline.py --enrich-authors
  python3 run_pipeline.py --enrich-specific "å¤ç›® æ¼±çŸ³" "èŠ¥å· é¾ä¹‹ä»‹"
  
  # AIæ¤œè¨¼æ©Ÿèƒ½
  python3 run_pipeline.py --ai-verify
  python3 run_pipeline.py --ai-verify-delete --ai-verify-limit 50
  python3 run_pipeline.py --ai-verify --ai-confidence-threshold 0.8
        """
    )
    
    # å‡¦ç†å¯¾è±¡
    parser.add_argument('--author', '-a', help='å˜ä¸€ä½œè€…å')
    parser.add_argument('--status', '-s', help='ä½œè€…ã®å‡¦ç†çŠ¶æ³ç¢ºèª')
    
    # å®Ÿè¡Œåˆ¶å¾¡
    parser.add_argument('--works-only', action='store_true', help='ä½œå“åé›†ã®ã¿ï¼ˆåœ°åæŠ½å‡ºãªã—ï¼‰')
    parser.add_argument('--no-geocoding', action='store_true', help='åœ°åæŠ½å‡ºã®ã¿ï¼ˆã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãªã—ï¼‰')
    parser.add_argument('--no-maintenance', action='store_true', help='ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼ãƒ»ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—')
    
    # åœ°åç®¡ç†æ©Ÿèƒ½
    parser.add_argument('--delete', nargs='+', help='æŒ‡å®šã—ãŸåœ°åã‚’å‰Šé™¤')
    parser.add_argument('--cleanup', action='store_true', help='ç„¡åŠ¹åœ°åã®è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ')
    parser.add_argument('--cleanup-preview', action='store_true', help='ç„¡åŠ¹åœ°åã®å‰Šé™¤å€™è£œã‚’è¡¨ç¤ºï¼ˆå®Ÿè¡Œãªã—ï¼‰')
    parser.add_argument('--analyze', type=str, help='æŒ‡å®šã—ãŸåœ°åã®ä½¿ç”¨çŠ¶æ³ã‚’è©³ç´°åˆ†æ')
    parser.add_argument('--stats', action='store_true', help='ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°çµ±è¨ˆè¡¨ç¤º')
    
    # Wikipediaä½œè€…æƒ…å ±è£œå®Œæ©Ÿèƒ½
    parser.add_argument('--enrich-authors', action='store_true', help='Wikipediaä½œè€…æƒ…å ±è‡ªå‹•è£œå®Œï¼ˆå…¨ä½œè€…ï¼‰')
    parser.add_argument('--enrich-preview', action='store_true', help='ä½œè€…æƒ…å ±ä¸è¶³çŠ¶æ³ã‚’ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤º')
    parser.add_argument('--enrich-specific', nargs='+', help='æŒ‡å®šä½œè€…ã®ã¿æƒ…å ±è£œå®Œï¼ˆè¤‡æ•°å¯ï¼‰')
    
    # AIæ¤œè¨¼æ©Ÿèƒ½
    parser.add_argument('--ai-verify', action='store_true', help='AIå¤§é‡æ¤œè¨¼ã‚’å®Ÿè¡Œ')
    parser.add_argument('--ai-verify-delete', action='store_true', help='AIæ¤œè¨¼ã§å‰Šé™¤å€™è£œã‚’è‡ªå‹•å‰Šé™¤')
    parser.add_argument('--ai-verify-limit', type=int, default=20, help='AIæ¤œè¨¼ã™ã‚‹åœ°åæ•°ã®ä¸Šé™ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 20ï¼‰')
    parser.add_argument('--ai-confidence-threshold', type=float, default=0.7, help='AIæ¤œè¨¼ã®ä¿¡é ¼åº¦é–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.7ï¼‰')
    
    args = parser.parse_args()
    
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–
    pipeline = BungoPipeline()
    
    # Wikipediaä½œè€…æƒ…å ±è£œå®Œãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
    if args.enrich_preview:
        print("=== ğŸ” ä½œè€…æƒ…å ±ä¸è¶³çŠ¶æ³ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ ===")
        missing_info = pipeline.wikipedia_enricher.preview_missing_info()
        return
    
    # Wikipediaä½œè€…æƒ…å ±è‡ªå‹•è£œå®Œï¼ˆå…¨ä½œè€…ï¼‰
    if args.enrich_authors:
        print("=== ğŸŒŸ Wikipediaä½œè€…æƒ…å ±è‡ªå‹•è£œå®Œï¼ˆå…¨ä½œè€…ï¼‰ ===")
        try:
            stats = pipeline.wikipedia_enricher.enrich_all_authors()
            pipeline.wikipedia_enricher.print_statistics()
        except KeyboardInterrupt:
            print("\nâš ï¸ å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        finally:
            pipeline.wikipedia_enricher.close()
        return
    
    # Wikipediaä½œè€…æƒ…å ±è£œå®Œï¼ˆæŒ‡å®šä½œè€…ã®ã¿ï¼‰
    if args.enrich_specific:
        print(f"=== ğŸ¯ Wikipediaä½œè€…æƒ…å ±è£œå®Œï¼ˆæŒ‡å®šä½œè€…: {', '.join(args.enrich_specific)}ï¼‰ ===")
        try:
            stats = pipeline.wikipedia_enricher.enrich_specific_authors(args.enrich_specific)
            pipeline.wikipedia_enricher.print_statistics()
        except KeyboardInterrupt:
            print("\nâš ï¸ å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        finally:
            pipeline.wikipedia_enricher.close()
        return
    
    # çµ±è¨ˆè¡¨ç¤º
    if args.stats:
        print("=== ğŸ“Š ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°çµ±è¨ˆæƒ…å ± ===")
        stats = pipeline.get_geocoding_stats()
        print(f"ç·åœ°åæ•°: {stats['total_places']:,}")
        print(f"Geocodingæ¸ˆã¿åœ°åæ•°: {stats['geocoded_places']:,}")
        print(f"Geocodingç‡: {stats['geocoding_rate']:.1f}%")
        if stats.get('source_stats'):
            print("\nğŸ”§ Geocodingã‚½ãƒ¼ã‚¹:")
            for source, count in stats['source_stats'].items():
                print(f"  {source}: {count}ä»¶")
        return
    
    # åœ°åä½¿ç”¨çŠ¶æ³åˆ†æ
    if args.analyze:
        analysis_result = pipeline.analyze_place_usage(args.analyze)
        if "error" in analysis_result:
            print(f"âŒ {analysis_result['error']}")
            return
        
        place_data = analysis_result["place_data"]
        print(f"=== ğŸ” åœ°åä½¿ç”¨çŠ¶æ³åˆ†æ: {args.analyze} ===")
        print(f"ğŸ“ åœ°åæƒ…å ±:")
        print(f"   ID: {place_data['place_id']}")
        print(f"   åœ°å: {place_data['place_name']}")
        print(f"   ç¨®åˆ¥: {place_data['place_type']}")
        print(f"   åº§æ¨™: ({place_data['latitude']}, {place_data['longitude']})")
        print(f"   ä¿¡é ¼åº¦: {place_data['confidence']}")
        print(f"   ã‚½ãƒ¼ã‚¹: {place_data['source_system']}")
        
        print(f"\nğŸ“Š ä½¿ç”¨çµ±è¨ˆ:")
        print(f"   ä½¿ç”¨å›æ•°: {analysis_result['usage_count']}å›")
        print(f"   æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {analysis_result['recommended_action']}")
        
        print(f"\nğŸ“ ä½¿ç”¨ä¾‹:")
        for i, context in enumerate(analysis_result["context_analyses"][:3]):
            print(f"   ä¾‹{i+1}: {context['sentence'][:100]}...")
            print(f"        åœ°ååˆ¤å®š: {context['is_place_name']} (ä¿¡é ¼åº¦: {context['confidence']:.2f})")
            print(f"        ç†ç”±: {context['reasoning']}")
        return
    
    # å‰Šé™¤å€™è£œãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
    if args.cleanup_preview:
        print("=== ğŸ” ç„¡åŠ¹åœ°åå‰Šé™¤å€™è£œãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ ===")
        cleanup_result = pipeline.cleanup_invalid_places(auto_confirm=False)
        
        if not cleanup_result["candidates"]:
            print("âœ… å‰Šé™¤å€™è£œã®ç„¡åŠ¹åœ°åã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        print(f"ğŸ“Š å‰Šé™¤å€™è£œ: {len(cleanup_result['candidates'])}ä»¶")
        for candidate in cleanup_result["candidates"]:
            print(f"   ğŸ—‘ï¸ {candidate['place_name']}")
            print(f"      ç†ç”±: {candidate['reason']}")
            print(f"      ä½¿ç”¨å›æ•°: {candidate['usage_count']}å›")
            print(f"      ä¾‹: {candidate['sample']}")
        
        print("ğŸ’¡ å‰Šé™¤ã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ --cleanup ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„")
        return
    
    # è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ
    if args.cleanup:
        print("=== ğŸ—‘ï¸ ç„¡åŠ¹åœ°åè‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ ===")
        cleanup_result = pipeline.cleanup_invalid_places(auto_confirm=True)
        
        if cleanup_result["deletion_result"]["total_deleted"] > 0:
            print(f"âœ… {cleanup_result['deletion_result']['total_deleted']}ä»¶ã®ç„¡åŠ¹åœ°åã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
            for deleted in cleanup_result["deletion_result"]["deleted_places"]:
                print(f"   ğŸ—‘ï¸ {deleted['place_name']} (ç†ç”±: {deleted['reason']})")
        else:
            print("âœ… å‰Šé™¤å¯¾è±¡ã®ç„¡åŠ¹åœ°åã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    # æŒ‡å®šåœ°åå‰Šé™¤
    if args.delete:
        print(f"=== ğŸ—‘ï¸ æŒ‡å®šåœ°åå‰Šé™¤: {', '.join(args.delete)} ===")
        deletion_result = pipeline.delete_invalid_places(args.delete, "æ‰‹å‹•å‰Šé™¤")
        
        if deletion_result["total_deleted"] > 0:
            print(f"âœ… {deletion_result['total_deleted']}ä»¶ã®åœ°åã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
            for deleted in deletion_result["deleted_places"]:
                print(f"   ğŸ—‘ï¸ {deleted['place_name']} (é–¢é€£: {deleted['deleted_relations']}ä»¶)")
        
        if deletion_result["not_found_places"]:
            print(f"âš ï¸ è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸåœ°å: {', '.join(deletion_result['not_found_places'])}")
        return
    
    # AIæ¤œè¨¼æ©Ÿèƒ½
    if args.ai_verify or args.ai_verify_delete:
        print(f"=== ğŸ¤– AIå¤§é‡æ¤œè¨¼ (ä¸Šé™: {args.ai_verify_limit}ä»¶, ä¿¡é ¼åº¦é–¾å€¤: {args.ai_confidence_threshold}) ===")
        ai_result = pipeline.ai_verify_places(
            limit=args.ai_verify_limit,
            confidence_threshold=args.ai_confidence_threshold,
            auto_delete=args.ai_verify_delete
        )
        return
    
    # å‡¦ç†çŠ¶æ³ç¢ºèª
    if args.status:
        pipeline.check_status(args.status)
        return
    
    # åœ°åæŠ½å‡ºã‚’å«ã‚€ã‹ã©ã†ã‹
    include_places = not args.works_only
    include_geocoding = not args.no_geocoding
    include_maintenance = not args.no_maintenance
    
    # å‡¦ç†å®Ÿè¡Œ
    if args.author:
        # å˜ä¸€ä½œè€…å‡¦ç†
        pipeline.run_full_pipeline(args.author, include_places, include_geocoding, include_maintenance)
    else:
        print("âŒ å‡¦ç†å¯¾è±¡ä½œè€…ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
        parser.print_help()

if __name__ == '__main__':
    main() 