#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡è±ªã‚†ã‹ã‚Šåœ°å›³ã‚·ã‚¹ãƒ†ãƒ  v4.0 - çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œå™¨
ä½œè€…æŒ‡å®šã§é’ç©ºæ–‡åº«â†’ã‚»ãƒ³ãƒ†ãƒ³ã‚¹åˆ†å‰²â†’åœ°åæŠ½å‡ºã¾ã§å®Œå…¨è‡ªå‹•åŒ–

ä½¿ç”¨ä¾‹:
    python3 run_pipeline.py --author "æ¢¶äº• åŸºæ¬¡éƒ"
    python3 run_pipeline.py --status "æ¢¶äº• åŸºæ¬¡éƒ"
"""

import sys
import os
import argparse
import time
from datetime import datetime
from typing import List, Dict, Any

# ãƒ‘ã‚¹è¨­å®š
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# æ—¢å­˜ã®å‡¦ç†ã‚¯ãƒ©ã‚¹
from extractors.process_complete_author import CompleteAuthorProcessor
from extractors.enhanced_place_extractor_v2 import EnhancedPlaceExtractorV2
from ai.context_aware_geocoding import ContextAwareGeocoder
from extractors.wikipedia_author_enricher import WikipediaAuthorEnricher
from database.sentence_places_enricher import SentencePlacesEnricher
from extractors.aozora_metadata_extractor import AozoraMetadataExtractor

class BungoPipeline:
    """æ–‡è±ªåœ°å›³ã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    def __init__(self):
        print("ğŸš€ æ–‡è±ªã‚†ã‹ã‚Šåœ°å›³ã‚·ã‚¹ãƒ†ãƒ  v4.0 - ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–ä¸­...")
        self.author_processor = CompleteAuthorProcessor()
        self.place_extractor = EnhancedPlaceExtractorV2()
        self.context_aware_geocoder = ContextAwareGeocoder()
        self.wikipedia_enricher = WikipediaAuthorEnricher()
        self.sentence_places_enricher = SentencePlacesEnricher()
        self.metadata_extractor = AozoraMetadataExtractor()
        print("âœ… ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å®Œäº†")
    
    def run_full_pipeline(self, author_name: str, include_places: bool = True, include_geocoding: bool = True, include_maintenance: bool = True) -> Dict[str, Any]:
        """å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        start_time = datetime.now()
        print(f"\nğŸŒŸ æ–‡è±ªã‚†ã‹ã‚Šåœ°å›³ã‚·ã‚¹ãƒ†ãƒ  - å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹")
        print(f"ğŸ‘¤ å¯¾è±¡ä½œè€…: {author_name}")
        print("=" * 80)
        
        results = {
            'author': author_name,
            'start_time': start_time,
            'success': False,
            'works_processed': 0,
            'sentences_created': 0,
            'places_extracted': 0,
            'places_saved': 0,
            'places_geocoded': 0,
            'geocoding_skipped': 0,
            'geocoding_success_rate': 0.0,
            'sentences_processed': 0,
            'errors': []
        }
        
        try:
            # ã‚¹ãƒ†ãƒƒãƒ—1: ä½œè€…ãƒ»ä½œå“å‡¦ç†
            print("ğŸ”„ ã‚¹ãƒ†ãƒƒãƒ—1: ä½œè€…ä½œå“å‡¦ç†é–‹å§‹...")
            print("  ğŸ“š é’ç©ºæ–‡åº«ã‹ã‚‰ä½œå“åé›†")
            print("  ğŸ“„ æœ¬æ–‡å–å¾—ãƒ»ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†")
            print("  ğŸ“ ã‚»ãƒ³ãƒ†ãƒ³ã‚¹åˆ†å‰²ãƒ»ä¿å­˜")
            
            step1_result = self.author_processor.process_author_complete(author_name, content_processing=True)
            
            if step1_result and step1_result.get('success', False):
                # æ­£ã—ã„çµæœã‹ã‚‰æƒ…å ±ã‚’å–å¾—
                results['works_processed'] = step1_result.get('works_collection', {}).get('new_works', 0)
                results['sentences_created'] = step1_result.get('content_processing', {}).get('total_sentences', 0)
                print(f"âœ… ã‚¹ãƒ†ãƒƒãƒ—1å®Œäº†: {results['works_processed']}ä½œå“ã€{results['sentences_created']:,}ã‚»ãƒ³ãƒ†ãƒ³ã‚¹")
            else:
                raise Exception("ä½œè€…ãƒ»ä½œå“å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ")
            
            # ã‚¹ãƒ†ãƒƒãƒ—2: åœ°åå‡¦ç†
            if include_places:
                if include_geocoding:
                    print("\nğŸ”„ ã‚¹ãƒ†ãƒƒãƒ—2: åœ°åæŠ½å‡ºâ†’AIæ–‡è„ˆåˆ¤æ–­å‹ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é–‹å§‹...")
                    print("  ğŸ—ºï¸  ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ã‹ã‚‰åœ°åæŠ½å‡ºï¼ˆå‰å¾Œæ–‡ä»˜ãï¼‰")
                    print("  ğŸ¤– AIæ–‡è„ˆåˆ†æã«ã‚ˆã‚‹ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°")
                    print("  ğŸŒ Google Maps APIçµ±åˆåº§æ¨™å–å¾—")
                    print("  ğŸ’¾ é«˜ç²¾åº¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜")
                    
                    # ã‚¹ãƒ†ãƒƒãƒ—2A: åœ°åæŠ½å‡º
                    step2a_result = self.place_extractor.process_sentences_batch()
                    results['sentences_processed'] = step2a_result.get('processed_sentences', 0)
                    results['places_extracted'] = step2a_result.get('extracted_places', 0)
                    results['places_saved'] = step2a_result.get('saved_places', 0)
                    
                    print(f"âœ… ã‚¹ãƒ†ãƒƒãƒ—2Aå®Œäº†: {results['sentences_processed']}ã‚»ãƒ³ãƒ†ãƒ³ã‚¹å‡¦ç†ã€{results['places_extracted']}åœ°åæŠ½å‡º")
                    
                    # ã‚¹ãƒ†ãƒƒãƒ—2B: AIæ–‡è„ˆåˆ¤æ–­å‹ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
                    if results['places_extracted'] > 0:
                        print("\nğŸ”„ ã‚¹ãƒ†ãƒƒãƒ—2B: AIæ–‡è„ˆåˆ¤æ–­å‹ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é–‹å§‹...")
                        step2b_result = self.context_aware_geocoder.geocode_places_batch()
                        
                        results['places_geocoded'] = step2b_result.get('geocoded_places', 0)
                        
                        if results['places_extracted'] > 0:
                            results['geocoding_success_rate'] = (results['places_geocoded'] / results['places_extracted']) * 100
                        else:
                            results['geocoding_success_rate'] = 0.0
                        
                        print(f"âœ… ã‚¹ãƒ†ãƒƒãƒ—2Bå®Œäº†: {results['places_geocoded']}ä»¶ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æˆåŠŸ ({results['geocoding_success_rate']:.1f}%)")
                    
                    print(f"âœ… ã‚¹ãƒ†ãƒƒãƒ—2çµ±åˆå®Œäº†: {results['sentences_processed']}ã‚»ãƒ³ãƒ†ãƒ³ã‚¹å‡¦ç†ã€{results['places_extracted']}åœ°åæŠ½å‡ºã€{results['places_geocoded']}ä»¶ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æˆåŠŸ ({results['geocoding_success_rate']:.1f}%)")
                else:
                    print("\nğŸ”„ ã‚¹ãƒ†ãƒƒãƒ—2: åœ°åæŠ½å‡ºé–‹å§‹...")
                    print("  ğŸ—ºï¸  ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ã‹ã‚‰åœ°åæŠ½å‡ºï¼ˆå‰å¾Œæ–‡ä»˜ãï¼‰")
                    
                    # åœ°åæŠ½å‡ºã®ã¿å®Ÿè¡Œ
                    step2_result = self.place_extractor.process_sentences_batch()
                    
                    results['sentences_processed'] = step2_result.get('processed_sentences', 0)
                    results['places_extracted'] = step2_result.get('extracted_places', 0)
                    results['places_saved'] = step2_result.get('saved_places', 0)
                    print(f"âœ… ã‚¹ãƒ†ãƒƒãƒ—2å®Œäº†: {results['sentences_processed']}ã‚»ãƒ³ãƒ†ãƒ³ã‚¹å‡¦ç†ã€{results['places_extracted']}åœ°åæŠ½å‡ºã€{results['places_saved']}ä»¶ä¿å­˜")
            
            # ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼ï¼ˆæ–°æ©Ÿèƒ½ï¼‰
            if include_maintenance:
                print("\nğŸ”„ ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼ãƒ»ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹...")
                print("  ğŸ“ sentence_placesä½œè€…ãƒ»ä½œå“æƒ…å ±è£œå®Œ")
                print("  ğŸ“š worksãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è‡ªå‹•è£œå®Œ")
                print("  ğŸ“… å‡ºç‰ˆå¹´æƒ…å ±æ›´æ–°")
                print("  ğŸ”§ matched_textæ–‡å…¨ä½“ä¿®æ­£")
                
                step3_result = self._run_data_quality_maintenance()
                results.update(step3_result)
                
                if step3_result['maintenance_success']:
                    print(f"âœ… ã‚¹ãƒ†ãƒƒãƒ—3å®Œäº†: ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼å‡¦ç†æ­£å¸¸å®Œäº†")
                else:
                    print(f"âš ï¸ ã‚¹ãƒ†ãƒƒãƒ—3è­¦å‘Š: ä¸€éƒ¨ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸãŒã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯ç¶™ç¶šã—ã¾ã™")
            else:
                print("\nâ­ï¸ ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼ãƒ»ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
            
            results['success'] = True
            
        except Exception as e:
            print(f"âŒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            results['errors'].append(str(e))
        
        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ
        end_time = datetime.now()
        results['end_time'] = end_time
        results['duration'] = (end_time - start_time).total_seconds()
        
        self._print_report(results)
        return results
    
    def check_status(self, author_name: str):
        """ä½œè€…ã®å‡¦ç†çŠ¶æ³ç¢ºèª"""
        print(f"ğŸ” {author_name} ã®å‡¦ç†çŠ¶æ³ç¢ºèª")
        print("=" * 60)
        try:
            status = self.author_processor.get_author_processing_status(author_name)
            
            # çŠ¶æ³è¡¨ç¤º
            print(f"ğŸ‘¤ ä½œè€…: {status.get('author_name', 'N/A')}")
            print(f"ğŸ“š ä½œå“æ•°: {status.get('total_works', 0)}ä»¶")
            print(f"ğŸ“ ã‚»ãƒ³ãƒ†ãƒ³ã‚¹æ•°: {status.get('total_sentences', 0):,}ä»¶")
            print(f"ğŸ—ºï¸ åœ°åæ•°: {status.get('total_places', 0)}ä»¶")
            print(f"âœ… å‡¦ç†çŠ¶æ³: {status.get('status', 'N/A')}")
            
        except Exception as e:
            print(f"âŒ çŠ¶æ³ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
    
    def delete_invalid_places(self, place_names: List[str], reason: str = "ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç®¡ç†") -> Dict[str, Any]:
        """ç„¡åŠ¹åœ°åå‰Šé™¤"""
        print(f"ğŸ—‘ï¸ ç„¡åŠ¹åœ°åå‰Šé™¤: {len(place_names)}ä»¶")
        return self.context_aware_geocoder.delete_invalid_places(place_names, reason)
    
    def cleanup_invalid_places(self, auto_confirm: bool = False) -> Dict[str, Any]:
        """ç„¡åŠ¹åœ°åè‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        print("ğŸ§¹ ç„¡åŠ¹åœ°åè‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ...")
        return self.context_aware_geocoder.cleanup_invalid_places(auto_confirm)
    
    def analyze_place_usage(self, place_name: str) -> Dict[str, Any]:
        """åœ°åä½¿ç”¨çŠ¶æ³åˆ†æ"""
        print(f"ğŸ” åœ°åä½¿ç”¨çŠ¶æ³åˆ†æ: {place_name}")
        return self.context_aware_geocoder.get_place_usage_analysis(place_name)
    
    def get_geocoding_stats(self) -> Dict[str, Any]:
        """ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°çµ±è¨ˆå–å¾—"""
        return self.context_aware_geocoder.get_geocoding_statistics()
    
    def _run_data_quality_maintenance(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼ãƒ»ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹å‡¦ç†"""
        maintenance_results = {
            'maintenance_success': True,
            'enriched_sentence_places': 0,
            'enriched_works': 0,
            'updated_publication_years': 0,
            'fixed_matched_texts': 0,
            'maintenance_errors': []
        }
        
        try:
            # 1. sentence_placesè£œå®Œ
            try:
                enrichment_result = self.sentence_places_enricher.run_full_enrichment()
                maintenance_results['enriched_sentence_places'] = enrichment_result.get('total_updates', 0)
            except Exception as e:
                maintenance_results['maintenance_errors'].append(f"sentence_placesè£œå®Œã‚¨ãƒ©ãƒ¼: {e}")
                maintenance_results['maintenance_success'] = False
            
            # 2. worksãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è£œå®Œ
            try:
                works_result = self._enrich_works_metadata()
                maintenance_results['enriched_works'] = works_result.get('enriched_count', 0)
            except Exception as e:
                maintenance_results['maintenance_errors'].append(f"worksãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è£œå®Œã‚¨ãƒ©ãƒ¼: {e}")
                maintenance_results['maintenance_success'] = False
            
            # 3. work_publication_yearæ›´æ–°
            try:
                publication_result = self._update_work_publication_years()
                maintenance_results['updated_publication_years'] = publication_result.get('updated_count', 0)
            except Exception as e:
                maintenance_results['maintenance_errors'].append(f"å‡ºç‰ˆå¹´æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
                maintenance_results['maintenance_success'] = False
            
            # 4. matched_textä¿®æ­£
            try:
                matched_text_result = self._fix_matched_text()
                maintenance_results['fixed_matched_texts'] = matched_text_result.get('fixed_count', 0)
            except Exception as e:
                maintenance_results['maintenance_errors'].append(f"matched_textä¿®æ­£ã‚¨ãƒ©ãƒ¼: {e}")
                maintenance_results['maintenance_success'] = False
                
        except Exception as e:
            maintenance_results['maintenance_errors'].append(f"ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹å‡¦ç†å…¨ä½“ã‚¨ãƒ©ãƒ¼: {e}")
            maintenance_results['maintenance_success'] = False
        
        return maintenance_results
    
    def _enrich_works_metadata(self) -> Dict[str, Any]:
        """worksãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è£œå®Œ"""
        import subprocess
        
        result = subprocess.run([
            'python3', 'extractors/enrich_works_metadata.py'
        ], capture_output=True, text=True, cwd=os.path.dirname(os.path.abspath(__file__)))
        
        if result.returncode == 0:
            # æˆåŠŸæ™‚ã®å‡¦ç†ä»¶æ•°ã‚’å‡ºåŠ›ã‹ã‚‰æŠ½å‡º
            output_lines = result.stdout.split('\n')
            enriched_count = 0
            for line in output_lines:
                if 'ä»¶å‡¦ç†å®Œäº†' in line:
                    try:
                        enriched_count = int(line.split('ä»¶å‡¦ç†å®Œäº†')[0].split()[-1])
                    except:
                        pass
            
            return {'enriched_count': enriched_count}
        else:
            raise Exception(f"worksãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è£œå®Œå¤±æ•—: {result.stderr}")
    
    def _update_work_publication_years(self) -> Dict[str, Any]:
        """work_publication_yearæ›´æ–°"""
        import subprocess
        
        result = subprocess.run([
            'python3', 'extractors/update_work_publication_year.py'
        ], capture_output=True, text=True, cwd=os.path.dirname(os.path.abspath(__file__)))
        
        if result.returncode == 0:
            # æˆåŠŸæ™‚ã®å‡¦ç†ä»¶æ•°ã‚’å‡ºåŠ›ã‹ã‚‰æŠ½å‡º
            output_lines = result.stdout.split('\n')
            updated_count = 0
            for line in output_lines:
                if 'ä»¶æ›´æ–°' in line:
                    try:
                        updated_count = int(line.split('ä»¶æ›´æ–°')[0].split()[-1])
                    except:
                        pass
            
            return {'updated_count': updated_count}
        else:
            raise Exception(f"å‡ºç‰ˆå¹´æ›´æ–°å¤±æ•—: {result.stderr}")
    
    def _fix_matched_text(self) -> Dict[str, Any]:
        """matched_textä¿®æ­£"""
        import sqlite3
        
        try:
            conn = sqlite3.connect('data/bungo_map.db')
            cursor = conn.cursor()
            
            # matched_textã‚’å¯¾å¿œã™ã‚‹sentence_textã§æ›´æ–°
            cursor.execute("""
                UPDATE sentence_places 
                SET matched_text = (
                    SELECT sentence_text 
                    FROM sentences 
                    WHERE sentences.sentence_id = sentence_places.sentence_id
                )
            """)
            
            fixed_count = cursor.rowcount
            conn.commit()
            conn.close()
            
            return {'fixed_count': fixed_count}
        except Exception as e:
            raise Exception(f"matched_textä¿®æ­£å¤±æ•—: {e}")
    
    def _print_report(self, results: Dict[str, Any]):
        """ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º"""
        print(f"\nğŸ‰ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ")
        print("=" * 80)
        print(f"ğŸ‘¤ ä½œè€…: {results['author']}")
        print(f"â±ï¸  å®Ÿè¡Œæ™‚é–“: {results['duration']:.1f}ç§’")
        print(f"ğŸ† çµæœ: {'æˆåŠŸ' if results['success'] else 'å¤±æ•—'}")
        print(f"ğŸ“š å‡¦ç†ä½œå“: {results['works_processed']}ä»¶")
        print(f"ğŸ“ ç”Ÿæˆã‚»ãƒ³ãƒ†ãƒ³ã‚¹: {results['sentences_created']:,}ä»¶")
        print(f"ğŸ“„ åœ°åå‡¦ç†ã‚»ãƒ³ãƒ†ãƒ³ã‚¹: {results.get('sentences_processed', 0)}ä»¶")
        print(f"ğŸ—ºï¸  æŠ½å‡ºåœ°å: {results['places_extracted']}ä»¶")
        print(f"ğŸ’¾ ä¿å­˜åœ°å: {results.get('places_saved', 0)}ä»¶")
        print(f"ğŸŒ ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æˆåŠŸ: {results.get('places_geocoded', 0)}ä»¶")
        print(f"ğŸ“Š ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æˆåŠŸç‡: {results.get('geocoding_success_rate', 0):.1f}%")
        
        # ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹çµæœ
        if 'maintenance_success' in results:
            print(f"\nğŸ”§ ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼çµæœ:")
            print(f"  ğŸ“ sentence_placesè£œå®Œ: {results.get('enriched_sentence_places', 0)}ä»¶")
            print(f"  ğŸ“š worksãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è£œå®Œ: {results.get('enriched_works', 0)}ä»¶")
            print(f"  ğŸ“… å‡ºç‰ˆå¹´æ›´æ–°: {results.get('updated_publication_years', 0)}ä»¶")
            print(f"  ğŸ”§ matched_textä¿®æ­£: {results.get('fixed_matched_texts', 0)}ä»¶")
            print(f"  ğŸ† ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹çµæœ: {'æˆåŠŸ' if results.get('maintenance_success', False) else 'ä¸€éƒ¨ã‚¨ãƒ©ãƒ¼'}")
        
        # é€šå¸¸ã®ã‚¨ãƒ©ãƒ¼
        if results['errors']:
            print(f"\nâŒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼: {len(results['errors'])}ä»¶")
            for error in results['errors']:
                print(f"  - {error}")
        
        # ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã‚¨ãƒ©ãƒ¼
        if results.get('maintenance_errors'):
            print(f"\nâš ï¸ ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã‚¨ãƒ©ãƒ¼: {len(results['maintenance_errors'])}ä»¶")
            for error in results['maintenance_errors']:
                print(f"  - {error}")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(
        description='æ–‡è±ªã‚†ã‹ã‚Šåœ°å›³ã‚·ã‚¹ãƒ†ãƒ  v4.0 - çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # åŸºæœ¬å‡¦ç†
  python3 run_pipeline.py --author "æ¢¶äº• åŸºæ¬¡éƒ"
  python3 run_pipeline.py --author "å¤ç›® æ¼±çŸ³" --works-only
  python3 run_pipeline.py --author "èŠ¥å· é¾ä¹‹ä»‹" --no-geocoding
  python3 run_pipeline.py --author "å¤ªå®° æ²»" --no-maintenance
  python3 run_pipeline.py --status "æ¢¶äº• åŸºæ¬¡éƒ"
  
  # åœ°åç®¡ç†
  python3 run_pipeline.py --stats
  python3 run_pipeline.py --analyze "å±±é“"
  python3 run_pipeline.py --cleanup-preview
  python3 run_pipeline.py --cleanup
  python3 run_pipeline.py --delete "å…ˆæ—¥é£¯å³¶" "ä»Šé£¯å³¶" "å¤•æ–¹å±±"
  
  # Wikipediaä½œè€…æƒ…å ±è£œå®Œ
  python3 run_pipeline.py --enrich-preview
  python3 run_pipeline.py --enrich-authors
  python3 run_pipeline.py --enrich-specific "å¤ç›® æ¼±çŸ³" "èŠ¥å· é¾ä¹‹ä»‹"
        """
    )
    
    # å‡¦ç†å¯¾è±¡
    parser.add_argument('--author', '-a', help='å˜ä¸€ä½œè€…å')
    parser.add_argument('--status', '-s', help='ä½œè€…ã®å‡¦ç†çŠ¶æ³ç¢ºèª')
    
    # å®Ÿè¡Œåˆ¶å¾¡
    parser.add_argument('--works-only', action='store_true', help='ä½œå“åé›†ã®ã¿ï¼ˆåœ°åæŠ½å‡ºãªã—ï¼‰')
    parser.add_argument('--no-geocoding', action='store_true', help='åœ°åæŠ½å‡ºã®ã¿ï¼ˆã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãªã—ï¼‰')
    parser.add_argument('--no-maintenance', action='store_true', help='ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼ãƒ»ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—')
    
    # åœ°åç®¡ç†æ©Ÿèƒ½
    parser.add_argument('--delete', nargs='+', help='æŒ‡å®šã—ãŸåœ°åã‚’å‰Šé™¤')
    parser.add_argument('--cleanup', action='store_true', help='ç„¡åŠ¹åœ°åã®è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ')
    parser.add_argument('--cleanup-preview', action='store_true', help='ç„¡åŠ¹åœ°åã®å‰Šé™¤å€™è£œã‚’è¡¨ç¤ºï¼ˆå®Ÿè¡Œãªã—ï¼‰')
    parser.add_argument('--analyze', type=str, help='æŒ‡å®šã—ãŸåœ°åã®ä½¿ç”¨çŠ¶æ³ã‚’è©³ç´°åˆ†æ')
    parser.add_argument('--stats', action='store_true', help='ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°çµ±è¨ˆè¡¨ç¤º')
    
    # Wikipediaä½œè€…æƒ…å ±è£œå®Œæ©Ÿèƒ½
    parser.add_argument('--enrich-authors', action='store_true', help='Wikipediaä½œè€…æƒ…å ±è‡ªå‹•è£œå®Œï¼ˆå…¨ä½œè€…ï¼‰')
    parser.add_argument('--enrich-preview', action='store_true', help='ä½œè€…æƒ…å ±ä¸è¶³çŠ¶æ³ã‚’ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤º')
    parser.add_argument('--enrich-specific', nargs='+', help='æŒ‡å®šä½œè€…ã®ã¿æƒ…å ±è£œå®Œï¼ˆè¤‡æ•°å¯ï¼‰')
    
    args = parser.parse_args()
    
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–
    pipeline = BungoPipeline()
    
    # Wikipediaä½œè€…æƒ…å ±è£œå®Œãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
    if args.enrich_preview:
        print("=== ğŸ” ä½œè€…æƒ…å ±ä¸è¶³çŠ¶æ³ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ ===")
        missing_info = pipeline.wikipedia_enricher.preview_missing_info()
        return
    
    # Wikipediaä½œè€…æƒ…å ±è‡ªå‹•è£œå®Œï¼ˆå…¨ä½œè€…ï¼‰
    if args.enrich_authors:
        print("=== ğŸŒŸ Wikipediaä½œè€…æƒ…å ±è‡ªå‹•è£œå®Œï¼ˆå…¨ä½œè€…ï¼‰ ===")
        try:
            stats = pipeline.wikipedia_enricher.enrich_all_authors()
            pipeline.wikipedia_enricher.print_statistics()
        except KeyboardInterrupt:
            print("\nâš ï¸ å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        finally:
            pipeline.wikipedia_enricher.close()
        return
    
    # Wikipediaä½œè€…æƒ…å ±è£œå®Œï¼ˆæŒ‡å®šä½œè€…ã®ã¿ï¼‰
    if args.enrich_specific:
        print(f"=== ğŸ¯ Wikipediaä½œè€…æƒ…å ±è£œå®Œï¼ˆæŒ‡å®šä½œè€…: {', '.join(args.enrich_specific)}ï¼‰ ===")
        try:
            stats = pipeline.wikipedia_enricher.enrich_specific_authors(args.enrich_specific)
            pipeline.wikipedia_enricher.print_statistics()
        except KeyboardInterrupt:
            print("\nâš ï¸ å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        finally:
            pipeline.wikipedia_enricher.close()
        return
    
    # çµ±è¨ˆè¡¨ç¤º
    if args.stats:
        print("=== ğŸ“Š ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°çµ±è¨ˆæƒ…å ± ===")
        stats = pipeline.get_geocoding_stats()
        print(f"ç·åœ°åæ•°: {stats['total_places']:,}")
        print(f"Geocodingæ¸ˆã¿åœ°åæ•°: {stats['geocoded_places']:,}")
        print(f"Geocodingç‡: {stats['geocoding_rate']:.1f}%")
        if stats.get('source_stats'):
            print("\nğŸ”§ Geocodingã‚½ãƒ¼ã‚¹:")
            for source, count in stats['source_stats'].items():
                print(f"  {source}: {count}ä»¶")
        return
    
    # åœ°åä½¿ç”¨çŠ¶æ³åˆ†æ
    if args.analyze:
        analysis_result = pipeline.analyze_place_usage(args.analyze)
        if "error" in analysis_result:
            print(f"âŒ {analysis_result['error']}")
            return
        
        place_data = analysis_result["place_data"]
        print(f"=== ğŸ” åœ°åä½¿ç”¨çŠ¶æ³åˆ†æ: {args.analyze} ===")
        print(f"ğŸ“ åœ°åæƒ…å ±:")
        print(f"   ID: {place_data['place_id']}")
        print(f"   åœ°å: {place_data['place_name']}")
        print(f"   ç¨®åˆ¥: {place_data['place_type']}")
        print(f"   åº§æ¨™: ({place_data['latitude']}, {place_data['longitude']})")
        print(f"   ä¿¡é ¼åº¦: {place_data['confidence']}")
        print(f"   ã‚½ãƒ¼ã‚¹: {place_data['source_system']}")
        
        print(f"\nğŸ“Š ä½¿ç”¨çµ±è¨ˆ:")
        print(f"   ä½¿ç”¨å›æ•°: {analysis_result['usage_count']}å›")
        print(f"   æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {analysis_result['recommended_action']}")
        
        print(f"\nğŸ“ ä½¿ç”¨ä¾‹:")
        for i, context in enumerate(analysis_result["context_analyses"][:3]):
            print(f"   ä¾‹{i+1}: {context['sentence'][:100]}...")
            print(f"        åœ°ååˆ¤å®š: {context['is_place_name']} (ä¿¡é ¼åº¦: {context['confidence']:.2f})")
            print(f"        ç†ç”±: {context['reasoning']}")
        return
    
    # å‰Šé™¤å€™è£œãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
    if args.cleanup_preview:
        print("=== ğŸ” ç„¡åŠ¹åœ°åå‰Šé™¤å€™è£œãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ ===")
        cleanup_result = pipeline.cleanup_invalid_places(auto_confirm=False)
        
        if not cleanup_result["candidates"]:
            print("âœ… å‰Šé™¤å€™è£œã®ç„¡åŠ¹åœ°åã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        print(f"ğŸ“Š å‰Šé™¤å€™è£œ: {len(cleanup_result['candidates'])}ä»¶")
        for candidate in cleanup_result["candidates"]:
            print(f"   ğŸ—‘ï¸ {candidate['place_name']}")
            print(f"      ç†ç”±: {candidate['reason']}")
            print(f"      ä½¿ç”¨å›æ•°: {candidate['usage_count']}å›")
            print(f"      ä¾‹: {candidate['sample']}")
        
        print("ğŸ’¡ å‰Šé™¤ã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ --cleanup ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„")
        return
    
    # è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ
    if args.cleanup:
        print("=== ğŸ—‘ï¸ ç„¡åŠ¹åœ°åè‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ ===")
        cleanup_result = pipeline.cleanup_invalid_places(auto_confirm=True)
        
        if cleanup_result["deletion_result"]["total_deleted"] > 0:
            print(f"âœ… {cleanup_result['deletion_result']['total_deleted']}ä»¶ã®ç„¡åŠ¹åœ°åã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
            for deleted in cleanup_result["deletion_result"]["deleted_places"]:
                print(f"   ğŸ—‘ï¸ {deleted['place_name']} (ç†ç”±: {deleted['reason']})")
        else:
            print("âœ… å‰Šé™¤å¯¾è±¡ã®ç„¡åŠ¹åœ°åã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    # æŒ‡å®šåœ°åå‰Šé™¤
    if args.delete:
        print(f"=== ğŸ—‘ï¸ æŒ‡å®šåœ°åå‰Šé™¤: {', '.join(args.delete)} ===")
        deletion_result = pipeline.delete_invalid_places(args.delete, "æ‰‹å‹•å‰Šé™¤")
        
        if deletion_result["total_deleted"] > 0:
            print(f"âœ… {deletion_result['total_deleted']}ä»¶ã®åœ°åã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
            for deleted in deletion_result["deleted_places"]:
                print(f"   ğŸ—‘ï¸ {deleted['place_name']} (é–¢é€£: {deleted['deleted_relations']}ä»¶)")
        
        if deletion_result["not_found_places"]:
            print(f"âš ï¸ è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸåœ°å: {', '.join(deletion_result['not_found_places'])}")
        return
    
    # å‡¦ç†çŠ¶æ³ç¢ºèª
    if args.status:
        pipeline.check_status(args.status)
        return
    
    # åœ°åæŠ½å‡ºã‚’å«ã‚€ã‹ã©ã†ã‹
    include_places = not args.works_only
    include_geocoding = not args.no_geocoding
    include_maintenance = not args.no_maintenance
    
    # å‡¦ç†å®Ÿè¡Œ
    if args.author:
        # å˜ä¸€ä½œè€…å‡¦ç†
        pipeline.run_full_pipeline(args.author, include_places, include_geocoding, include_maintenance)
    else:
        print("âŒ å‡¦ç†å¯¾è±¡ä½œè€…ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
        parser.print_help()

if __name__ == '__main__':
    main() 