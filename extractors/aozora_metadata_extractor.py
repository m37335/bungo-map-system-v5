#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é’ç©ºæ–‡åº«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è‡ªå‹•æŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ  v4.0
ä½œå“ã®å‡ºç‰ˆå¹´ã€ã‚«ãƒ¼ãƒ‰IDç­‰ã‚’è‡ªå‹•è£œå®Œ
"""

import re
import requests
import time
from typing import Dict, Optional, List
from bs4 import BeautifulSoup
import sys
import os

# ãƒ‘ã‚¹è¨­å®š
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from database.manager import DatabaseManager


class AozoraMetadataExtractor:
    """é’ç©ºæ–‡åº«ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•æŠ½å‡ºã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’è£œå®Œ"""
    
    def __init__(self):
        self.db = DatabaseManager()
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'BungoMapBot/4.0 (bungo-map@example.com)'
        })
        
        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            'total_works': 0,
            'success_count': 0,
            'failure_count': 0,
            'publication_year_updated': 0,
            'card_id_updated': 0,
            'aozora_work_id_updated': 0,
            'processing_time': 0
        }
    
    def extract_metadata_from_url(self, aozora_url: str) -> Dict:
        """é’ç©ºæ–‡åº«URLã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        try:
            print(f"ğŸ” {aozora_url} ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºä¸­...")
            
            # ãƒšãƒ¼ã‚¸å–å¾—
            response = self.session.get(aozora_url, timeout=10)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            
            soup = BeautifulSoup(response.text, 'html.parser')
            metadata = {}
            
            # 1. URLè§£æã§card_idã¨aozora_work_idã‚’æŠ½å‡º
            url_match = re.search(r'/cards/(\d+)/card(\d+)\.html', aozora_url)
            if url_match:
                metadata['aozora_work_id'] = url_match.group(1)
                metadata['card_id'] = url_match.group(2)
            
            # 2. ä½œå“ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰åˆå‡ºå¹´ã‚’æŠ½å‡º
            metadata['publication_year'] = self._extract_publication_year(soup)
            
            # 3. ãã®ä»–ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            metadata['copyright_status'] = "ãƒ‘ãƒ–ãƒªãƒƒã‚¯ãƒ‰ãƒ¡ã‚¤ãƒ³"
            metadata['input_person'] = self._extract_input_person(soup)
            metadata['proof_person'] = self._extract_proof_person(soup)
            
            return metadata
            
        except Exception as e:
            print(f"âŒ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼ ({aozora_url}): {e}")
            return {}
    
    def _extract_publication_year(self, soup: BeautifulSoup) -> Optional[int]:
        """åˆå‡ºå¹´ã‚’æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼šè¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œï¼‰"""
        try:
            # ä½œå“ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¢ã™
            tables = soup.find_all('table', {'summary': 'ä½œå“ãƒ‡ãƒ¼ã‚¿'})
            
            for table in tables:
                rows = table.find_all('tr')
                for row in rows:
                    cells = row.find_all('td')
                    if len(cells) >= 2:
                        header = cells[0].get_text(strip=True)
                        content = cells[1].get_text(strip=True)
                        
                        # è¤‡æ•°ã®å¹´æƒ…å ±ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œ
                        if any(keyword in header for keyword in ['åˆå‡º', 'ç™ºè¡¨', 'ç™ºè¡Œ', 'å‡ºç‰ˆ']):
                            year = self._extract_year_from_text(content)
                            if year:
                                print(f"âœ… {header}ã‹ã‚‰å¹´æŠ½å‡º: {year}å¹´ ({content[:50]}...)")
                                return year
            
            # ä½œå“ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€ãƒšãƒ¼ã‚¸å…¨ä½“ã‹ã‚‰æŠ½å‡º
            print("âš ï¸ ä½œå“ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒšãƒ¼ã‚¸å…¨ä½“ã‚’æ¤œç´¢...")
            page_text = soup.get_text()
            year = self._extract_year_from_text(page_text[:1000])  # æœ€åˆã®1000æ–‡å­—ã®ã¿
            if year:
                print(f"âœ… ãƒšãƒ¼ã‚¸å…¨ä½“ã‹ã‚‰å¹´æŠ½å‡º: {year}å¹´")
                return year
            
            return None
            
        except Exception as e:
            print(f"âš ï¸ åˆå‡ºå¹´æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _extract_year_from_text(self, text: str) -> Optional[int]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å¹´ã‚’æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼šå¤šæ§˜ãªãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œï¼‰"""
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: è¥¿æš¦å¹´ï¼ˆ1900-2100ï¼‰
        year_patterns = [
            r'(\d{4})å¹´',  # 1930å¹´
            r'(\d{4})ï¼ˆ.*?ï¼‰å¹´',  # 1930ï¼ˆæ˜­å’Œ5ï¼‰å¹´
            r'ã€Œ.*?ã€(\d{4})å¹´',  # ã€Œé›‘èªŒåã€1930å¹´
            r'ã€.*?ã€(\d{4})å¹´',  # ã€é›‘èªŒåã€1930å¹´
        ]
        
        for pattern in year_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                try:
                    year = int(match)
                    if 1800 <= year <= 2100:  # å¦¥å½“ãªå¹´ã®ç¯„å›²
                        return year
                except ValueError:
                    continue
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: å¹´å·å¤‰æ›
        era_patterns = [
            (r'æ˜æ²»(\d+)å¹´', 1867),  # æ˜æ²»å…ƒå¹´=1868å¹´
            (r'å¤§æ­£(\d+)å¹´', 1911),  # å¤§æ­£å…ƒå¹´=1912å¹´
            (r'æ˜­å’Œ(\d+)å¹´', 1925),  # æ˜­å’Œå…ƒå¹´=1926å¹´
        ]
        
        for pattern, base_year in era_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                try:
                    era_year = int(match)
                    if era_year > 0:  # 0å¹´ã¯ç„¡åŠ¹
                        year = base_year + era_year
                        if 1800 <= year <= 2100:
                            return year
                except ValueError:
                    continue
        
        return None
    
    def _extract_input_person(self, soup: BeautifulSoup) -> Optional[str]:
        """å…¥åŠ›è€…ã‚’æŠ½å‡º"""
        try:
            tables = soup.find_all('table', {'summary': 'å·¥ä½œå“¡ãƒ‡ãƒ¼ã‚¿'})
            
            for table in tables:
                rows = table.find_all('tr')
                for row in rows:
                    cells = row.find_all('td')
                    if len(cells) >= 2:
                        header = cells[0].get_text(strip=True)
                        content = cells[1].get_text(strip=True)
                        
                        if 'å…¥åŠ›' in header:
                            return content
            
            return None
            
        except Exception as e:
            print(f"âš ï¸ å…¥åŠ›è€…æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _extract_proof_person(self, soup: BeautifulSoup) -> Optional[str]:
        """æ ¡æ­£è€…ã‚’æŠ½å‡º"""
        try:
            tables = soup.find_all('table', {'summary': 'å·¥ä½œå“¡ãƒ‡ãƒ¼ã‚¿'})
            
            for table in tables:
                rows = table.find_all('tr')
                for row in rows:
                    cells = row.find_all('td')
                    if len(cells) >= 2:
                        header = cells[0].get_text(strip=True)
                        content = cells[1].get_text(strip=True)
                        
                        if 'æ ¡æ­£' in header:
                            return content
            
            return None
            
        except Exception as e:
            print(f"âš ï¸ æ ¡æ­£è€…æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def update_work_metadata(self, work_id: int, work_title: str, aozora_url: str):
        """ä½œå“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°"""
        try:
            print(f"\nğŸ“ ä½œå“ '{work_title}' (ID: {work_id}) ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›´æ–°ä¸­...")
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            metadata = self.extract_metadata_from_url(aozora_url)
            
            if not metadata:
                print(f"âŒ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")
                self.stats['failure_count'] += 1
                return False
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°
            update_fields = {}
            updates_made = []
            
            if metadata.get('publication_year'):
                update_fields['publication_year'] = metadata['publication_year']
                updates_made.append(f"å‡ºç‰ˆå¹´: {metadata['publication_year']}å¹´")
                self.stats['publication_year_updated'] += 1
            
            if metadata.get('card_id'):
                update_fields['card_id'] = metadata['card_id']
                updates_made.append(f"ã‚«ãƒ¼ãƒ‰ID: {metadata['card_id']}")
                self.stats['card_id_updated'] += 1
            
            if metadata.get('aozora_work_id'):
                update_fields['aozora_work_id'] = metadata['aozora_work_id']
                updates_made.append(f"ä½œå“ID: {metadata['aozora_work_id']}")
                self.stats['aozora_work_id_updated'] += 1
            
            if metadata.get('copyright_status'):
                update_fields['copyright_status'] = metadata['copyright_status']
                updates_made.append(f"è‘—ä½œæ¨©: {metadata['copyright_status']}")
            
            if metadata.get('input_person'):
                update_fields['input_person'] = metadata['input_person']
                updates_made.append(f"å…¥åŠ›è€…: {metadata['input_person']}")
            
            if metadata.get('proof_person'):
                update_fields['proof_person'] = metadata['proof_person']
                updates_made.append(f"æ ¡æ­£è€…: {metadata['proof_person']}")
            
            if update_fields:
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°
                set_clause = ', '.join([f"{field} = ?" for field in update_fields.keys()])
                values = list(update_fields.values()) + [work_id]
                
                query = f"""
                UPDATE works 
                SET {set_clause}
                WHERE work_id = ?
                """
                
                with self.db.get_connection() as conn:
                    cursor = conn.execute(query, values)
                    conn.commit()
                
                print(f"âœ… {work_title} ã®æƒ…å ±ã‚’æ›´æ–°: {', '.join(updates_made)}")
                self.stats['success_count'] += 1
                return True
            else:
                print(f"âš ï¸ {work_title}: æ›´æ–°å¯èƒ½ãªæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                self.stats['failure_count'] += 1
                return False
                
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°ã‚¨ãƒ©ãƒ¼ ({work_title}): {e}")
            self.stats['failure_count'] += 1
            return False
    
    def enrich_all_works(self) -> Dict:
        """å…¨ä½œå“ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬è£œå®Œ"""
        print("ğŸŒŸ å…¨ä½œå“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä¸€æ‹¬è£œå®Œã‚’é–‹å§‹ã—ã¾ã™...")
        start_time = time.time()
        
        try:
            # å…¨ä½œå“å–å¾—
            with self.db.get_connection() as conn:
                cursor = conn.execute("""
                    SELECT work_id, work_title, aozora_url 
                    FROM works 
                    WHERE aozora_url IS NOT NULL 
                    ORDER BY work_id
                """)
                works = cursor.fetchall()
            
            if not works:
                print("âŒ ä½œå“ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return self.stats
            
            self.stats['total_works'] = len(works)
            print(f"ğŸ“Š å‡¦ç†å¯¾è±¡: {len(works)} ä»¶ã®ä½œå“")
            
            # å„ä½œå“ã«ã¤ã„ã¦å‡¦ç†
            for i, (work_id, work_title, aozora_url) in enumerate(works, 1):
                print(f"\nğŸ”„ [{i}/{len(works)}] å‡¦ç†ä¸­...")
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›´æ–°
                self.update_work_metadata(work_id, work_title, aozora_url)
                
                # APIåˆ¶é™å¯¾ç­–ï¼ˆ1ç§’é–“éš”ï¼‰
                time.sleep(1.0)
            
            # çµ±è¨ˆæƒ…å ±æ›´æ–°
            self.stats['processing_time'] = time.time() - start_time
            
            return self.stats
            
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            self.stats['processing_time'] = time.time() - start_time
            return self.stats
    
    def preview_missing_metadata(self) -> Dict:
        """ä¸è¶³ã—ã¦ã„ã‚‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼"""
        print("ğŸ” ä¸è¶³ã—ã¦ã„ã‚‹ä½œå“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªä¸­...")
        
        try:
            with self.db.get_connection() as conn:
                cursor = conn.execute("""
                    SELECT 
                        work_id, 
                        work_title,
                        publication_year,
                        card_id,
                        aozora_work_id
                    FROM works 
                    WHERE publication_year IS NULL 
                       OR card_id IS NULL 
                       OR aozora_work_id IS NULL 
                    ORDER BY work_id
                """)
                missing_works = cursor.fetchall()
            
            if not missing_works:
                print("âœ… å…¨ã¦ã®ä½œå“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒå®Œå…¨ã§ã™ï¼")
                return {'missing_count': 0, 'works': []}
            
            print(f"ğŸ“‹ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®ä½œå“: {len(missing_works)} ä»¶")
            print("-"*80)
            print(f"{'ID':<4} {'ä½œå“å':<30} {'å‡ºç‰ˆå¹´':<8} {'ã‚«ãƒ¼ãƒ‰ID':<10} {'ä½œå“ID':<8}")
            print("-"*80)
            
            for work_id, title, pub_year, card_id, work_id_val in missing_works:
                pub_str = str(pub_year) if pub_year else "ãªã—"
                card_str = str(card_id) if card_id else "ãªã—"
                work_id_str = str(work_id_val) if work_id_val else "ãªã—"
                title_truncated = title[:28] + ".." if len(title) > 30 else title
                print(f"{work_id:<4} {title_truncated:<30} {pub_str:<8} {card_str:<10} {work_id_str:<8}")
            
            return {
                'missing_count': len(missing_works),
                'works': missing_works
            }
            
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            return {'missing_count': 0, 'works': []}
    
    def print_statistics(self):
        """çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º"""
        print("="*60)
        print("ğŸ“Š é’ç©ºæ–‡åº«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è£œå®Œ - å‡¦ç†çµæœ")
        print("="*60)
        print(f"å‡¦ç†å¯¾è±¡ä½œå“æ•°: {self.stats['total_works']} ä»¶")
        print(f"æˆåŠŸ: {self.stats['success_count']} ä»¶")
        print(f"å¤±æ•—: {self.stats['failure_count']} ä»¶")
        
        if self.stats['total_works'] > 0:
            success_rate = (self.stats['success_count'] / self.stats['total_works']) * 100
            print(f"æˆåŠŸç‡: {success_rate:.1f}%")
        
        print("-"*60)
        print(f"å‡ºç‰ˆå¹´è£œå®Œ: {self.stats['publication_year_updated']} ä»¶")
        print(f"ã‚«ãƒ¼ãƒ‰IDè£œå®Œ: {self.stats['card_id_updated']} ä»¶")
        print(f"ä½œå“IDè£œå®Œ: {self.stats['aozora_work_id_updated']} ä»¶")
        print(f"å‡¦ç†æ™‚é–“: {self.stats['processing_time']:.1f} ç§’")
        print("="*60)


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    extractor = AozoraMetadataExtractor()
    
    try:
        # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤º
        extractor.preview_missing_metadata()
        
        # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆæœ€åˆã®1ä»¶ã®ã¿ï¼‰
        print("\nğŸ§ª ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆæœ€åˆã®1ä»¶ï¼‰...")
        extractor.update_work_metadata(1, "ãƒ†ã‚¹ãƒˆä½œå“", "https://www.aozora.gr.jp/cards/000074/card411.html")
        
        # çµ±è¨ˆè¡¨ç¤º
        extractor.print_statistics()
        
    except KeyboardInterrupt:
        print("\nâš ï¸ å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}") 