#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GinZAçµ±åˆåœ°åæŠ½å‡ºå™¨ v4.0
Enhanced Place Extractor V2 + GiNZAçµ±åˆç‰ˆ

æ©Ÿèƒ½:
- spaCy + GiNZAã«ã‚ˆã‚‹é«˜ç²¾åº¦NLPåœ°åæŠ½å‡º
- æ—¢å­˜ã®æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨çµ„ã¿åˆã‚ã›
- AIæ¤œè¨¼æ©Ÿèƒ½ã¨ã®çµ±åˆ
- ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ï¼ˆGiNZAæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ™‚ï¼‰
"""

import re
import sqlite3
import logging
import openai
import os
import json
from typing import List, Dict, Tuple, Optional, Set
from dataclasses import dataclass
from datetime import datetime
from dotenv import load_dotenv

# GiNZA/spaCyã®å‹•çš„ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ä¾å­˜ï¼‰
try:
    import spacy
    import ginza
    GINZA_AVAILABLE = True
    logging.info("âœ… GiNZA/spaCyåˆ©ç”¨å¯èƒ½")
except ImportError:
    GINZA_AVAILABLE = False
    logging.warning("âš ï¸ GiNZA/spaCyæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« - æ­£è¦è¡¨ç¾ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§å‹•ä½œ")

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class ExtractedPlace:
    """æŠ½å‡ºã•ã‚ŒãŸåœ°åã®æƒ…å ±"""
    name: str
    canonical_name: str
    place_type: str
    confidence: float
    position: int
    matched_text: str
    context_before: str
    context_after: str
    extraction_method: str
    priority: int = 99
    entity_type: str = ""
    pos_tag: str = ""
    lemma: str = ""
    reading: str = ""

class GinzaEnhancedExtractor:
    """GinZAçµ±åˆåœ°åæŠ½å‡ºå™¨ï¼ˆEnhanced V2 + GiNZAï¼‰"""
    
    def __init__(self):
        self._init_ginza_system()
        self._init_enhanced_patterns()
        self._init_ai_verification()
        logger.info("ğŸŒŸ GinZAçµ±åˆåœ°åæŠ½å‡ºå™¨v4.0åˆæœŸåŒ–å®Œäº†")
        
    def _init_ginza_system(self):
        """GiNZAã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        self.nlp = None
        self.ginza_enabled = False
        
        if GINZA_AVAILABLE:
            try:
                self.nlp = spacy.load('ja_ginza')
                self.ginza_enabled = True
                logger.info("âœ… GiNZAæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–æˆåŠŸ")
            except OSError:
                logger.warning("âš ï¸ GiNZAãƒ¢ãƒ‡ãƒ«æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« - pip install ja-ginza ã§å°å…¥ã—ã¦ãã ã•ã„")
        
        # åœ°åé–¢é€£ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚¿ã‚¤ãƒ—
        self.place_entity_types = {
            'GPE',      # åœ°æ”¿å­¦çš„ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£
            'LOC',      # å ´æ‰€
            'FACILITY', # æ–½è¨­
            'ORG'       # çµ„ç¹”ï¼ˆå ´æ‰€åã‚’å«ã‚€å ´åˆï¼‰
        }
        
    def _init_enhanced_patterns(self):
        """Enhanced V2ãƒ‘ã‚¿ãƒ¼ãƒ³çµ±åˆ"""
        # å®Ÿè¨¼æ¸ˆã¿é«˜æ€§èƒ½æœ‰ååœ°åãƒªã‚¹ãƒˆï¼ˆ4,281ä»¶å¯¾å¿œï¼‰
        self.famous_places = [
            # æ±äº¬ä¸­å¿ƒéƒ¨ï¼ˆé«˜é »åº¦ï¼‰
            'éŠ€åº§', 'æ–°å®¿', 'æ¸‹è°·', 'ä¸Šé‡', 'æµ…è‰', 'å“å·', 'æ± è¢‹', 'æ–°æ©‹', 'æœ‰æ¥½ç”º',
            'ä¸¸ã®å†…', 'è¡¨å‚é“', 'åŸå®¿', 'æµæ¯”å¯¿', 'å…­æœ¬æœ¨', 'èµ¤å‚', 'é’å±±', 'éº»å¸ƒ',
            'ç›®é»’', 'ä¸–ç”°è°·', 'æœ¬éƒ·', 'ç¥ç”°', 'æ—¥æœ¬æ©‹', 'ç¯‰åœ°', 'æœˆå³¶', 'ä¸¡å›½',
            
            # é–¢æ±ä¸»è¦åœ°å
            'æ¨ªæµœ', 'å·å´', 'åƒè‘‰', 'åŸ¼ç‰', 'å¤§å®®', 'æµ¦å’Œ', 'èˆ¹æ©‹', 'æŸ', 'å·è¶Š',
            'éŒå€‰', 'æ¹˜å—', 'ç®±æ ¹', 'ç†±æµ·', 'è»½äº•æ²¢', 'æ—¥å…‰', 'é‚£é ˆ', 'è‰æ´¥',
            
            # é–¢è¥¿ä¸»è¦åœ°å
            'äº¬éƒ½', 'å¤§é˜ª', 'ç¥æˆ¸', 'å¥ˆè‰¯', 'å’Œæ­Œå±±', 'æ»‹è³€', 'æ¯”å¡å±±', 'åµå±±', 'ç¥‡åœ’',
            'æ¸…æ°´', 'é‡‘é–£å¯º', 'éŠ€é–£å¯º', 'ä¼è¦‹', 'å®‡æ²»', 'å¹³å®‰äº¬', 'é›£æ³¢', 'æ¢…ç”°',
            
            # å¤å…¸ãƒ»æ­´å²åœ°å
            'æ±Ÿæˆ¸', 'å¹³å®‰äº¬', 'æ­¦è”µ', 'ç›¸æ¨¡', 'ç”²æ–', 'ä¿¡æ¿ƒ', 'è¶Šå¾Œ', 'ä¸‹é‡', 'ä¸Šé‡',
            'å¤§å’Œ', 'æ²³å†…', 'å’Œæ³‰', 'æ‘‚æ´¥', 'è¿‘æ±Ÿ', 'ç¾æ¿ƒ', 'å°¾å¼µ', 'ä¸‰æ²³', 'é£›é¨¨',
        ]
        
        # æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.enhanced_patterns = [
            {
                'pattern': r'(?<![ä¸€-é¾¯])[åŒ—æµ·é’æ£®å²©æ‰‹å®®åŸç§‹ç”°å±±å½¢ç¦å³¶èŒ¨åŸæ ƒæœ¨ç¾¤é¦¬åŸ¼ç‰åƒè‘‰æ±äº¬ç¥å¥ˆå·æ–°æ½Ÿå¯Œå±±çŸ³å·ç¦äº•å±±æ¢¨é•·é‡å²é˜œé™å²¡æ„›çŸ¥ä¸‰é‡æ»‹è³€äº¬éƒ½å¤§é˜ªå…µåº«å¥ˆè‰¯å’Œæ­Œå±±é³¥å–å³¶æ ¹å²¡å±±åºƒå³¶å±±å£å¾³å³¶é¦™å·æ„›åª›é«˜çŸ¥ç¦å²¡ä½è³€é•·å´ç†Šæœ¬å¤§åˆ†å®®å´é¹¿å…å³¶æ²–ç¸„][éƒ½é“åºœçœŒ](?![ä¸€-é¾¯])',
                'category': 'éƒ½é“åºœçœŒ',
                'confidence': 0.95,
                'priority': 1
            },
            {
                'pattern': r'(?<![ä¸€-é¾¯])[ä¸€-é¾¯]{2,6}[å¸‚åŒºç”ºæ‘](?![ä¸€-é¾¯])',
                'category': 'å¸‚åŒºç”ºæ‘',
                'confidence': 0.85,
                'priority': 2
            },
            {
                'pattern': r'(?<![ä¸€-é¾¯])[ä¸€-é¾¯]{1,4}[å±±å·æ¹–æµ·å³ è°·é‡åŸå³¶å²¬æµ¦å´](?![ä¸€-é¾¯])',
                'category': 'è‡ªç„¶åœ°å',
                'confidence': 0.75,
                'priority': 4
            }
        ]
        
    def _init_ai_verification(self):
        """AIæ¤œè¨¼æ©Ÿèƒ½ã®åˆæœŸåŒ–"""
        api_key = os.getenv('OPENAI_API_KEY')
        if api_key:
            self.openai_client = openai.OpenAI(api_key=api_key)
            self.ai_enabled = True
            logger.info("âœ… AIåœ°åæ¤œè¨¼æ©Ÿèƒ½æœ‰åŠ¹")
        else:
            self.openai_client = None
            self.ai_enabled = False
            logger.warning("âš ï¸ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆAIæ¤œè¨¼æ©Ÿèƒ½ç„¡åŠ¹ï¼‰")
    
    def extract_places_from_text(self, text: str) -> List[ExtractedPlace]:
        """çµ±åˆåœ°åæŠ½å‡ºï¼ˆGiNZA + Enhanced V2ï¼‰"""
        all_places = []
        
        # 1. GiNZAåœ°åæŠ½å‡ºï¼ˆå„ªå…ˆï¼‰
        if self.ginza_enabled:
            ginza_places = self._extract_with_ginza(text)
            all_places.extend(ginza_places)
            logger.debug(f"GiNZAæŠ½å‡º: {len(ginza_places)}ä»¶")
        
        # 2. æœ‰ååœ°åæŠ½å‡ºï¼ˆEnhanced V2ï¼‰
        famous_places = self._extract_famous_places(text)
        all_places.extend(famous_places)
        logger.debug(f"æœ‰ååœ°åæŠ½å‡º: {len(famous_places)}ä»¶")
        
        # 3. æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º
        pattern_places = self._extract_with_patterns(text)
        all_places.extend(pattern_places)
        logger.debug(f"ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º: {len(pattern_places)}ä»¶")
        
        # 4. é‡è¤‡é™¤å»ãƒ»çµ±åˆ
        unified_places = self._unify_and_deduplicate(all_places)
        logger.info(f"âœ… çµ±åˆåœ°åæŠ½å‡ºå®Œäº†: {len(unified_places)}ä»¶")
        
        return unified_places
    
    def _extract_with_ginza(self, text: str) -> List[ExtractedPlace]:
        """GiNZAæŠ½å‡ºå‡¦ç†"""
        places = []
        
        try:
            # å¤§å®¹é‡ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²å‡¦ç†
            max_length = 100000
            if len(text) > max_length:
                chunks = [text[i:i+max_length] for i in range(0, len(text), max_length)]
                for chunk in chunks:
                    places.extend(self._process_ginza_chunk(chunk))
            else:
                places = self._process_ginza_chunk(text)
        except Exception as e:
            logger.error(f"âŒ GiNZAæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return places
    
    def _process_ginza_chunk(self, text: str) -> List[ExtractedPlace]:
        """GiNZAãƒãƒ£ãƒ³ã‚¯å‡¦ç†"""
        places = []
        
        try:
            doc = self.nlp(text)
            
            # å›ºæœ‰è¡¨ç¾æŠ½å‡º
            for ent in doc.ents:
                if (ent.label_ in self.place_entity_types and 
                    self._is_valid_ginza_place(ent.text)):
                    
                    confidence = self._calculate_ginza_confidence(ent)
                    context_before, context_after = self._get_ginza_context(text, ent)
                    
                    place = ExtractedPlace(
                        name=ent.text,
                        canonical_name=self._normalize_place_name(ent.text),
                        place_type=self._categorize_ginza_place(ent),
                        confidence=confidence,
                        position=ent.start_char,
                        matched_text=ent.text,
                        context_before=context_before,
                        context_after=context_after,
                        extraction_method='ginza_ner',
                        priority=0,  # æœ€é«˜å„ªå…ˆåº¦
                        entity_type=ent.label_,
                        reading=self._get_reading(ent)
                    )
                    places.append(place)
            
            # å›ºæœ‰åè©è¿½åŠ ãƒã‚§ãƒƒã‚¯
            for token in doc:
                if (token.pos_ == 'PROPN' and 
                    len(token.text) >= 2 and
                    token.text not in [p.name for p in places] and
                    self._is_potential_place_name(token.text)):
                    
                    context_before, context_after = self._get_token_context(text, token)
                    
                    place = ExtractedPlace(
                        name=token.text,
                        canonical_name=self._normalize_place_name(token.text),
                        place_type='propn_place',
                        confidence=0.70,
                        position=token.idx,
                        matched_text=token.text,
                        context_before=context_before,
                        context_after=context_after,
                        extraction_method='ginza_propn',
                        priority=1,
                        pos_tag=token.pos_,
                        lemma=token.lemma_
                    )
                    places.append(place)
                    
        except Exception as e:
            logger.error(f"âŒ GiNZAãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        
        return places
    
    def _extract_famous_places(self, text: str) -> List[ExtractedPlace]:
        """æœ‰ååœ°åæŠ½å‡ºï¼ˆEnhanced V2çµ±åˆï¼‰"""
        places = []
        
        for place_name in self.famous_places:
            for match in re.finditer(re.escape(place_name), text):
                position = match.start()
                context_before = text[max(0, position-20):position]
                context_after = text[position+len(place_name):position+len(place_name)+20]
                
                place = ExtractedPlace(
                    name=place_name,
                    canonical_name=self._normalize_place_name(place_name),
                    place_type='æœ‰ååœ°å',
                    confidence=0.92,
                    position=position,
                    matched_text=place_name,
                    context_before=context_before,
                    context_after=context_after,
                    extraction_method='famous_places',
                    priority=0  # é«˜å„ªå…ˆåº¦
                )
                places.append(place)
        
        return places
    
    def _extract_with_patterns(self, text: str) -> List[ExtractedPlace]:
        """æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º"""
        places = []
        
        for pattern_info in self.enhanced_patterns:
            pattern = pattern_info['pattern']
            
            for match in re.finditer(pattern, text):
                name = match.group()
                position = match.start()
                
                if not self._is_likely_place_name(name, pattern_info['category']):
                    continue
                
                context_before = text[max(0, position-20):position]
                context_after = text[position+len(name):position+len(name)+20]
                
                place = ExtractedPlace(
                    name=name,
                    canonical_name=self._normalize_place_name(name),
                    place_type=pattern_info['category'],
                    confidence=pattern_info['confidence'],
                    position=position,
                    matched_text=name,
                    context_before=context_before,
                    context_after=context_after,
                    extraction_method='regex_pattern',
                    priority=pattern_info['priority']
                )
                places.append(place)
        
        return places
    
    def _unify_and_deduplicate(self, places: List[ExtractedPlace]) -> List[ExtractedPlace]:
        """åœ°åçµ±åˆãƒ»é‡è¤‡é™¤å»"""
        # æ­£è¦åŒ–åã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        place_groups = {}
        for place in places:
            canonical_name = place.canonical_name
            if canonical_name not in place_groups:
                place_groups[canonical_name] = []
            place_groups[canonical_name].append(place)
        
        # å„ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰æœ€é©ãªåœ°åã‚’é¸æŠ
        unified_places = []
        for canonical_name, group in place_groups.items():
            best_place = self._select_best_place(group)
            unified_places.append(best_place)
        
        # å„ªå…ˆåº¦ãƒ»ä¿¡é ¼åº¦ã§ã‚½ãƒ¼ãƒˆ
        unified_places.sort(key=lambda x: (x.priority, -x.confidence, x.position))
        
        return unified_places
    
    def _select_best_place(self, candidates: List[ExtractedPlace]) -> ExtractedPlace:
        """å€™è£œã‹ã‚‰æœ€é©ãªåœ°åã‚’é¸æŠï¼ˆæ”¹å–„ç‰ˆï¼šã‚ˆã‚Šå…·ä½“çš„ãªåœ°åã‚’å„ªå…ˆï¼‰"""
        if not candidates:
            return None
        
        # 1. ã‚ˆã‚Šå…·ä½“çš„ãªåœ°åï¼ˆé•·ã„åå‰ï¼‰ã‚’å„ªå…ˆ
        max_length = max(len(p.name) for p in candidates)
        longest_candidates = [p for p in candidates if len(p.name) == max_length]
        
        if len(longest_candidates) == 1:
            return longest_candidates[0]
        
        # 2. åŒã˜é•·ã•ã®å ´åˆã€GiNZAæŠ½å‡ºã‚’å„ªå…ˆ
        ginza_candidates = [p for p in longest_candidates if 'ginza' in p.extraction_method]
        if ginza_candidates:
            return max(ginza_candidates, key=lambda x: x.confidence)
        
        # 3. æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå®Œå…¨åœ°åï¼‰ã‚’å„ªå…ˆ
        pattern_candidates = [p for p in longest_candidates if p.extraction_method == 'regex_pattern']
        if pattern_candidates:
            return max(pattern_candidates, key=lambda x: x.confidence)
        
        # 4. æœ‰ååœ°åã‚’å„ªå…ˆ
        famous_candidates = [p for p in longest_candidates if p.extraction_method == 'famous_places']
        if famous_candidates:
            return max(famous_candidates, key=lambda x: x.confidence)
        
        # 5. ä¿¡é ¼åº¦æœ€é«˜ã‚’é¸æŠ
        return max(longest_candidates, key=lambda x: x.confidence)
    
    # ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¡ã‚½ãƒƒãƒ‰
    def _is_valid_ginza_place(self, place_name: str) -> bool:
        """GiNZAåœ°åå¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯"""
        if not place_name or len(place_name.strip()) <= 1:
            return False
        exclusions = {'æ—¥', 'æœˆ', 'å¹´', 'æ™‚', 'åˆ†', 'ç§’', 'äºº', 'æ–¹', 'é–“', 'å‰', 'å¾Œ'}
        return place_name not in exclusions
    
    def _is_potential_place_name(self, text: str) -> bool:
        """åœ°åå¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯"""
        place_suffixes = ['çœŒ', 'å¸‚', 'åŒº', 'ç”º', 'æ‘', 'å±±', 'å·', 'å³¶', 'é§…', 'æ¸¯']
        return any(text.endswith(suffix) for suffix in place_suffixes)
    
    def _is_likely_place_name(self, name: str, category: str) -> bool:
        """åœ°åå¦¥å½“æ€§åˆ¤å®šï¼ˆEnhanced V2çµ±åˆï¼‰"""
        # æ™‚é–“è¡¨ç¾é™¤å¤–
        time_prefixes = ['ä»Š', 'å…ˆæ—¥', 'æ˜¨æ—¥', 'æ˜æ—¥', 'ä»Šæ—¥', 'ä»Šå¤œ', 'å¤•æ–¹', 'æœ', 'åˆå‰', 'åˆå¾Œ']
        if any(name.startswith(prefix) for prefix in time_prefixes):
            return False
        
        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³
        exclude_patterns = [
            r'.*[åº—å±‹ç¤¾ä¼šé¤¨éƒ¨èª²å®¤ç§‘çµ„]$',
            r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]',
            r'.*[æ™‚åˆ†ç§’æ—¥æœˆå¹´]$',
            r'æ–‡è—éƒ½å¸‚',
            r'^å±±é“$'
        ]
        return not any(re.match(pattern, name) for pattern in exclude_patterns)
    
    def _calculate_ginza_confidence(self, ent) -> float:
        """GiNZAä¿¡é ¼åº¦è¨ˆç®—"""
        base_confidence = 0.75
        if ent.label_ == 'GPE':
            base_confidence += 0.20
        elif ent.label_ == 'LOC':
            base_confidence += 0.15
        elif ent.label_ == 'FACILITY':
            base_confidence += 0.10
        return min(base_confidence, 1.0)
    
    def _categorize_ginza_place(self, ent) -> str:
        """GiNZAåœ°åã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ†é¡"""
        category_map = {
            'GPE': 'geopolitical_entity',
            'LOC': 'location',
            'FACILITY': 'facility',
            'ORG': 'organization_place'
        }
        return category_map.get(ent.label_, 'unknown_place')
    
    def _get_reading(self, ent) -> str:
        """èª­ã¿ä»®åå–å¾—"""
        try:
            return getattr(ent, 'reading', '') or ''
        except:
            return ''
    
    def _get_ginza_context(self, text: str, ent) -> Tuple[str, str]:
        """GiNZAã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾—"""
        start = max(0, ent.start_char - 20)
        end = min(len(text), ent.end_char + 20)
        context_before = text[start:ent.start_char]
        context_after = text[ent.end_char:end]
        return context_before, context_after
    
    def _get_token_context(self, text: str, token) -> Tuple[str, str]:
        """ãƒˆãƒ¼ã‚¯ãƒ³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾—"""
        start = max(0, token.idx - 20)
        end = min(len(text), token.idx + len(token.text) + 20)
        context_before = text[start:token.idx]
        context_after = text[token.idx + len(token.text):end]
        return context_before, context_after
    
    def _normalize_place_name(self, name: str) -> str:
        """åœ°åæ­£è¦åŒ–ï¼ˆéšå±¤åœ°åçµ±ä¸€å¯¾å¿œç‰ˆï¼‰"""
        if not name:
            return name
            
        normalized = name.strip()
        
        # è¡¨è¨˜æºã‚Œçµ±ä¸€
        normalized = normalized.replace('ãƒ¶', 'ãŒ')
        normalized = normalized.replace('ã‚±', 'ãŒ') 
        normalized = normalized.replace('ãƒµ', 'ãŒ')
        normalized = normalized.replace('ã€€', ' ')
        
        # éšå±¤åœ°åã®æ­£è¦åŒ–ï¼ˆåŒ…å«é–¢ä¿‚å¯¾å¿œï¼‰
        # ä¾‹: æ±äº¬éƒ½æ–°å®¿åŒº â†’ æ–°å®¿ã€ç¥å¥ˆå·çœŒæ¨ªæµœå¸‚ â†’ æ¨ªæµœ
        if 'éƒ½' in normalized and ('åŒº' in normalized or 'å¸‚' in normalized):
            # éƒ½é“åºœçœŒ+å¸‚åŒºã‚’å«ã‚€å ´åˆã€å¸‚åŒºéƒ¨åˆ†ã‚’æŠ½å‡º
            parts = normalized.split('éƒ½')
            if len(parts) == 2:
                after_to = parts[1]
                if 'åŒº' in after_to:
                    # æ–°å®¿åŒº â†’ æ–°å®¿
                    return after_to.replace('åŒº', '')
                elif 'å¸‚' in after_to:
                    # æ¨ªæµœå¸‚ â†’ æ¨ªæµœ
                    return after_to.replace('å¸‚', '')
        
        if 'çœŒ' in normalized and ('åŒº' in normalized or 'å¸‚' in normalized):
            # çœŒ+å¸‚åŒºã‚’å«ã‚€å ´åˆã€å¸‚åŒºéƒ¨åˆ†ã‚’æŠ½å‡º
            parts = normalized.split('çœŒ')
            if len(parts) == 2:
                after_ken = parts[1]
                if 'å¸‚' in after_ken:
                    # æ¨ªæµœå¸‚ â†’ æ¨ªæµœ
                    return after_ken.replace('å¸‚', '')
                elif 'åŒº' in after_ken:
                    # ä¸­å¤®åŒº â†’ ä¸­å¤®
                    return after_ken.replace('åŒº', '')
        
        # åºœ+å¸‚åŒºã‚’å«ã‚€å ´åˆ
        if 'åºœ' in normalized and ('åŒº' in normalized or 'å¸‚' in normalized):
            parts = normalized.split('åºœ')
            if len(parts) == 2:
                after_fu = parts[1]
                if 'å¸‚' in after_fu:
                    return after_fu.replace('å¸‚', '')
                elif 'åŒº' in after_fu:
                    return after_fu.replace('åŒº', '')
        
        # å˜ç´”ãªéƒ½é“åºœçœŒã®æ­£è¦åŒ–
        if normalized.endswith('éƒ½') or normalized.endswith('åºœ') or normalized.endswith('çœŒ'):
            base = normalized[:-1]
            if base:
                return base
        
        # å¸‚åŒºç”ºæ‘ã®æ­£è¦åŒ–
        if normalized.endswith('å¸‚') or normalized.endswith('åŒº'):
            base = normalized[:-1]
            if base:
                return base
                
        return normalized

def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    extractor = GinzaEnhancedExtractor()
    
    test_text = """
    æ˜¨æ—¥ã€æ±äº¬éƒ½æ–°å®¿åŒºã‹ã‚‰ç¥å¥ˆå·çœŒæ¨ªæµœå¸‚ã¾ã§é›»è»Šã§ç§»å‹•ã—ã¾ã—ãŸã€‚
    åŒ—æµ·é“ã®æœ­å¹Œå¸‚ã¯é›ªãŒç¾ã—ã„è¡—ã§ã™ã€‚
    å¤ã„æ™‚ä»£ã®æ±Ÿæˆ¸ã‹ã‚‰æ˜æ²»ã®æ±äº¬ã¸ã®å¤‰é·ã¯èˆˆå‘³æ·±ã„ã‚‚ã®ãŒã‚ã‚Šã¾ã™ã€‚
    äº¬éƒ½ã®é‡‘é–£å¯ºã‚„å¥ˆè‰¯ã®æ±å¤§å¯ºã‚’è¦‹å­¦ã—ã¾ã—ãŸã€‚
    å¯Œå£«å±±ã®é ‚ä¸Šã‹ã‚‰è¦‹ã‚‹æ—¥æœ¬ã®æ™¯è‰²ã¯æ ¼åˆ¥ã§ã™ã€‚
    """
    
    print("=== ğŸŒŸ GinZAçµ±åˆåœ°åæŠ½å‡ºå™¨v4.0ãƒ†ã‚¹ãƒˆ ===")
    places = extractor.extract_places_from_text(test_text)
    
    print(f"âœ… æŠ½å‡ºåœ°åæ•°: {len(places)}ä»¶")
    print(f"ğŸ”§ GiNZAåˆ©ç”¨å¯èƒ½: {extractor.ginza_enabled}")
    print(f"ğŸ¤– AIæ¤œè¨¼æ©Ÿèƒ½: {extractor.ai_enabled}")
    
    for i, place in enumerate(places, 1):
        print(f"ğŸ—ºï¸ {i}. {place.name} [{place.place_type}] "
              f"({place.extraction_method}, ä¿¡é ¼åº¦: {place.confidence:.2f})")
        if place.reading:
            print(f"    èª­ã¿: {place.reading}")

if __name__ == "__main__":
    main() 