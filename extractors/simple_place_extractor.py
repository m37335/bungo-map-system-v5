#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ã‚·ãƒ³ãƒ—ãƒ«åœ°åæŠ½å‡ºæ©Ÿèƒ½

ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ã‹ã‚‰åœ°åã‚’æŠ½å‡ºã—ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã™ã‚‹å®Ÿç”¨çš„ãªæ©Ÿèƒ½ã€‚
æ­£è¦è¡¨ç¾ãƒ™ãƒ¼ã‚¹ã®ç¢ºå®ŸãªæŠ½å‡ºã«ç‰¹åŒ–ã€‚
"""

import re
import sqlite3
import logging
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime
import argparse

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class ExtractedPlace:
    """æŠ½å‡ºã•ã‚ŒãŸåœ°åã®æƒ…å ±"""
    name: str
    canonical_name: str
    place_type: str
    confidence: float
    position: int
    matched_text: str
    context_before: str
    context_after: str
    extraction_method: str

class SimplePlaceExtractor:
    """ã‚·ãƒ³ãƒ—ãƒ«åœ°åæŠ½å‡ºã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self._init_patterns()
        logger.info("ğŸ—ºï¸ ã‚·ãƒ³ãƒ—ãƒ«åœ°åæŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        
    def _init_patterns(self):
        """åœ°åæŠ½å‡ºç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆæœŸåŒ–"""
        
        # éƒ½é“åºœçœŒãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆçœŒãƒ»åºœãƒ»éƒ½ãƒ»é“ã‚’å«ã‚€ï¼‰
        self.prefecture_list = [
            'åŒ—æµ·é“', 'é’æ£®çœŒ', 'å²©æ‰‹çœŒ', 'å®®åŸçœŒ', 'ç§‹ç”°çœŒ', 'å±±å½¢çœŒ', 'ç¦å³¶çœŒ',
            'èŒ¨åŸçœŒ', 'æ ƒæœ¨çœŒ', 'ç¾¤é¦¬çœŒ', 'åŸ¼ç‰çœŒ', 'åƒè‘‰çœŒ', 'æ±äº¬éƒ½', 'ç¥å¥ˆå·çœŒ',
            'æ–°æ½ŸçœŒ', 'å¯Œå±±çœŒ', 'çŸ³å·çœŒ', 'ç¦äº•çœŒ', 'å±±æ¢¨çœŒ', 'é•·é‡çœŒ',
            'å²é˜œçœŒ', 'é™å²¡çœŒ', 'æ„›çŸ¥çœŒ', 'ä¸‰é‡çœŒ',
            'æ»‹è³€çœŒ', 'äº¬éƒ½åºœ', 'å¤§é˜ªåºœ', 'å…µåº«çœŒ', 'å¥ˆè‰¯çœŒ', 'å’Œæ­Œå±±çœŒ',
            'é³¥å–çœŒ', 'å³¶æ ¹çœŒ', 'å²¡å±±çœŒ', 'åºƒå³¶çœŒ', 'å±±å£çœŒ',
            'å¾³å³¶çœŒ', 'é¦™å·çœŒ', 'æ„›åª›çœŒ', 'é«˜çŸ¥çœŒ',
            'ç¦å²¡çœŒ', 'ä½è³€çœŒ', 'é•·å´çœŒ', 'ç†Šæœ¬çœŒ', 'å¤§åˆ†çœŒ', 'å®®å´çœŒ', 'é¹¿å…å³¶çœŒ', 'æ²–ç¸„çœŒ'
        ]
        
        # ä¸»è¦éƒ½å¸‚ãƒ»æœ‰ååœ°åï¼ˆç¢ºå®Ÿã«åœ°åã¨ã—ã¦èªè­˜ã§ãã‚‹ã‚‚ã®ï¼‰
        self.famous_places = [
            # æ±äº¬å‘¨è¾º
            'éŠ€åº§', 'æ–°å®¿', 'æ¸‹è°·', 'ä¸Šé‡', 'æµ…è‰', 'å“å·', 'æ± è¢‹', 'æ–°æ©‹', 'æœ‰æ¥½ç”º',
            'ç¥ç”°', 'æ—¥æœ¬æ©‹', 'æœ¬éƒ·', 'èµ¤å‚', 'é’å±±', 'è¡¨å‚é“', 'æµæ¯”å¯¿', 'ä»£å®˜å±±',
            
            # é–¢æ±ä¸»è¦éƒ½å¸‚
            'æ¨ªæµœ', 'å·å´', 'åƒè‘‰', 'èˆ¹æ©‹', 'æŸ', 'éŒå€‰', 'æ¹˜å—', 'ç®±æ ¹', 'ç†±æµ·',
            'å®‡éƒ½å®®', 'é«˜å´', 'æ°´æˆ¸', 'ã¤ãã°',
            
            # é–¢è¥¿
            'äº¬éƒ½', 'å¤§é˜ª', 'ç¥æˆ¸', 'å¥ˆè‰¯', 'å’Œæ­Œå±±', 'å§«è·¯', 
            
            # ãã®ä»–ä¸»è¦éƒ½å¸‚
            'æœ­å¹Œ', 'ä»™å°', 'åå¤å±‹', 'é‡‘æ²¢', 'ç¦å²¡', 'é•·å´', 'ç†Šæœ¬', 'é¹¿å…å³¶',
            'åºƒå³¶', 'å²¡å±±', 'é«˜æ¾', 'æ¾å±±', 'é«˜çŸ¥',
            
            # æ­´å²åœ°å
            'æ±Ÿæˆ¸', 'ç”²æ–', 'ä¿¡æ¿ƒ', 'è¿‘æ±Ÿ', 'å±±åŸ', 'å¤§å’Œ', 'æ²³å†…', 'å’Œæ³‰', 'æ‘‚æ´¥',
            'ä¼Šå‹¢', 'å°¾å¼µ', 'ä¸‰æ²³', 'é§¿æ²³', 'é æ±Ÿ', 'ç›¸æ¨¡', 'æ­¦è”µ', 'ä¸‹é‡',
            'å¸¸é™¸', 'ä¸‹ç·', 'ä¸Šç·', 'å®‰æˆ¿', 'è¶Šå¾Œ', 'è¶Šä¸­', 'è¶Šå‰', 'åŠ è³€', 'èƒ½ç™»',
            
            # è‡ªç„¶åœ°å
            'å¯Œå£«å±±', 'çµç¶æ¹–', 'ç€¬æˆ¸å†…æµ·', 'æ±äº¬æ¹¾', 'ç›¸æ¨¡æ¹¾', 'é§¿æ²³æ¹¾'
        ]
        
        # å¸‚åŒºç”ºæ‘ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆä¸€èˆ¬çš„ãªå¸‚åŒºç”ºæ‘åï¼‰
        self.city_suffixes = ['å¸‚', 'åŒº', 'ç”º', 'æ‘']
        self.gun_suffixes = ['éƒ¡']
        
    def extract_places_from_text(self, text: str) -> List[ExtractedPlace]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰åœ°åã‚’æŠ½å‡º"""
        extracted_places = []
        
        # 1. éƒ½é“åºœçœŒæŠ½å‡º
        for prefecture in self.prefecture_list:
            places = self._extract_by_exact_match(text, prefecture, 'éƒ½é“åºœçœŒ', 0.95)
            extracted_places.extend(places)
            
        # 2. æœ‰ååœ°åæŠ½å‡º
        for place_name in self.famous_places:
            places = self._extract_by_exact_match(text, place_name, 'æœ‰ååœ°å', 0.90)
            extracted_places.extend(places)
            
        # 3. å¸‚åŒºç”ºæ‘ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º
        city_places = self._extract_cities(text)
        extracted_places.extend(city_places)
        
        # 4. éƒ¡ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º
        gun_places = self._extract_guns(text)
        extracted_places.extend(gun_places)
        
        # é‡è¤‡é™¤å»ã¨ã‚½ãƒ¼ãƒˆ
        extracted_places = self._remove_duplicates(extracted_places)
        extracted_places.sort(key=lambda x: x.position)
        
        return extracted_places
        
    def _extract_by_exact_match(self, text: str, place_name: str, place_type: str, confidence: float) -> List[ExtractedPlace]:
        """å®Œå…¨ä¸€è‡´ã«ã‚ˆã‚‹åœ°åæŠ½å‡º"""
        places = []
        
        # ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã—ã¦å®Œå…¨ä¸€è‡´æ¤œç´¢
        pattern = re.escape(place_name)
        
        for match in re.finditer(pattern, text):
            position = match.start()
            
            # å‰å¾Œã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
            context_before = text[max(0, position-15):position]
            context_after = text[position+len(place_name):position+len(place_name)+15]
            
            place = ExtractedPlace(
                name=place_name,
                canonical_name=self._normalize_place_name(place_name),
                place_type=place_type,
                confidence=confidence,
                position=position,
                matched_text=place_name,
                context_before=context_before,
                context_after=context_after,
                extraction_method='exact_match'
            )
            
            places.append(place)
            
        return places
        
    def _extract_cities(self, text: str) -> List[ExtractedPlace]:
        """å¸‚åŒºç”ºæ‘ãƒ‘ã‚¿ãƒ¼ãƒ³ã§åœ°åæŠ½å‡º"""
        places = []
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³: 2-8æ–‡å­—ã®æ¼¢å­— + å¸‚åŒºç”ºæ‘
        for suffix in self.city_suffixes:
            pattern = f'[ä¸€-é¾¯]{{2,8}}{re.escape(suffix)}'
            
            for match in re.finditer(pattern, text):
                name = match.group()
                position = match.start()
                
                # æ˜ã‚‰ã‹ã«åœ°åã§ãªã„ã‚‚ã®ã‚’é™¤å¤–
                if self._is_likely_place_name(name):
                    context_before = text[max(0, position-15):position]
                    context_after = text[position+len(name):position+len(name)+15]
                    
                    place = ExtractedPlace(
                        name=name,
                        canonical_name=self._normalize_place_name(name),
                        place_type='å¸‚åŒºç”ºæ‘',
                        confidence=0.80,
                        position=position,
                        matched_text=name,
                        context_before=context_before,
                        context_after=context_after,
                        extraction_method='pattern_match'
                    )
                    
                    places.append(place)
                    
        return places
        
    def _extract_guns(self, text: str) -> List[ExtractedPlace]:
        """éƒ¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã§åœ°åæŠ½å‡º"""
        places = []
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³: 2-6æ–‡å­—ã®æ¼¢å­— + éƒ¡
        pattern = r'[ä¸€-é¾¯]{2,6}éƒ¡'
        
        for match in re.finditer(pattern, text):
            name = match.group()
            position = match.start()
            
            context_before = text[max(0, position-15):position]
            context_after = text[position+len(name):position+len(name)+15]
            
            place = ExtractedPlace(
                name=name,
                canonical_name=self._normalize_place_name(name),
                place_type='éƒ¡',
                confidence=0.75,
                position=position,
                matched_text=name,
                context_before=context_before,
                context_after=context_after,
                extraction_method='pattern_match'
            )
            
            places.append(place)
            
        return places
        
    def _is_likely_place_name(self, name: str) -> bool:
        """åœ°åã‚‰ã—ã„åå‰ã‹ã©ã†ã‹ã®ç°¡å˜ãªåˆ¤å®š"""
        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³
        exclude_patterns = [
            r'.*[åº—å±‹ç¤¾ä¼šé¤¨]$',  # åº—èˆ—åã€æ–½è¨­å
            r'.*[éƒ¨èª²å®¤ç§‘]$',    # çµ„ç¹”å
            r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]',  # æ•°å­—ã§å§‹ã¾ã‚‹
        ]
        
        for pattern in exclude_patterns:
            if re.match(pattern, name):
                return False
                
        return True
        
    def _normalize_place_name(self, name: str) -> str:
        """åœ°åã‚’æ­£è¦åŒ–"""
        # éƒ½é“åºœçœŒã®æ­£è¦åŒ–
        normalized = re.sub(r'[çœŒåºœéƒ½é“]$', '', name)
        return normalized if normalized else name
        
    def _remove_duplicates(self, places: List[ExtractedPlace]) -> List[ExtractedPlace]:
        """é‡è¤‡ã™ã‚‹åœ°åã‚’é™¤å»"""
        seen = set()
        unique_places = []
        
        for place in places:
            # åŒä¸€ä½ç½®ã®åŒä¸€åœ°åã¯é‡è¤‡ã¨ã¿ãªã™
            key = (place.canonical_name, place.position)
            if key not in seen:
                seen.add(key)
                unique_places.append(place)
                
        return unique_places
        
    def save_places_to_db(self, sentence_id: int, places: List[ExtractedPlace]) -> int:
        """æŠ½å‡ºã•ã‚ŒãŸåœ°åã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
        if not places:
            return 0
            
        saved_count = 0
        
        for place in places:
            try:
                # åœ°åãƒã‚¹ã‚¿ãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ«ã«è¿½åŠ 
                place_id = self._get_or_create_place(place)
                
                # ã‚»ãƒ³ãƒ†ãƒ³ã‚¹-åœ°åé–¢é€£ãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜
                self._save_sentence_place_relation(sentence_id, place_id, place)
                
                saved_count += 1
                logger.debug(f"åœ°åä¿å­˜: {place.name} (ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ {sentence_id})")
                
            except Exception as e:
                logger.error(f"åœ°åä¿å­˜ã‚¨ãƒ©ãƒ¼: {place.name} - {e}")
                
        return saved_count
        
    def _get_or_create_place(self, place: ExtractedPlace) -> int:
        """åœ°åãƒã‚¹ã‚¿ãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰åœ°åIDã‚’å–å¾—ã€å­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ"""
        conn = sqlite3.connect('data/bungo_map.db')
        cursor = conn.cursor()
        
        try:
            # æ—¢å­˜åœ°åã‚’æ¤œç´¢
            cursor.execute("""
                SELECT place_id FROM places 
                WHERE canonical_name = ? OR place_name = ?
            """, (place.canonical_name, place.name))
            
            result = cursor.fetchone()
            if result:
                return result[0]
                
            # æ–°è¦åœ°åã‚’ä½œæˆ
            cursor.execute("""
                INSERT INTO places (
                    place_name, canonical_name, place_type, confidence,
                    mention_count, source_system, verification_status,
                    created_at
                ) VALUES (?, ?, ?, ?, 1, 'simple_place_extractor', 'auto_extracted', ?)
            """, (
                place.name, place.canonical_name, place.place_type, 
                place.confidence, datetime.now()
            ))
            
            place_id = cursor.lastrowid
            conn.commit()
            
            logger.info(f"æ–°è¦åœ°åç™»éŒ²: {place.name} (ID: {place_id}, ã‚¿ã‚¤ãƒ—: {place.place_type})")
            return place_id
            
        finally:
            conn.close()
            
    def _save_sentence_place_relation(self, sentence_id: int, place_id: int, place: ExtractedPlace):
        """ã‚»ãƒ³ãƒ†ãƒ³ã‚¹-åœ°åé–¢é€£ã‚’ä¿å­˜"""
        conn = sqlite3.connect('data/bungo_map.db')
        cursor = conn.cursor()
        
        try:
            cursor.execute("""
                INSERT INTO sentence_places (
                    sentence_id, place_id, extraction_method, confidence,
                    position_in_sentence, context_before, context_after,
                    matched_text, verification_status, created_at
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, 'auto_extracted', ?)
            """, (
                sentence_id, place_id, place.extraction_method, place.confidence,
                place.position, place.context_before, place.context_after,
                place.matched_text, datetime.now()
            ))
            
            conn.commit()
            
        finally:
            conn.close()
            
    def process_sentences_batch(self, limit: Optional[int] = None) -> Dict[str, int]:
        """ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ã‚’ä¸€æ‹¬å‡¦ç†ã—ã¦åœ°åæŠ½å‡ºã‚’å®Ÿè¡Œ"""
        conn = sqlite3.connect('data/bungo_map.db')
        cursor = conn.cursor()
        
        try:
            # æœªå‡¦ç†ã®ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ã‚’å–å¾—
            query = """
                SELECT s.sentence_id, s.sentence_text 
                FROM sentences s
                LEFT JOIN sentence_places sp ON s.sentence_id = sp.sentence_id
                WHERE sp.sentence_id IS NULL
            """
            
            if limit:
                query += f" LIMIT {limit}"
                
            cursor.execute(query)
            sentences = cursor.fetchall()
            
            logger.info(f"ğŸ¯ å‡¦ç†å¯¾è±¡ã‚»ãƒ³ãƒ†ãƒ³ã‚¹: {len(sentences)}ä»¶")
            
            stats = {
                'processed_sentences': 0,
                'extracted_places': 0,
                'saved_places': 0,
                'errors': 0
            }
            
            for sentence_id, sentence_text in sentences:
                try:
                    # åœ°åæŠ½å‡º
                    places = self.extract_places_from_text(sentence_text)
                    
                    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜
                    saved_count = self.save_places_to_db(sentence_id, places)
                    
                    stats['processed_sentences'] += 1
                    stats['extracted_places'] += len(places)
                    stats['saved_places'] += saved_count
                    
                    if len(places) > 0:
                        logger.info(f"âœ… ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ {sentence_id}: {len(places)}ä»¶æŠ½å‡º â†’ {saved_count}ä»¶ä¿å­˜")
                        for place in places:
                            logger.debug(f"   - {place.name} ({place.place_type}, {place.confidence:.2f})")
                        
                except Exception as e:
                    stats['errors'] += 1
                    logger.error(f"âŒ ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ {sentence_id} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    
            return stats
            
        finally:
            conn.close()
            
    def get_statistics(self) -> Dict[str, any]:
        """åœ°åæŠ½å‡ºã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        conn = sqlite3.connect('data/bungo_map.db')
        cursor = conn.cursor()
        
        try:
            stats = {}
            
            # åŸºæœ¬çµ±è¨ˆ
            cursor.execute("SELECT COUNT(*) FROM sentences")
            stats['total_sentences'] = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(DISTINCT sentence_id) FROM sentence_places")
            stats['processed_sentences'] = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM places")
            stats['total_places'] = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM sentence_places")
            stats['total_relations'] = cursor.fetchone()[0]
            
            # åœ°åç¨®åˆ¥çµ±è¨ˆ
            cursor.execute("""
                SELECT place_type, COUNT(*) 
                FROM places 
                GROUP BY place_type
                ORDER BY COUNT(*) DESC
            """)
            stats['place_types'] = dict(cursor.fetchall())
            
            # æŠ½å‡ºæ‰‹æ³•çµ±è¨ˆ
            cursor.execute("""
                SELECT extraction_method, COUNT(*) 
                FROM sentence_places 
                GROUP BY extraction_method
                ORDER BY COUNT(*) DESC
            """)
            stats['extraction_methods'] = dict(cursor.fetchall())
            
            # å‡¦ç†ç‡
            if stats['total_sentences'] > 0:
                stats['processing_rate'] = stats['processed_sentences'] / stats['total_sentences'] * 100
            else:
                stats['processing_rate'] = 0
                
            return stats
            
        finally:
            conn.close()

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = argparse.ArgumentParser(description='ã‚·ãƒ³ãƒ—ãƒ«åœ°åæŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ ')
    parser.add_argument('--limit', type=int, help='å‡¦ç†ã™ã‚‹ã‚»ãƒ³ãƒ†ãƒ³ã‚¹æ•°ã®ä¸Šé™')
    parser.add_argument('--stats-only', action='store_true', help='çµ±è¨ˆæƒ…å ±ã®ã¿è¡¨ç¤º')
    
    args = parser.parse_args()
    
    extractor = SimplePlaceExtractor()
    
    # çµ±è¨ˆæƒ…å ±è¡¨ç¤º
    print("=== ğŸ—ºï¸  åœ°åæŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆ ===")
    stats = extractor.get_statistics()
    print(f"ç·ã‚»ãƒ³ãƒ†ãƒ³ã‚¹æ•°: {stats['total_sentences']:,}")
    print(f"å‡¦ç†æ¸ˆã¿ã‚»ãƒ³ãƒ†ãƒ³ã‚¹æ•°: {stats['processed_sentences']:,}")
    print(f"å‡¦ç†ç‡: {stats['processing_rate']:.1f}%")
    print(f"æŠ½å‡ºæ¸ˆã¿åœ°åæ•°: {stats['total_places']:,}")
    print(f"åœ°å-ã‚»ãƒ³ãƒ†ãƒ³ã‚¹é–¢é€£æ•°: {stats['total_relations']:,}")
    
    if stats.get('place_types'):
        print("\nğŸ“ åœ°åç¨®åˆ¥:")
        for place_type, count in stats['place_types'].items():
            print(f"  {place_type}: {count}ä»¶")
    
    if stats.get('extraction_methods'):
        print("\nğŸ”§ æŠ½å‡ºæ‰‹æ³•:")
        for method, count in stats['extraction_methods'].items():
            print(f"  {method}: {count}ä»¶")
    
    if args.stats_only:
        return
    
    # åœ°åæŠ½å‡ºå‡¦ç†å®Ÿè¡Œ
    print("\n=== ğŸš€ åœ°åæŠ½å‡ºå‡¦ç†é–‹å§‹ ===")
    result = extractor.process_sentences_batch(limit=args.limit)
    
    print("\n=== ğŸ“Š å‡¦ç†çµæœ ===")
    print(f"å‡¦ç†ã‚»ãƒ³ãƒ†ãƒ³ã‚¹æ•°: {result['processed_sentences']:,}")
    print(f"æŠ½å‡ºåœ°åæ•°: {result['extracted_places']:,}")
    print(f"ä¿å­˜åœ°åæ•°: {result['saved_places']:,}")
    print(f"ã‚¨ãƒ©ãƒ¼æ•°: {result['errors']:,}")
    
    # æ›´æ–°å¾Œçµ±è¨ˆ
    if result['processed_sentences'] > 0:
        print("\n=== ğŸ“ˆ æ›´æ–°å¾Œçµ±è¨ˆ ===")
        final_stats = extractor.get_statistics()
        print(f"ç·ã‚»ãƒ³ãƒ†ãƒ³ã‚¹æ•°: {final_stats['total_sentences']:,}")
        print(f"å‡¦ç†æ¸ˆã¿ã‚»ãƒ³ãƒ†ãƒ³ã‚¹æ•°: {final_stats['processed_sentences']:,}")
        print(f"å‡¦ç†ç‡: {final_stats['processing_rate']:.1f}%")
        print(f"æŠ½å‡ºæ¸ˆã¿åœ°åæ•°: {final_stats['total_places']:,}")
        
        if final_stats.get('place_types'):
            print("\nğŸ“ åœ°åç¨®åˆ¥ï¼ˆæ›´æ–°å¾Œï¼‰:")
            for place_type, count in final_stats['place_types'].items():
                print(f"  {place_type}: {count}ä»¶")

if __name__ == "__main__":
    main() 