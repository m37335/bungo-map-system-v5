from typing import List, Dict, Any, Optional, Tuple
import logging
import re
from dataclasses import dataclass
from ..ai.llm import LLMManager
from ..ai.nlp import NLPManager, NLPResult
import sqlite3
from datetime import datetime
import json

logger = logging.getLogger(__name__)

@dataclass
class ExtractedPlace:
    """æŠ½å‡ºã•ã‚ŒãŸåœ°åæƒ…å ±"""
    name: str
    context: str
    confidence: float
    source: str
    metadata: Dict[str, Any]
    position: int = 0
    sentence_index: int = 0
    category: str = ""
    matched_text: str = ""

class EnhancedPlaceExtractor:
    """å¼·åŒ–ç‰ˆåœ°åæŠ½å‡ºã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆlegacyã‹ã‚‰ã®æ”¹è‰¯çµ±åˆç‰ˆï¼‰"""
    
    def __init__(self):
        self.llm_manager = LLMManager()
        self.nlp_manager = NLPManager()
        self.db_manager = DatabaseManager()
        
        # æŠ½å‡ºãƒ¬ãƒ™ãƒ«ã¨ãã®ä¿¡é ¼åº¦é–¾å€¤
        self.extraction_levels = {
            "llm_context": 0.8,    # LLMæ–‡è„ˆç†è§£
            "famous_place": 0.7,   # æœ‰ååœ°å
            "complete_name": 0.95, # å®Œå…¨åœ°åï¼ˆéƒ½é“åºœçœŒ+å¸‚åŒºç”ºæ‘ï¼‰
            "city": 0.6,          # å¸‚åŒºç”ºæ‘
            "prefecture": 0.5,     # éƒ½é“åºœçœŒ
            "district": 0.4        # éƒ¡
        }
        
        # å¼·åŒ–ç‰ˆæ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.patterns = self._build_enhanced_patterns()
        
        logger.info("ğŸ—ºï¸ å¼·åŒ–ç‰ˆåœ°åæŠ½å‡ºã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")

    def _build_enhanced_patterns(self) -> List[Dict]:
        """å¼·åŒ–ã•ã‚ŒãŸåœ°åæŠ½å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³"""
        return [
            # 0. å®Œå…¨åœ°åï¼ˆéƒ½é“åºœçœŒ+å¸‚åŒºç”ºæ‘ï¼‰ - æœ€é«˜å„ªå…ˆåº¦
            {
                'pattern': r'(?<![ä¸€-é¾¯])[åŒ—æµ·é’æ£®å²©æ‰‹å®®åŸç§‹ç”°å±±å½¢ç¦å³¶èŒ¨åŸæ ƒæœ¨ç¾¤é¦¬åŸ¼ç‰åƒè‘‰æ±äº¬ç¥å¥ˆå·æ–°æ½Ÿå¯Œå±±çŸ³å·ç¦äº•å±±æ¢¨é•·é‡å²é˜œé™å²¡æ„›çŸ¥ä¸‰é‡æ»‹è³€äº¬éƒ½å¤§é˜ªå…µåº«å¥ˆè‰¯å’Œæ­Œå±±é³¥å–å³¶æ ¹å²¡å±±åºƒå³¶å±±å£å¾³å³¶é¦™å·æ„›åª›é«˜çŸ¥ç¦å²¡ä½è³€é•·å´ç†Šæœ¬å¤§åˆ†å®®å´é¹¿å…å³¶æ²–ç¸„][éƒ½é“åºœçœŒ][ä¸€-é¾¯]{2,8}[å¸‚åŒºç”ºæ‘](?![ä¸€-é¾¯])',
                'category': 'å®Œå…¨åœ°å',
                'confidence': 0.98,
                'priority': 0
            },
            
            # 1. éƒ½é“åºœçœŒï¼ˆå¢ƒç•Œæ¡ä»¶å¼·åŒ–ï¼‰
            {
                'pattern': r'(?<![ä¸€-é¾¯])[åŒ—æµ·é’æ£®å²©æ‰‹å®®åŸç§‹ç”°å±±å½¢ç¦å³¶èŒ¨åŸæ ƒæœ¨ç¾¤é¦¬åŸ¼ç‰åƒè‘‰æ±äº¬ç¥å¥ˆå·æ–°æ½Ÿå¯Œå±±çŸ³å·ç¦äº•å±±æ¢¨é•·é‡å²é˜œé™å²¡æ„›çŸ¥ä¸‰é‡æ»‹è³€äº¬éƒ½å¤§é˜ªå…µåº«å¥ˆè‰¯å’Œæ­Œå±±é³¥å–å³¶æ ¹å²¡å±±åºƒå³¶å±±å£å¾³å³¶é¦™å·æ„›åª›é«˜çŸ¥ç¦å²¡ä½è³€é•·å´ç†Šæœ¬å¤§åˆ†å®®å´é¹¿å…å³¶æ²–ç¸„][éƒ½é“åºœçœŒ](?![ä¸€-é¾¯])',
                'category': 'éƒ½é“åºœçœŒ',
                'confidence': 0.95,
                'priority': 1
            },
            
            # 2. å¸‚åŒºç”ºæ‘ï¼ˆå¢ƒç•Œæ¡ä»¶å¼·åŒ–ï¼‰
            {
                'pattern': r'(?<![ä¸€-é¾¯])[ä¸€-é¾¯]{2,6}[å¸‚åŒºç”ºæ‘](?![ä¸€-é¾¯])',
                'category': 'å¸‚åŒºç”ºæ‘',
                'confidence': 0.85,
                'priority': 2
            },
            
            # 3. æœ‰ååœ°åï¼ˆæ˜ç¤ºãƒªã‚¹ãƒˆï¼‰
            {
                'pattern': r'(?:' + '|'.join([
                    'éŠ€åº§', 'æ–°å®¿', 'æ¸‹è°·', 'ä¸Šé‡', 'æµ…è‰', 'å“å·', 'æ± è¢‹', 'æ–°æ©‹', 'æœ‰æ¥½ç”º',
                    'æ¨ªæµœ', 'å·å´', 'åƒè‘‰', 'èˆ¹æ©‹', 'æŸ', 'éŒå€‰', 'æ¹˜å—', 'ç®±æ ¹',
                    'äº¬éƒ½', 'å¤§é˜ª', 'ç¥æˆ¸', 'å¥ˆè‰¯', 'æ±Ÿæˆ¸', 'æœ¬éƒ·', 'ç¥ç”°', 'æ—¥æœ¬æ©‹',
                    'æ´¥è»½', 'æ¾å±±', 'å››å›½', 'ä¹å·', 'æœ¬å·', 'åŒ—æµ·é“', 'å¯Œå£«å±±', 'çµç¶æ¹–'
                ]) + r')',
                'category': 'æœ‰ååœ°å',
                'confidence': 0.90,
                'priority': 3
            },
            
            # 4. è‡ªç„¶åœ°åãƒ‘ã‚¿ãƒ¼ãƒ³
            {
                'pattern': r'(?<![ä¸€-é¾¯])[ä¸€-é¾¯]{1,4}[å·å±±æ¹–æµ·å³ è°·é‡åŸå³¶å²¬æµ¦å´](?![ä¸€-é¾¯])',
                'category': 'è‡ªç„¶åœ°å',
                'confidence': 0.80,
                'priority': 4
            },
            
            # 5. æ­´å²åœ°åãƒ»å¤ã„åœ°å
            {
                'pattern': r'(?<![ä¸€-é¾¯])[ä¸€-é¾¯]{2,5}[å›½éƒ¡è—©æ‘å®¿é§…é–¢æ‰€åŸ](?![ä¸€-é¾¯])',
                'category': 'æ­´å²åœ°å',
                'confidence': 0.75,
                'priority': 5
            }
        ]

    async def extract_places(
        self,
        text: str,
        work_title: str = "",
        author_name: str = ""
    ) -> List[ExtractedPlace]:
        """åœ°åã®æŠ½å‡ºï¼ˆçµ±åˆç‰ˆï¼‰"""
        try:
            if not text or len(text) < 10:
                logger.warning(f"ãƒ†ã‚­ã‚¹ãƒˆãŒçŸ­ã™ãã¾ã™: {len(text)}æ–‡å­—")
                return []
            
            # 1. æ­£è¦è¡¨ç¾ã«ã‚ˆã‚‹é«˜é€ŸæŠ½å‡º
            regex_places = self._extract_with_regex(text)
            
            # 2. NLPã«ã‚ˆã‚‹åŸºæœ¬çš„ãªåœ°åæŠ½å‡º
            nlp_result = self.nlp_manager.analyze_text(text)
            nlp_places = self._process_nlp_entities(nlp_result, text)
            
            # 3. LLMã«ã‚ˆã‚‹æ–‡è„ˆç†è§£ã¨åœ°åæŠ½å‡ºï¼ˆã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ï¼‰
            sample_text = text[:2000] if len(text) > 2000 else text
            llm_places = await self.llm_manager.extract_places(sample_text)
            
            # 4. åœ°åã®çµ±åˆã¨é‡è¤‡é™¤å»
            all_places = regex_places + nlp_places + llm_places
            merged_places = self._merge_and_deduplicate_places(all_places)
            
            # 5. æ–‡è„ˆåˆ†æã®è¿½åŠ 
            if work_title or author_name:
                places_with_context = await self._add_context_analysis(
                    merged_places,
                    work_title,
                    author_name
                )
            else:
                places_with_context = merged_places
            
            logger.info(f"âœ… åœ°åæŠ½å‡ºå®Œäº†: {len(places_with_context)}ä»¶")
            return places_with_context
            
        except Exception as e:
            logger.error(f"åœ°åæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []

    def _extract_with_regex(self, text: str) -> List[Dict[str, Any]]:
        """æ­£è¦è¡¨ç¾ã«ã‚ˆã‚‹åœ°åæŠ½å‡º"""
        all_matches = []
        sentences = self._split_into_sentences(text)
        
        for sentence_idx, sentence in enumerate(sentences):
            sentence_matches = self._extract_from_sentence(
                sentence, sentence_idx, sentences
            )
            # é‡è¤‡æ’é™¤å‡¦ç†
            deduplicated_matches = self._deduplicate_overlapping_matches(sentence_matches)
            all_matches.extend(deduplicated_matches)
        
        # è¾æ›¸ã‹ã‚‰ExtractedPlaceã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¸ã®å¤‰æ›
        places = []
        for match in all_matches:
            places.append({
                'name': match['text'],
                'context': match['sentence'],
                'confidence': match['confidence'],
                'source': 'regex',
                'metadata': {
                    'category': match['category'],
                    'priority': match['priority'],
                    'method': f"regex_{match['category']}"
                },
                'position': match.get('start', 0),
                'sentence_index': match.get('sentence_index', 0),
                'category': match['category'],
                'matched_text': match['text']
            })
        
        return places

    def _extract_from_sentence(self, sentence: str, sentence_idx: int, sentences: List[str]) -> List[Dict]:
        """å˜ä¸€æ–‡ã‹ã‚‰ã®åœ°åæŠ½å‡º"""
        matches = []
        
        for pattern_info in self.patterns:
            pattern_matches = list(re.finditer(pattern_info['pattern'], sentence))
            
            for match in pattern_matches:
                place_name = match.group(0)
                
                # å‰å¾Œã®æ–‡è„ˆå–å¾—
                before_text = sentences[sentence_idx - 1] if sentence_idx > 0 else ""
                after_text = sentences[sentence_idx + 1] if sentence_idx < len(sentences) - 1 else ""
                
                matches.append({
                    'text': place_name,
                    'start': match.start(),
                    'end': match.end(),
                    'sentence': sentence,
                    'before_text': before_text,
                    'after_text': after_text,
                    'sentence_index': sentence_idx,
                    'category': pattern_info['category'],
                    'confidence': pattern_info['confidence'],
                    'priority': pattern_info['priority']
                })
        
        return matches

    def _deduplicate_overlapping_matches(self, matches: List[Dict]) -> List[Dict]:
        """é‡è¤‡ã™ã‚‹åœ°åã®æ’é™¤"""
        if not matches:
            return []
        
        # å„ªå…ˆåº¦é †ã§ã‚½ãƒ¼ãƒˆï¼ˆpriority 0ãŒæœ€é«˜å„ªå…ˆåº¦ï¼‰
        matches.sort(key=lambda x: (x['priority'], -x['confidence'], -len(x['text'])))
        
        deduplicated = []
        used_ranges = []
        
        for match in matches:
            match_range = (match['start'], match['end'])
            
            # æ—¢ã«ä½¿ç”¨ã•ã‚ŒãŸç¯„å›²ã¨é‡è¤‡ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            is_overlapping = any(
                self._ranges_overlap(match_range, used_range) 
                for used_range in used_ranges
            )
            
            if not is_overlapping:
                deduplicated.append(match)
                used_ranges.append(match_range)
        
        return deduplicated

    def _ranges_overlap(self, range1: Tuple[int, int], range2: Tuple[int, int]) -> bool:
        """2ã¤ã®ç¯„å›²ãŒé‡è¤‡ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        start1, end1 = range1
        start2, end2 = range2
        return not (end1 <= start2 or end2 <= start1)

    def _process_nlp_entities(self, nlp_result: NLPResult, text: str) -> List[Dict[str, Any]]:
        """NLPã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®å‡¦ç†"""
        places = []
        for entity in nlp_result.entities:
            if entity["label"] in ["åœ°å", "å›½ãƒ»åœ°åŸŸ", "LOC", "GPE"]:
                context_start = max(0, entity["start"] - 50)
                context_end = min(len(text), entity["end"] + 50)
                context = text[context_start:context_end]
                
                places.append({
                    'name': entity["text"],
                    'context': context,
                    'confidence': entity["confidence"] * 0.8,  # NLPçµæœã®ä¿¡é ¼åº¦èª¿æ•´
                    'source': 'nlp',
                    'metadata': {
                        'category': 'nlp_entity',
                        'original_label': entity["label"]
                    },
                    'position': entity["start"],
                    'sentence_index': 0,
                    'category': 'nlp_entity',
                    'matched_text': entity["text"]
                })
        return places

    def _merge_and_deduplicate_places(self, all_places: List[Dict[str, Any]]) -> List[ExtractedPlace]:
        """åœ°åã®çµ±åˆã¨é‡è¤‡é™¤å»"""
        merged = {}
        
        for place in all_places:
            name = place['name']
            
            if name in merged:
                # æ—¢å­˜ã®åœ°åã®å ´åˆã€ä¿¡é ¼åº¦ã®é«˜ã„æ–¹ã‚’æ¡ç”¨
                if place['confidence'] > merged[name].confidence:
                    merged[name] = ExtractedPlace(**place)
                else:
                    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ
                    if place['source'] not in merged[name].source:
                        merged[name].source += f"+{place['source']}"
            else:
                merged[name] = ExtractedPlace(**place)
        
        return list(merged.values())

    async def _add_context_analysis(
        self,
        places: List[ExtractedPlace],
        work_title: str,
        author_name: str
    ) -> List[ExtractedPlace]:
        """æ–‡è„ˆåˆ†æã®è¿½åŠ """
        result = []
        
        for place in places:
            try:
                # æ–‡è„ˆåˆ†æã®å–å¾—
                context_analysis = await self.llm_manager.analyze_place_context(
                    place.name,
                    work_title,
                    author_name
                )
                
                # ç·åˆä¿¡é ¼åº¦ã®è¨ˆç®—
                confidence = self._calculate_enhanced_confidence(place, context_analysis)
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æ›´æ–°
                enhanced_place = ExtractedPlace(
                    name=place.name,
                    context=place.context,
                    confidence=confidence,
                    source=place.source,
                    metadata={
                        **place.metadata,
                        'context_analysis': context_analysis,
                        'enhanced': True
                    },
                    position=place.position,
                    sentence_index=place.sentence_index,
                    category=place.category,
                    matched_text=place.matched_text
                )
                
                result.append(enhanced_place)
                
            except Exception as e:
                logger.warning(f"æ–‡è„ˆåˆ†æå¤±æ•—: {place.name} - {e}")
                result.append(place)  # å…ƒã®åœ°åã‚’ãã®ã¾ã¾è¿½åŠ 
        
        return result

    def _calculate_enhanced_confidence(self, place: ExtractedPlace, context_analysis: Dict) -> float:
        """å¼·åŒ–ç‰ˆä¿¡é ¼åº¦è¨ˆç®—"""
        base_confidence = place.confidence
        
        # æŠ½å‡ºãƒ¬ãƒ™ãƒ«ã«åŸºã¥ãèª¿æ•´
        level = self._determine_extraction_level(place.name)
        level_multiplier = self.extraction_levels.get(level, 0.5)
        
        # ã‚½ãƒ¼ã‚¹ã«åŸºã¥ãèª¿æ•´
        source_multiplier = 1.0
        if 'regex' in place.source:
            source_multiplier += 0.1
        if 'nlp' in place.source:
            source_multiplier += 0.05
        if 'llm' in place.source:
            source_multiplier += 0.15
        
        # æ–‡è„ˆåˆ†æã«åŸºã¥ãèª¿æ•´
        context_multiplier = 1.0
        if context_analysis:
            relevance = context_analysis.get('relevance', 0.5)
            context_multiplier = 1.0 + (relevance - 0.5) * 0.3
        
        # æœ€çµ‚ä¿¡é ¼åº¦è¨ˆç®—
        final_confidence = base_confidence * level_multiplier * source_multiplier * context_multiplier
        
        return min(1.0, max(0.0, final_confidence))

    def _determine_extraction_level(self, place_name: str) -> str:
        """æŠ½å‡ºãƒ¬ãƒ™ãƒ«ã®åˆ¤å®š"""
        # éƒ½é“åºœçœŒã®åˆ¤å®š
        if place_name.endswith(("éƒ½", "é“", "åºœ", "çœŒ")):
            return "prefecture"
            
        # å¸‚åŒºç”ºæ‘ã®åˆ¤å®š
        if place_name.endswith(("å¸‚", "åŒº", "ç”º", "æ‘")):
            return "city"
            
        # éƒ¡ã®åˆ¤å®š
        if place_name.endswith("éƒ¡"):
            return "district"
            
        # æœ‰ååœ°åã®åˆ¤å®š
        famous_places = {"æ±äº¬", "äº¬éƒ½", "å¤§é˜ª", "åå¤å±‹", "æ¨ªæµœ", "ç¥æˆ¸", "æ±Ÿæˆ¸", "æœ¬éƒ·", "ç¥ç”°"}
        if place_name in famous_places:
            return "famous_place"
            
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯LLMæ–‡è„ˆç†è§£
        return "llm_context"

    def _split_into_sentences(self, text: str) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’æ–‡ã«åˆ†å‰²"""
        sentences = re.split(r'[ã€‚ï¼ï¼ï¼Ÿ!?]+', text)
        return [s.strip() for s in sentences if s.strip() and len(s.strip()) > 5]

    def extract_places_from_text(self, text: str) -> List[ExtractedPlace]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰åœ°åã‚’æŠ½å‡ºã™ã‚‹"""
        extracted_places = []
        
        # éƒ½é“åºœçœŒæŠ½å‡º
        for pattern in self.prefecture_patterns:
            places = self._extract_by_pattern(text, pattern, 'éƒ½é“åºœçœŒ', 0.9)
            extracted_places.extend(places)
            
        # ä¸»è¦éƒ½å¸‚æŠ½å‡º
        for pattern in self.city_patterns:
            places = self._extract_by_pattern(text, pattern, 'å¸‚åŒºç”ºæ‘', 0.85)
            extracted_places.extend(places)
            
        # æ­´å²åœ°åæŠ½å‡º
        for pattern in self.historical_patterns:
            places = self._extract_by_pattern(text, pattern, 'æ­´å²åœ°å', 0.8)
            extracted_places.extend(places)
            
        # æœ‰ååœ°åæŠ½å‡º
        for pattern in self.famous_patterns:
            places = self._extract_by_pattern(text, pattern, 'æœ‰ååœ°å', 0.85)
            extracted_places.extend(places)
            
        # åŒºãƒ»éƒ¡æŠ½å‡º
        for pattern in self.district_patterns:
            places = self._extract_by_pattern(text, pattern, 'éƒ¡', 0.7)
            extracted_places.extend(places)
            
        # é‡è¤‡é™¤å»
        extracted_places = self._remove_duplicates(extracted_places)
        
        return extracted_places
        
    def _extract_by_pattern(self, text: str, pattern: str, place_type: str, confidence: float) -> List[ExtractedPlace]:
        """æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã§åœ°åã‚’æŠ½å‡ºã™ã‚‹"""
        places = []
        
        for match in re.finditer(pattern, text):
            name = match.group()
            position = match.start()
            
            # å‰å¾Œã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            context_before = text[max(0, position-20):position]
            context_after = text[position+len(name):position+len(name)+20]
            
            place = ExtractedPlace(
                name=name,
                canonical_name=self._normalize_place_name(name),
                place_type=place_type,
                confidence=confidence,
                position=position,
                matched_text=name,
                context_before=context_before,
                context_after=context_after,
                extraction_method='regex'
            )
            
            places.append(place)
            
        return places
        
    def _normalize_place_name(self, name: str) -> str:
        """åœ°åã‚’æ­£è¦åŒ–ã™ã‚‹"""
        # ã€ŒçœŒã€ã€Œåºœã€ã€Œéƒ½ã€ãªã©ã‚’é™¤å»ã—ã¦æ­£è¦åŒ–
        normalized = re.sub(r'[çœŒåºœéƒ½é“]$', '', name)
        return normalized if normalized else name
        
    def _remove_duplicates(self, places: List[ExtractedPlace]) -> List[ExtractedPlace]:
        """é‡è¤‡ã™ã‚‹åœ°åã‚’é™¤å»ã™ã‚‹"""
        seen = set()
        unique_places = []
        
        for place in places:
            key = (place.canonical_name, place.position)
            if key not in seen:
                seen.add(key)
                unique_places.append(place)
                
        return unique_places
        
    def extract_places_from_sentence(self, sentence_id: int, sentence_text: str) -> List[ExtractedPlace]:
        """ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ã‹ã‚‰åœ°åã‚’æŠ½å‡ºã—ã€çµæœã‚’è¿”ã™"""
        logger.info(f"ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ {sentence_id} ã‹ã‚‰åœ°åæŠ½å‡ºé–‹å§‹")
        
        places = self.extract_places_from_text(sentence_text)
        
        logger.info(f"ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ {sentence_id}: {len(places)}ä»¶ã®åœ°åã‚’æŠ½å‡º")
        
        return places
        
    def save_extracted_places(self, sentence_id: int, places: List[ExtractedPlace]) -> int:
        """æŠ½å‡ºã•ã‚ŒãŸåœ°åã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã™ã‚‹"""
        if not places:
            return 0
            
        saved_count = 0
        
        for place in places:
            try:
                # åœ°åãƒã‚¹ã‚¿ãƒ¼ã«è¿½åŠ ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆï¼‰
                place_id = self._get_or_create_place_master(place)
                
                # ã‚»ãƒ³ãƒ†ãƒ³ã‚¹-åœ°åé–¢é€£ã‚’ä¿å­˜
                self._save_sentence_place_relation(sentence_id, place_id, place)
                
                saved_count += 1
                
            except Exception as e:
                logger.error(f"åœ°åä¿å­˜ã‚¨ãƒ©ãƒ¼: {place.name} - {e}")
                
        return saved_count
        
    def _get_or_create_place_master(self, place: ExtractedPlace) -> int:
        """åœ°åãƒã‚¹ã‚¿ãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰åœ°åIDã‚’å–å¾—ã€å­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ"""
        conn = sqlite3.connect('data/bungo_map.db')
        cursor = conn.cursor()
        
        try:
            # æ—¢å­˜åœ°åã‚’æ¤œç´¢
            cursor.execute("""
                SELECT place_id FROM places 
                WHERE canonical_name = ? OR place_name = ?
            """, (place.canonical_name, place.name))
            
            result = cursor.fetchone()
            if result:
                return result[0]
                
            # æ–°è¦åœ°åã‚’ä½œæˆ
            cursor.execute("""
                INSERT INTO places (
                    place_name, canonical_name, place_type, confidence,
                    mention_count, source_system, verification_status,
                    created_at
                ) VALUES (?, ?, ?, ?, 1, 'place_extractor', 'auto_extracted', ?)
            """, (
                place.name, place.canonical_name, place.place_type, 
                place.confidence, datetime.now()
            ))
            
            place_id = cursor.lastrowid
            conn.commit()
            
            logger.info(f"æ–°è¦åœ°åç™»éŒ²: {place.name} (ID: {place_id})")
            return place_id
            
        finally:
            conn.close()
            
    def _save_sentence_place_relation(self, sentence_id: int, place_id: int, place: ExtractedPlace):
        """ã‚»ãƒ³ãƒ†ãƒ³ã‚¹-åœ°åé–¢é€£ã‚’ä¿å­˜"""
        conn = sqlite3.connect('data/bungo_map.db')
        cursor = conn.cursor()
        
        try:
            cursor.execute("""
                INSERT INTO sentence_places (
                    sentence_id, place_id, extraction_method, confidence,
                    position_in_sentence, context_before, context_after,
                    matched_text, verification_status, created_at
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, 'auto_extracted', ?)
            """, (
                sentence_id, place_id, place.extraction_method, place.confidence,
                place.position, place.context_before, place.context_after,
                place.matched_text, datetime.now()
            ))
            
            conn.commit()
            
        finally:
            conn.close()
            
    def process_sentences_batch(self, limit: Optional[int] = None) -> Dict[str, int]:
        """ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ã‚’ä¸€æ‹¬å‡¦ç†ã—ã¦åœ°åæŠ½å‡ºã‚’å®Ÿè¡Œ"""
        conn = sqlite3.connect('data/bungo_map.db')
        cursor = conn.cursor()
        
        try:
            # æœªå‡¦ç†ã®ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ã‚’å–å¾—
            query = """
                SELECT s.sentence_id, s.sentence_text 
                FROM sentences s
                LEFT JOIN sentence_places sp ON s.sentence_id = sp.sentence_id
                WHERE sp.sentence_id IS NULL
            """
            
            if limit:
                query += f" LIMIT {limit}"
                
            cursor.execute(query)
            sentences = cursor.fetchall()
            
            logger.info(f"å‡¦ç†å¯¾è±¡ã‚»ãƒ³ãƒ†ãƒ³ã‚¹: {len(sentences)}ä»¶")
            
            stats = {
                'processed_sentences': 0,
                'extracted_places': 0,
                'saved_places': 0,
                'errors': 0
            }
            
            for sentence_id, sentence_text in sentences:
                try:
                    # åœ°åæŠ½å‡º
                    places = self.extract_places_from_sentence(sentence_id, sentence_text)
                    
                    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜
                    saved_count = self.save_extracted_places(sentence_id, places)
                    
                    stats['processed_sentences'] += 1
                    stats['extracted_places'] += len(places)
                    stats['saved_places'] += saved_count
                    
                    if len(places) > 0:
                        logger.info(f"ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ {sentence_id}: {len(places)}ä»¶æŠ½å‡º")
                        
                except Exception as e:
                    stats['errors'] += 1
                    logger.error(f"ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ {sentence_id} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    
            return stats
            
        finally:
            conn.close()
            
    def get_extraction_statistics(self) -> Dict[str, any]:
        """åœ°åæŠ½å‡ºã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        conn = sqlite3.connect('data/bungo_map.db')
        cursor = conn.cursor()
        
        try:
            stats = {}
            
            # ç·ã‚»ãƒ³ãƒ†ãƒ³ã‚¹æ•°
            cursor.execute("SELECT COUNT(*) FROM sentences")
            stats['total_sentences'] = cursor.fetchone()[0]
            
            # å‡¦ç†æ¸ˆã¿ã‚»ãƒ³ãƒ†ãƒ³ã‚¹æ•°
            cursor.execute("SELECT COUNT(DISTINCT sentence_id) FROM sentence_places")
            stats['processed_sentences'] = cursor.fetchone()[0]
            
            # æŠ½å‡ºã•ã‚ŒãŸåœ°åæ•°
            cursor.execute("SELECT COUNT(*) FROM places")
            stats['total_places'] = cursor.fetchone()[0]
            
            # åœ°å-ã‚»ãƒ³ãƒ†ãƒ³ã‚¹é–¢é€£æ•°
            cursor.execute("SELECT COUNT(*) FROM sentence_places")
            stats['total_relations'] = cursor.fetchone()[0]
            
            # åœ°åç¨®åˆ¥çµ±è¨ˆ
            cursor.execute("""
                SELECT place_type, COUNT(*) 
                FROM places 
                GROUP BY place_type
            """)
            stats['place_types'] = dict(cursor.fetchall())
            
            # æŠ½å‡ºæ‰‹æ³•çµ±è¨ˆ
            cursor.execute("""
                SELECT extraction_method, COUNT(*) 
                FROM sentence_places 
                GROUP BY extraction_method
            """)
            stats['extraction_methods'] = dict(cursor.fetchall())
            
            return stats
            
        finally:
            conn.close()

# ä¸‹ä½äº’æ›ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹
PlaceExtractor = EnhancedPlaceExtractor
