#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¹è‰¯ç‰ˆãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 
é’ç©ºæ–‡åº«ãƒ†ã‚­ã‚¹ãƒˆã®é«˜å“è³ªã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ­£è¦åŒ–ã‚’å®Ÿè£…
"""

import re
import logging
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class ProcessingStats:
    """å‡¦ç†çµ±è¨ˆ"""
    original_length: int
    processed_length: int
    removed_ruby_count: int
    removed_html_count: int
    removed_annotation_count: int
    sentence_count: int

class TextProcessor:
    """æ”¹è‰¯ç‰ˆãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        # æ”¹è‰¯ç‰ˆãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.cleanup_patterns = [
            # rubyã‚¿ã‚°ï¼ˆèª­ã¿ä»®åï¼‰ã®é©åˆ‡ãªå‡¦ç†
            (r'<ruby><rb>([^<]+)</rb><rp>[ï¼ˆ(]</rp><rt>([^<]*)</rt><rp>[ï¼‰)]</rp></ruby>', r'\1ï¼ˆ\2ï¼‰'),
            (r'<ruby><rb>([^<]+)</rb><rp>ï¼ˆ</rp><rt>([^<]*)</rt><rp>ï¼‰</rp></ruby>', r'\1ï¼ˆ\2ï¼‰'),
            (r'<ruby>([^<]+)<rt>([^<]*)</rt></ruby>', r'\1ï¼ˆ\2ï¼‰'),
            
            # HTMLã‚¿ã‚°é™¤å»
            (r'<br\s*/?\s*>', ''),
            (r'<[^>]+>', ''),
            
            # é’ç©ºæ–‡åº«æ³¨é‡ˆè¨˜å·é™¤å»
            (r'ã€Š[^ã€‹]*ã€‹', ''),
            (r'ï¼»[^ï¼½]*ï¼½', ''),
            (r'ã€”[^ã€•]*ã€•', ''),
            (r'ï¼»ï¼ƒ[^ï¼½]*ï¼½', ''),
            
            # åº•æœ¬æƒ…å ±é™¤å»
            (r'åº•æœ¬ï¼š[^\n]*\n?', ''),
            (r'å…¥åŠ›ï¼š[^\n]*\n?', ''),
            (r'æ ¡æ­£ï¼š[^\n]*\n?', ''),
            (r'â€»[^\n]*\n?', ''),
            (r'åˆå‡ºï¼š[^\n]*\n?', ''),
            
            # XMLãƒ˜ãƒƒãƒ€ãƒ¼é™¤å»
            (r'<\?xml[^>]*\?>', ''),
            (r'<!DOCTYPE[^>]*>', ''),
            
            # å¤šé‡ç©ºç™½ãƒ»æ”¹è¡Œã®æ­£è¦åŒ–
            (r'\n\s*\n\s*\n+', '\n\n'),
            (r'[ \t]+', ' '),
            (r'ã€€+', 'ã€€'),
        ]
        
        # æ–‡åˆ†å‰²ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.sentence_pattern = re.compile(r'[ã€‚ï¼ï¼ï¼Ÿ!?]+')
        
        logger.info("ğŸ“ æ”¹è‰¯ç‰ˆãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def clean_aozora_text(self, raw_text: str) -> Tuple[str, ProcessingStats]:
        """é’ç©ºæ–‡åº«ãƒ†ã‚­ã‚¹ãƒˆã®æ”¹è‰¯ç‰ˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        if not raw_text:
            return "", ProcessingStats(0, 0, 0, 0, 0, 0)
        
        original_length = len(raw_text)
        processed_text = raw_text
        
        # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®çµ±è¨ˆ
        ruby_count = 0
        html_count = 0
        annotation_count = 0
        
        # ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é †æ¬¡é©ç”¨
        for pattern, replacement in self.cleanup_patterns:
            if 'ruby' in pattern.lower():
                matches = len(re.findall(pattern, processed_text))
                ruby_count += matches
            elif '<' in pattern and '>' in pattern:
                matches = len(re.findall(pattern, processed_text))
                html_count += matches
            elif any(char in pattern for char in ['ã€Š', 'ï¼»', 'ã€”']):
                matches = len(re.findall(pattern, processed_text))
                annotation_count += matches
            
            processed_text = re.sub(pattern, replacement, processed_text)
        
        # æœ€çµ‚çš„ãªæ­£è¦åŒ–
        processed_text = self._final_normalization(processed_text)
        
        # æ–‡åˆ†å‰²
        sentences = self.split_into_sentences(processed_text)
        
        stats = ProcessingStats(
            original_length=original_length,
            processed_length=len(processed_text),
            removed_ruby_count=ruby_count,
            removed_html_count=html_count,
            removed_annotation_count=annotation_count,
            sentence_count=len(sentences)
        )
        
        logger.info(f"ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†: {original_length:,} â†’ {len(processed_text):,}æ–‡å­—, {len(sentences)}æ–‡")
        
        return processed_text, stats
    
    def _final_normalization(self, text: str) -> str:
        """æœ€çµ‚çš„ãªæ­£è¦åŒ–å‡¦ç†"""
        # é€£ç¶šã™ã‚‹ç©ºç™½ã®æ­£è¦åŒ–
        text = re.sub(r'[ \t]+', ' ', text)
        text = re.sub(r'ã€€+', 'ã€€', text)
        
        # é€£ç¶šã™ã‚‹æ”¹è¡Œã®æ­£è¦åŒ–
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
        
        # è¡Œé ­ãƒ»è¡Œæœ«ã®ç©ºç™½é™¤å»
        lines = text.split('\n')
        normalized_lines = [line.strip() for line in lines]
        text = '\n'.join(normalized_lines)
        
        # æ–‡é ­ãƒ»æ–‡æœ«ã®ç©ºç™½é™¤å»
        text = text.strip()
        
        return text
    
    def split_into_sentences(self, text: str) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’æ–‡ã«åˆ†å‰²"""
        if not text:
            return []
        
        # æ”¹è¡Œã‚’ç©ºç™½ã«ç½®æ›
        text = text.replace('\n', ' ')
        
        # æ–‡æœ«è¨˜å·ã§åˆ†å‰²
        sentences = []
        current = ""
        
        for char in text:
            current += char
            if self.sentence_pattern.search(char):
                if current.strip() and len(current.strip()) > 5:  # æœ€å°æ–‡é•·åˆ¶é™
                    sentences.append(current.strip())
                current = ""
        
        # æœ€å¾Œã®æ–‡ã‚’è¿½åŠ 
        if current.strip() and len(current.strip()) > 5:
            sentences.append(current.strip())
        
        # çŸ­ã™ãã‚‹æ–‡ã‚„é•·ã™ãã‚‹æ–‡ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        filtered_sentences = []
        for sentence in sentences:
            if 5 <= len(sentence) <= 500:  # é©åˆ‡ãªæ–‡ã®é•·ã•
                filtered_sentences.append(sentence)
        
        return filtered_sentences
    
    def extract_metadata(self, text: str) -> Dict[str, str]:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º"""
        metadata = {}
        
        # ã‚¿ã‚¤ãƒˆãƒ«
        title_patterns = [
            r'ã€ã‚¿ã‚¤ãƒˆãƒ«ã€‘\s*(.*?)(?:\n|$)',
            r'ä½œå“åï¼š\s*(.*?)(?:\n|$)',
            r'é¡Œåï¼š\s*(.*?)(?:\n|$)'
        ]
        for pattern in title_patterns:
            match = re.search(pattern, text)
            if match:
                metadata['title'] = match.group(1).strip()
                break
        
        # ä½œè€…
        author_patterns = [
            r'ã€ä½œè€…ã€‘\s*(.*?)(?:\n|$)',
            r'è‘—è€…ï¼š\s*(.*?)(?:\n|$)',
            r'ä½œè€…ï¼š\s*(.*?)(?:\n|$)'
        ]
        for pattern in author_patterns:
            match = re.search(pattern, text)
            if match:
                metadata['author'] = match.group(1).strip()
                break
        
        # åº•æœ¬
        source_match = re.search(r'åº•æœ¬ï¼š\s*(.*?)(?:\n|$)', text)
        if source_match:
            metadata['source'] = source_match.group(1).strip()
        
        # å…¥åŠ›
        input_match = re.search(r'å…¥åŠ›ï¼š\s*(.*?)(?:\n|$)', text)
        if input_match:
            metadata['input'] = input_match.group(1).strip()
        
        # æ ¡æ­£
        proof_match = re.search(r'æ ¡æ­£ï¼š\s*(.*?)(?:\n|$)', text)
        if proof_match:
            metadata['proof'] = proof_match.group(1).strip()
        
        # åˆå‡º
        debut_match = re.search(r'åˆå‡ºï¼š\s*(.*?)(?:\n|$)', text)
        if debut_match:
            metadata['debut'] = debut_match.group(1).strip()
        
        return metadata
    
    def validate_text_quality(self, text: str) -> Dict[str, Any]:
        """ãƒ†ã‚­ã‚¹ãƒˆå“è³ªã®æ¤œè¨¼"""
        if not text:
            return {
                'is_valid': False,
                'errors': ['ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ã™'],
                'warnings': [],
                'quality_score': 0.0
            }
        
        errors = []
        warnings = []
        quality_score = 1.0
        
        # é•·ã•ãƒã‚§ãƒƒã‚¯
        if len(text) < 100:
            errors.append('ãƒ†ã‚­ã‚¹ãƒˆãŒçŸ­ã™ãã¾ã™ï¼ˆ100æ–‡å­—æœªæº€ï¼‰')
            quality_score -= 0.3
        elif len(text) < 500:
            warnings.append('ãƒ†ã‚­ã‚¹ãƒˆãŒçŸ­ã‚ã§ã™ï¼ˆ500æ–‡å­—æœªæº€ï¼‰')
            quality_score -= 0.1
        
        # HTMLã‚¿ã‚°ã®æ®‹å­˜ãƒã‚§ãƒƒã‚¯
        html_tags = re.findall(r'<[^>]+>', text)
        if html_tags:
            warnings.append(f'HTMLã‚¿ã‚°ãŒæ®‹å­˜ã—ã¦ã„ã¾ã™: {len(html_tags)}å€‹')
            quality_score -= 0.1
        
        # é’ç©ºæ–‡åº«è¨˜å·ã®æ®‹å­˜ãƒã‚§ãƒƒã‚¯
        aozora_symbols = re.findall(r'[ã€Šã€‹ï¼»ï¼½ã€”ã€•]', text)
        if aozora_symbols:
            warnings.append(f'é’ç©ºæ–‡åº«è¨˜å·ãŒæ®‹å­˜ã—ã¦ã„ã¾ã™: {len(aozora_symbols)}å€‹')
            quality_score -= 0.1
        
        # æ–‡å­—åŒ–ã‘ãƒã‚§ãƒƒã‚¯
        mojibake_patterns = [r'[ï¿½ï¿½]+', r'[\x00-\x08\x0B\x0C\x0E-\x1F]+']
        for pattern in mojibake_patterns:
            if re.search(pattern, text):
                errors.append('æ–‡å­—åŒ–ã‘ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ')
                quality_score -= 0.5
                break
        
        # æ”¹è¡Œã®ç•°å¸¸ãƒã‚§ãƒƒã‚¯
        excessive_newlines = re.search(r'\n{5,}', text)
        if excessive_newlines:
            warnings.append('ç•°å¸¸ãªæ”¹è¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ')
            quality_score -= 0.05
        
        return {
            'is_valid': len(errors) == 0,
            'errors': errors,
            'warnings': warnings,
            'quality_score': max(0.0, min(1.0, quality_score))
        }
    
    def get_text_statistics(self, text: str) -> Dict[str, Any]:
        """ãƒ†ã‚­ã‚¹ãƒˆçµ±è¨ˆã®å–å¾—"""
        if not text:
            return {}
        
        sentences = self.split_into_sentences(text)
        
        # æ–‡å­—çµ±è¨ˆ
        char_count = len(text)
        char_count_no_spaces = len(re.sub(r'\s', '', text))
        
        # æ–‡çµ±è¨ˆ
        sentence_count = len(sentences)
        avg_sentence_length = sum(len(s) for s in sentences) / sentence_count if sentence_count > 0 else 0
        
        # æ®µè½çµ±è¨ˆ
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        paragraph_count = len(paragraphs)
        
        return {
            'char_count': char_count,
            'char_count_no_spaces': char_count_no_spaces,
            'sentence_count': sentence_count,
            'paragraph_count': paragraph_count,
            'avg_sentence_length': round(avg_sentence_length, 2),
            'longest_sentence': max(len(s) for s in sentences) if sentences else 0,
            'shortest_sentence': min(len(s) for s in sentences) if sentences else 0
        } 