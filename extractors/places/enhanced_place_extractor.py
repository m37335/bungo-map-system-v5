#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜ç²¾åº¦åœ°åæŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ  v3.0
åœ°åãƒã‚¹ã‚¿ãƒ¼å„ªå…ˆè¨­è¨ˆã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªæŠ½å‡ºãƒ»ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 

æ–°æ©Ÿèƒ½:
1. æŠ½å‡ºæ™‚ç‚¹ã§ã®ãƒã‚¹ã‚¿ãƒ¼æ¤œç´¢ãƒ»å‚ç…§
2. é‡è¤‡ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œå…¨å›é¿
3. AIæ¤œè¨¼çµ±åˆã«ã‚ˆã‚‹é«˜å“è³ªä¿è¨¼
4. ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ã«ã‚ˆã‚‹é«˜é€Ÿå‡¦ç†
"""

import sys
import os
import sqlite3
import spacy
import re
import time
from typing import Dict, List, Optional, Tuple
from datetime import datetime

# ãƒ‘ã‚¹è¨­å®š
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from .place_master_manager import PlaceMasterManagerV2


class EnhancedPlaceExtractorV3:
    """é«˜ç²¾åº¦åœ°åæŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ  v3.0"""
    
    def __init__(self):
        # GinZAåˆæœŸåŒ–
        try:
            self.nlp = spacy.load('ja_ginza')
        except Exception as e:
            print(f"âš ï¸ GinZAãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            self.nlp = None
        
        # åœ°åãƒã‚¹ã‚¿ãƒ¼ãƒãƒãƒ¼ã‚¸ãƒ£
        self.place_manager = PlaceMasterManagerV2()
        
        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            'sentences_processed': 0,
            'places_extracted': 0,
            'masters_reused': 0,
            'masters_created': 0,
            'geocoding_skipped': 0,
            'processing_time': 0,
            'ai_validations': 0
        }
        
        # åœ°åãƒ‘ã‚¿ãƒ¼ãƒ³
        self.place_patterns = [
            r'[æ±è¥¿å—åŒ—]?[éƒ½é“åºœçœŒå¸‚åŒºç”ºæ‘éƒ¡]',
            r'.*[å±±å·å³¶æ¹–æ¸¯é§…]$',
            r'.*[ç¥ç¤¾å¯ºé™¢]$',
            r'.*[å¤§å­¦é«˜æ ¡]$',
            r'[0-9]*[ä¸ç›®ç•ªåœ°å·]',
        ]
        
        print("ğŸš€ é«˜ç²¾åº¦åœ°åæŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ  v3.0 åˆæœŸåŒ–å®Œäº†")
        print("âœ… åœ°åãƒã‚¹ã‚¿ãƒ¼å„ªå…ˆè¨­è¨ˆã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªå‡¦ç†ãŒå¯èƒ½ã§ã™")
    
    def extract_places_from_sentence(self, sentence_id: int, sentence_text: str, 
                                   work_title: str = None) -> List[Dict]:
        """ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ã‹ã‚‰åœ°åæŠ½å‡ºï¼ˆãƒã‚¹ã‚¿ãƒ¼å„ªå…ˆå‡¦ç†ï¼‰"""
        start_time = time.time()
        extracted_places = []
        
        try:
            if not self.nlp:
                print("âš ï¸ GinZAãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
                return []
            
            self.stats['sentences_processed'] += 1
            
            # GinZAã«ã‚ˆã‚‹è‡ªç„¶è¨€èªå‡¦ç†
            doc = self.nlp(sentence_text)
            
            # åœ°åå€™è£œæŠ½å‡º
            place_candidates = []
            
            # 1. å›ºæœ‰è¡¨ç¾æŠ½å‡º
            for ent in doc.ents:
                if ent.label_ in ['GPE', 'LOC', 'FAC']:  # åœ°æ”¿å­¦çš„å®Ÿä½“ã€å ´æ‰€ã€æ–½è¨­
                    place_candidates.append({
                        'text': ent.text,
                        'start': ent.start_char,
                        'end': ent.end_char,
                        'label': ent.label_,
                        'confidence': 0.8
                    })
            
            # 2. å“è©ãƒ™ãƒ¼ã‚¹æŠ½å‡º
            for token in doc:
                if token.pos_ == 'PROPN':  # å›ºæœ‰åè©
                    # åœ°åãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
                    for pattern in self.place_patterns:
                        if re.search(pattern, token.text):
                            place_candidates.append({
                                'text': token.text,
                                'start': token.idx,
                                'end': token.idx + len(token.text),
                                'label': 'PATTERN',
                                'confidence': 0.6
                            })
                            break
            
            # 3. è¤‡åˆåœ°åæŠ½å‡º
            place_candidates.extend(self.extract_compound_places(doc))
            
            # é‡è¤‡é™¤å»
            unique_candidates = self.deduplicate_candidates(place_candidates)
            
            # å„å€™è£œã‚’ãƒã‚¹ã‚¿ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã§å‡¦ç†
            for candidate in unique_candidates:
                place_text = candidate['text']
                
                print(f"ğŸ” åœ°åå€™è£œå‡¦ç†: {place_text}")
                
                # ãƒã‚¹ã‚¿ãƒ¼æ¤œç´¢ãƒ»ç™»éŒ²
                master_id = self.place_manager.extract_and_register_place(
                    place_text=place_text,
                    sentence_id=sentence_id,
                    sentence_text=sentence_text,
                    extraction_method='ginza_v3'
                )
                
                if master_id:
                    place_info = {
                        'master_id': master_id,
                        'place_text': place_text,
                        'start_position': candidate['start'],
                        'end_position': candidate['end'],
                        'extraction_method': 'ginza_v3',
                        'confidence': candidate['confidence'],
                        'label': candidate['label']
                    }
                    
                    extracted_places.append(place_info)
                    self.stats['places_extracted'] += 1
                    
                    print(f"âœ… åœ°åç™»éŒ²å®Œäº†: {place_text} (master_id: {master_id})")
                else:
                    print(f"âš ï¸ åœ°åç™»éŒ²å¤±æ•—: {place_text}")
            
            processing_time = time.time() - start_time
            self.stats['processing_time'] += processing_time
            
            print(f"ğŸ“Š ã‚»ãƒ³ãƒ†ãƒ³ã‚¹å‡¦ç†å®Œäº†: {len(extracted_places)}ä»¶ã®åœ°åæŠ½å‡º ({processing_time:.3f}ç§’)")
            
            return extracted_places
            
        except Exception as e:
            print(f"âŒ åœ°åæŠ½å‡ºã‚¨ãƒ©ãƒ¼ (sentence_id: {sentence_id}): {e}")
            return []
    
    def extract_compound_places(self, doc) -> List[Dict]:
        """è¤‡åˆåœ°åæŠ½å‡ºï¼ˆã€Œæ±äº¬éƒ½åƒä»£ç”°åŒºã€ç­‰ï¼‰"""
        compounds = []
        
        try:
            # é€£ç¶šã™ã‚‹åœ°åè¦ç´ ã‚’æ¤œå‡º
            for i, token in enumerate(doc):
                if i < len(doc) - 1:
                    current = token.text
                    next_token = doc[i + 1].text
                    
                    # éƒ½é“åºœçœŒ + å¸‚åŒºç”ºæ‘ãƒ‘ã‚¿ãƒ¼ãƒ³
                    if (re.search(r'[éƒ½é“åºœçœŒ]$', current) and 
                        re.search(r'[å¸‚åŒºç”ºæ‘]', next_token)):
                        
                        compound_text = current + next_token
                        compounds.append({
                            'text': compound_text,
                            'start': token.idx,
                            'end': doc[i + 1].idx + len(next_token),
                            'label': 'COMPOUND',
                            'confidence': 0.9
                        })
            
            return compounds
            
        except Exception as e:
            print(f"âš ï¸ è¤‡åˆåœ°åæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def deduplicate_candidates(self, candidates: List[Dict]) -> List[Dict]:
        """åœ°åå€™è£œã®é‡è¤‡é™¤å»"""
        unique_candidates = []
        seen_texts = set()
        
        # ä¿¡é ¼åº¦ã®é«˜ã„é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_candidates = sorted(candidates, key=lambda x: x['confidence'], reverse=True)
        
        for candidate in sorted_candidates:
            text = candidate['text'].strip()
            
            if text and text not in seen_texts and len(text) > 1:
                # åŸºæœ¬çš„ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if not self.is_invalid_place_candidate(text):
                    unique_candidates.append(candidate)
                    seen_texts.add(text)
        
        return unique_candidates
    
    def is_invalid_place_candidate(self, text: str) -> bool:
        """ç„¡åŠ¹ãªåœ°åå€™è£œã®åˆ¤å®š"""
        invalid_patterns = [
            r'^[0-9]+$',  # æ•°å­—ã®ã¿
            r'^[ã-ã‚“]+$',  # ã²ã‚‰ãŒãªã®ã¿
            r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+$',  # æ¼¢æ•°å­—ã®ã¿
            r'^[ã€ã€‚ï¼ï¼Ÿ]+$',  # è¨˜å·ã®ã¿
        ]
        
        for pattern in invalid_patterns:
            if re.match(pattern, text):
                return True
        
        # ä¸€èˆ¬çš„ãªéåœ°åèª
        non_place_words = {
            'ã“ã¨', 'ã‚‚ã®', 'ã¨ã', 'ã¨ã“ã‚', 'ã‚ã®', 'ãã®', 'ã“ã®',
            'ç§', 'åƒ•', 'å›', 'å½¼', 'å½¼å¥³', 'å…ˆç”Ÿ', 'æ§˜', 'ã•ã‚“',
            'ä»Šæ—¥', 'æ˜æ—¥', 'æ˜¨æ—¥', 'ä»Š', 'å¾Œ', 'å‰', 'æ™‚'
        }
        
        return text in non_place_words
    
    def process_work_sentences(self, work_id: int, work_title: str = None) -> Dict:
        """ä½œå“ã®å…¨ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ã‹ã‚‰åœ°åæŠ½å‡º"""
        print(f"ğŸ“š ä½œå“åœ°åæŠ½å‡ºé–‹å§‹: {work_title or work_id}")
        start_time = time.time()
        
        try:
            # ã‚»ãƒ³ãƒ†ãƒ³ã‚¹å–å¾—
            db_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), 'data', 'bungo_map.db')
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT sentence_id, sentence_text 
                FROM sentences 
                WHERE work_id = ?
                ORDER BY sentence_order
            """, (work_id,))
            
            sentences = cursor.fetchall()
            conn.close()
            
            print(f"ğŸ“Š å‡¦ç†å¯¾è±¡ã‚»ãƒ³ãƒ†ãƒ³ã‚¹: {len(sentences)}ä»¶")
            
            work_stats = {
                'work_id': work_id,
                'work_title': work_title,
                'total_sentences': len(sentences),
                'processed_sentences': 0,
                'total_places': 0,
                'unique_places': 0,
                'processing_time': 0
            }
            
            all_extracted_places = []
            unique_masters = set()
            
            for sentence_id, sentence_text in sentences:
                places = self.extract_places_from_sentence(
                    sentence_id=sentence_id,
                    sentence_text=sentence_text,
                    work_title=work_title
                )
                
                all_extracted_places.extend(places)
                work_stats['processed_sentences'] += 1
                work_stats['total_places'] += len(places)
                
                # ãƒ¦ãƒ‹ãƒ¼ã‚¯åœ°åã‚«ã‚¦ãƒ³ãƒˆ
                for place in places:
                    unique_masters.add(place['master_id'])
                
                # é€²æ—è¡¨ç¤º
                if work_stats['processed_sentences'] % 100 == 0:
                    print(f"â³ é€²æ—: {work_stats['processed_sentences']}/{len(sentences)} ã‚»ãƒ³ãƒ†ãƒ³ã‚¹å‡¦ç†å®Œäº†")
            
            work_stats['unique_places'] = len(unique_masters)
            work_stats['processing_time'] = time.time() - start_time
            
            print(f"âœ… ä½œå“åœ°åæŠ½å‡ºå®Œäº†: {work_title or work_id}")
            print(f"ğŸ“Š çµæœ: {work_stats['total_places']}ä»¶ã®åœ°åã€{work_stats['unique_places']}ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯åœ°å")
            print(f"â±ï¸ å‡¦ç†æ™‚é–“: {work_stats['processing_time']:.2f}ç§’")
            
            return work_stats
            
        except Exception as e:
            print(f"âŒ ä½œå“åœ°åæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def get_extraction_statistics(self) -> Dict:
        """æŠ½å‡ºçµ±è¨ˆæƒ…å ±å–å¾—"""
        stats = self.stats.copy()
        stats.update(self.place_manager.get_master_statistics())
        
        return stats
    
    def print_statistics(self):
        """çµ±è¨ˆæƒ…å ±è¡¨ç¤º"""
        print("\n" + "="*60)
        print("ğŸ“Š åœ°åæŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ  v3.0 çµ±è¨ˆ")
        print("="*60)
        
        print(f"å‡¦ç†ã‚»ãƒ³ãƒ†ãƒ³ã‚¹æ•°: {self.stats['sentences_processed']:,}")
        print(f"æŠ½å‡ºåœ°åæ•°: {self.stats['places_extracted']:,}")
        print(f"ç·å‡¦ç†æ™‚é–“: {self.stats['processing_time']:.2f}ç§’")
        
        if self.stats['sentences_processed'] > 0:
            avg_time = self.stats['processing_time'] / self.stats['sentences_processed']
            print(f"å¹³å‡å‡¦ç†æ™‚é–“: {avg_time:.3f}ç§’/ã‚»ãƒ³ãƒ†ãƒ³ã‚¹")
        
        # ãƒã‚¹ã‚¿ãƒ¼çµ±è¨ˆã‚‚è¡¨ç¤º
        self.place_manager.print_statistics()


def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    extractor = EnhancedPlaceExtractorV3()
    
    # ãƒ†ã‚¹ãƒˆã‚»ãƒ³ãƒ†ãƒ³ã‚¹
    test_sentence = "ç§ã¯æ±äº¬ã‹ã‚‰äº¬éƒ½ã¸è¡Œãã€é‡‘é–£å¯ºã‚’è¦‹å­¦ã—ãŸã€‚"
    
    print("ğŸ§ª åœ°åæŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ  v3.0 ãƒ†ã‚¹ãƒˆ")
    print(f"ãƒ†ã‚¹ãƒˆã‚»ãƒ³ãƒ†ãƒ³ã‚¹: {test_sentence}")
    
    places = extractor.extract_places_from_sentence(
        sentence_id=999999,  # ãƒ†ã‚¹ãƒˆç”¨ID
        sentence_text=test_sentence
    )
    
    print(f"\nğŸ“ æŠ½å‡ºçµæœ: {len(places)}ä»¶")
    for place in places:
        print(f"  - {place['place_text']} (master_id: {place['master_id']})")
    
    extractor.print_statistics()


if __name__ == "__main__":
    main() 