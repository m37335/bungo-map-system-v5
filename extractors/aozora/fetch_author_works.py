#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡è±ªãƒãƒƒãƒ—ã‚·ã‚¹ãƒ†ãƒ  V4 - ä½œè€…ä½œå“åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ ã‚¹ãƒ†ãƒƒãƒ—â‘ â‘¡â‘¢: ä½œè€…URLå–å¾— â†’ ä½œå“ãƒªã‚¹ãƒˆå–å¾— â†’ ä½œå“DBä¿å­˜

Legacy codes from bungo-map-v4ã‚’æ´»ç”¨ã—ã¦v4ã‚·ã‚¹ãƒ†ãƒ ã«çµ±åˆ
"""

import sys
import os
import logging
import requests
import chardet
import unicodedata
import re
from typing import List, Dict, Optional, Tuple
from bs4 import BeautifulSoup
from pathlib import Path
from datetime import datetime
import time

# v4ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent))

from database.manager import DatabaseManager
from database.models import Author, Work

# ãƒ­ã‚¬ãƒ¼è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class AuthorWorksCollector:
    """ä½œè€…ä½œå“åé›†ã‚·ã‚¹ãƒ†ãƒ ï¼ˆLegacyé’ç©ºæ–‡åº«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼æ”¹è‰¯ç‰ˆï¼‰"""
    
    def __init__(self, db_path: str = "data/bungo_map.db"):
        self.db_path = db_path
        self.db_manager = DatabaseManager(db_path)
        self.session = requests.Session()
        self.base_url = "https://www.aozora.gr.jp"
        self.author_list_url = "https://www.aozora.gr.jp/index_pages/person_all.html"
        
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™è¨­å®š
        self.request_delay = 1.0  # 1ç§’é–“éš”
        
        logger.info("ğŸ“š ä½œè€…ä½œå“åé›†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def collect_author_works(self, author_name: str) -> Tuple[bool, Dict]:
        """
        æŒ‡å®šã•ã‚ŒãŸä½œè€…ã®ä½œå“ã‚’åé›†ã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
        
        Args:
            author_name: ä½œè€…å
            
        Returns:
            Tuple[bool, Dict]: (æˆåŠŸ/å¤±æ•—, çµæœæƒ…å ±)
        """
        logger.info(f"ğŸ¯ ä½œè€…ä½œå“åé›†é–‹å§‹: {author_name}")
        start_time = time.time()
        
        try:
            # ã‚¹ãƒ†ãƒƒãƒ—â‘ : ä½œè€…URLã®å–å¾—
            author_url = self._get_author_url(author_name)
            if not author_url:
                return False, {"error": "ä½œè€…ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}
            
            logger.info(f"âœ… ä½œè€…URLç™ºè¦‹: {author_url}")
            
            # ã‚¹ãƒ†ãƒƒãƒ—â‘¡: ä½œå“ãƒªã‚¹ãƒˆã¨ä½œå“URLã®å–å¾—
            works_list = self._get_works_list(author_url)
            if not works_list:
                return False, {"error": "ä½œå“ãƒªã‚¹ãƒˆãŒå–å¾—ã§ãã¾ã›ã‚“"}
            
            logger.info(f"âœ… ä½œå“ãƒªã‚¹ãƒˆå–å¾—: {len(works_list)}ä»¶")
            
            # ã‚¹ãƒ†ãƒƒãƒ—â‘¢: ä½œå“ãƒªã‚¹ãƒˆã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜
            saved_count = self._save_works_to_database(author_name, author_url, works_list)
            
            elapsed_time = time.time() - start_time
            
            result = {
                "author_name": author_name,
                "author_url": author_url,
                "total_works": len(works_list),
                "saved_works": saved_count,
                "elapsed_time": elapsed_time
            }
            
            logger.info(f"ğŸ‰ ä½œè€…ä½œå“åé›†å®Œäº†: {saved_count}/{len(works_list)}ä»¶ä¿å­˜ ({elapsed_time:.2f}ç§’)")
            return True, result
            
        except Exception as e:
            logger.error(f"âŒ ä½œè€…ä½œå“åé›†ã‚¨ãƒ©ãƒ¼: {e}")
            return False, {"error": str(e)}
    
    def _get_author_url(self, author_name: str) -> Optional[str]:
        """
        ã‚¹ãƒ†ãƒƒãƒ—â‘ : ä½œè€…åã‹ã‚‰ä½œè€…ãƒšãƒ¼ã‚¸ã®URLã‚’å–å¾—
        Legacy aozora_scraper.pyã® get_author_url ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æ”¹è‰¯
        """
        def normalize_name(name: str) -> str:
            """ä½œè€…åã‚’æ­£è¦åŒ–"""
            return unicodedata.normalize('NFKC', name).replace('\u3000', ' ').strip()
        
        def is_valid_person_href(href: str) -> bool:
            """hrefãŒæœ‰åŠ¹ãªpersonXX.htmlå½¢å¼ã‹ãƒã‚§ãƒƒã‚¯"""
            if not href:
                return False
            # #ä»¥é™ã‚’é™¤å»
            href = href.split('#')[0]
            # personXX.htmlã®å½¢å¼ã‹ãƒã‚§ãƒƒã‚¯
            return bool(re.match(r'^person\d+\.html$', href))
        
        try:
            logger.info(f"ğŸ” ä½œè€…URLæ¤œç´¢: {author_name}")
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            time.sleep(self.request_delay)
            
            # ä½œè€…ãƒªã‚¹ãƒˆãƒšãƒ¼ã‚¸å–å¾—
            response = self.session.get(self.author_list_url)
            response.raise_for_status()
            
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡º
            detected = chardet.detect(response.content)
            encoding = detected["encoding"] or "shift_jis"
            
            # HTMLãƒ‘ãƒ¼ã‚¹
            html = response.content.decode(encoding, errors="replace")
            soup = BeautifulSoup(html, 'html.parser', from_encoding=encoding)
            
            # ä½œè€…åã®æ­£è¦åŒ–
            target_name = normalize_name(author_name)
            
            # ãƒªãƒ³ã‚¯æ¤œç´¢
            for link in soup.find_all('a'):
                link_text = link.text
                normalized_text = normalize_name(link_text)
                href = link.get('href')
                
                # ä½œè€…åãŒä¸€è‡´ã—ã€ã‹ã¤æœ‰åŠ¹ãªpersonXX.htmlã®å ´åˆ
                if normalized_text == target_name and is_valid_person_href(href):
                    # #ä»¥é™ã‚’é™¤å»
                    href_clean = href.split('#')[0]
                    
                    # index_pages/ã®é‡è¤‡ã‚’é˜²ã
                    if href_clean.startswith("index_pages/"):
                        author_url = f"https://www.aozora.gr.jp/{href_clean}"
                    else:
                        author_url = f"https://www.aozora.gr.jp/index_pages/{href_clean}"
                    
                    return author_url
            
            return None
            
        except Exception as e:
            logger.error(f"ä½œè€…URLå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _get_works_list(self, author_url: str) -> List[Dict[str, str]]:
        """
        ã‚¹ãƒ†ãƒƒãƒ—â‘¡: ä½œè€…ãƒšãƒ¼ã‚¸ã‹ã‚‰ä½œå“ãƒªã‚¹ãƒˆã‚’å–å¾—
        Legacy aozora_scraper.pyã® get_works_list ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æ”¹è‰¯
        """
        logger.info(f"ğŸ“– ä½œå“ãƒªã‚¹ãƒˆå–å¾—: {author_url}")
        works = []
        
        try:
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            time.sleep(self.request_delay)
            
            response = self.session.get(author_url)
            response.raise_for_status()
            
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡º
            detected = chardet.detect(response.content)
            encoding = detected["encoding"] or "shift_jis"
            
            html = response.content.decode(encoding, errors="replace")
            soup = BeautifulSoup(html, 'html.parser', from_encoding=encoding)
            
            # ã€Œå…¬é–‹ä¸­ã®ä½œå“ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¢ã™
            for h2 in soup.find_all('h2'):
                if h2.text.strip() == 'å…¬é–‹ä¸­ã®ä½œå“':
                    ol = h2.find_next('ol')
                    if ol:
                        for li in ol.find_all('li'):
                            a_tag = li.find('a')
                            if a_tag:
                                title = a_tag.text.strip()
                                href = a_tag.get('href', '')
                                
                                if href.startswith('/cards/'):
                                    work_url = 'https://www.aozora.gr.jp' + href
                                    
                                    # ã‚¸ãƒ£ãƒ³ãƒ«åˆ¤å®šï¼ˆç°¡æ˜“ç‰ˆï¼‰
                                    genre = self._detect_genre(title)
                                    
                                    works.append({
                                        'title': title,
                                        'url': work_url,
                                        'genre': genre
                                    })
            
            logger.info(f"âœ… ä½œå“ãƒªã‚¹ãƒˆè§£æå®Œäº†: {len(works)}ä»¶")
            return works
            
        except Exception as e:
            logger.error(f"ä½œå“ãƒªã‚¹ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _detect_genre(self, title: str) -> Optional[str]:
        """ä½œå“ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ã‚¸ãƒ£ãƒ³ãƒ«ã‚’æ¨å®š"""
        genre_keywords = {
            'å°èª¬': ['ç‰©èª', 'è¨˜', 'æ—¥è¨˜', 'è¨˜éŒ²'],
            'éšç­†': ['éšç­†', 'é›‘æ„Ÿ', 'æ„Ÿæƒ³', 'æ€ã„å‡º'],
            'è©©': ['è©©', 'è©©é›†', 'æ­Œ'],
            'æˆ¯æ›²': ['æˆ¯æ›²', 'åŠ‡', 'å°æœ¬'],
            'è©•è«–': ['è©•è«–', 'è«–', 'æ‰¹è©•', 'è§£èª¬']
        }
        
        for genre, keywords in genre_keywords.items():
            if any(keyword in title for keyword in keywords):
                return genre
        
        return 'å°èª¬'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    def _get_author_with_flexible_name(self, author_name: str) -> Optional:
        """
        æŸ”è»Ÿãªä½œè€…åæ¤œç´¢ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®è¡¨è¨˜ã«åˆã‚ã›ã¦ã‚¹ãƒšãƒ¼ã‚¹ã‚’è¿½åŠ ï¼‰
        """
        # æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ§‹ç¯‰
        search_patterns = [author_name]  # å…¥åŠ›ãã®ã¾ã¾
        
        # ã‚¹ãƒšãƒ¼ã‚¹ãŒãªã„å ´åˆã€ã‚¹ãƒšãƒ¼ã‚¹ã‚’è¿½åŠ ã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œ
        if " " not in author_name and len(author_name) >= 3:
            # ä¸€èˆ¬çš„ãªåˆ†å‰²ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå§“2æ–‡å­—+åã€å§“1æ–‡å­—+åï¼‰
            if len(author_name) >= 4:
                search_patterns.append(f"{author_name[:2]} {author_name[2:]}")  # å¤ç›®æ¼±çŸ³ â†’ å¤ç›® æ¼±çŸ³
            if len(author_name) >= 3:
                search_patterns.append(f"{author_name[:1]} {author_name[1:]}")  # å¤ç›®æ¼±çŸ³ â†’ å¤ ç›®æ¼±çŸ³
        
        # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ¤œç´¢
        for pattern in search_patterns:
            logger.debug(f"ğŸ” æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³: '{pattern}'")
            author = self.db_manager.get_author_by_name(pattern)
            if author:
                logger.info(f"âœ… ä½œè€…ç™ºè¦‹: '{pattern}' â†’ {author.author_name}")
                return author
        
        # éƒ¨åˆ†ä¸€è‡´æ¤œç´¢ï¼ˆLIKEæ¼”ç®—å­ï¼‰ã§ã‚ˆã‚ŠæŸ”è»Ÿã«æ¤œç´¢
        try:
            import sqlite3
            with sqlite3.connect(self.db_manager.db_path) as conn:
                conn.row_factory = sqlite3.Row
                
                # è‹—å­—éƒ¨åˆ†ã§ã®éƒ¨åˆ†ä¸€è‡´æ¤œç´¢
                if " " in author_name:
                    lastname = author_name.split()[0]
                else:
                    lastname = author_name[:2] if len(author_name) >= 2 else author_name
                
                cursor = conn.execute(
                    "SELECT * FROM authors WHERE author_name LIKE ? LIMIT 5",
                    (f"%{lastname}%",)
                )
                results = cursor.fetchall()
                
                if results:
                    # æœ€åˆã®å€™è£œã‚’è¿”ã™
                    result = results[0]
                    from database.models import Author
                    author = Author()
                    for key in result.keys():
                        setattr(author, key, result[key])
                    logger.info(f"ğŸ” éƒ¨åˆ†ä¸€è‡´ã§ç™ºè¦‹: '{author_name}' â†’ {author.author_name}")
                    
                    # è¤‡æ•°ã®å€™è£œãŒã‚ã‚‹å ´åˆã¯è­¦å‘Š
                    if len(results) > 1:
                        candidates = [r['author_name'] for r in results]
                        logger.warning(f"âš ï¸ è¤‡æ•°ã®å€™è£œãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ: {candidates}")
                    
                    return author
                    
        except Exception as e:
            logger.warning(f"éƒ¨åˆ†ä¸€è‡´æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        
        return None
    
    def _save_works_to_database(self, author_name: str, author_url: str, works_list: List[Dict]) -> int:
        """
        ã‚¹ãƒ†ãƒƒãƒ—â‘¢: ä½œå“ãƒªã‚¹ãƒˆã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
        """
        logger.info(f"ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜é–‹å§‹: {len(works_list)}ä»¶")
        
        try:
            # ä½œè€…æƒ…å ±ã®å–å¾—
            author = self._get_author_with_flexible_name(author_name)
            if not author:
                logger.error(f"ä½œè€…æƒ…å ±ãŒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {author_name}")
                return 0
            
            saved_count = 0
            
            for work_data in works_list:
                try:
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    existing_work = self.db_manager.get_work_by_title_and_author(
                        work_data['title'], author.author_id
                    )
                    
                    if existing_work:
                        logger.debug(f"ä½œå“ã¯æ—¢ã«å­˜åœ¨: {work_data['title']}")
                        continue
                    
                    # æ–°è¦ä½œå“ä¿å­˜
                    work_info = {
                        'work_title': work_data['title'],
                        'author_id': author.author_id,
                        'genre': work_data.get('genre'),
                        'aozora_url': work_data['url'],
                        'source_system': 'v4.0',
                        'processing_status': 'pending',
                        'created_at': datetime.utcnow(),
                        'updated_at': datetime.utcnow()
                    }
                    
                    work_id = self.db_manager.save_work(work_info)
                    if work_id:
                        saved_count += 1
                        logger.debug(f"ä½œå“ä¿å­˜æˆåŠŸ: {work_data['title']} (ID: {work_id})")
                    
                except Exception as e:
                    logger.warning(f"ä½œå“ä¿å­˜ã‚¨ãƒ©ãƒ¼: {work_data['title']} - {e}")
                    continue
            
            logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜å®Œäº†: {saved_count}ä»¶")
            return saved_count
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return 0
    
    def collect_all_authors_works(self, limit: Optional[int] = None) -> Dict:
        """
        ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…ã®ã™ã¹ã¦ã®ä½œè€…ã®ä½œå“ã‚’åé›†
        
        Args:
            limit: å‡¦ç†ã™ã‚‹ä½œè€…æ•°ã®ä¸Šé™ï¼ˆNone = å…¨ä½œè€…ï¼‰
            
        Returns:
            Dict: åé›†çµæœã®çµ±è¨ˆæƒ…å ±
        """
        logger.info("ğŸŒŸ å…¨ä½œè€…ä½œå“åé›†é–‹å§‹")
        start_time = time.time()
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ä½œè€…ãƒªã‚¹ãƒˆã‚’å–å¾—
        authors = self.db_manager.get_all_authors()
        if limit:
            authors = authors[:limit]
        
        total_authors = len(authors)
        successful_authors = 0
        total_works_collected = 0
        
        results = []
        
        for i, author in enumerate(authors, 1):
            logger.info(f"ğŸ“š å‡¦ç†ä¸­ [{i}/{total_authors}]: {author.author_name}")
            
            success, result = self.collect_author_works(author.author_name)
            
            if success:
                successful_authors += 1
                total_works_collected += result.get('saved_works', 0)
            
            results.append({
                'author_name': author.author_name,
                'success': success,
                **result
            })
            
            # é€²æ—è¡¨ç¤º
            if i % 10 == 0:
                logger.info(f"ğŸ“Š é€²æ—: {i}/{total_authors} å®Œäº† ({successful_authors}äººæˆåŠŸ)")
        
        elapsed_time = time.time() - start_time
        
        summary = {
            'total_authors': total_authors,
            'successful_authors': successful_authors,
            'total_works_collected': total_works_collected,
            'elapsed_time': elapsed_time,
            'results': results
        }
        
        logger.info(f"ğŸ‰ å…¨ä½œè€…ä½œå“åé›†å®Œäº†: {successful_authors}/{total_authors}äºº "
                   f"({total_works_collected}ä½œå“, {elapsed_time:.2f}ç§’)")
        
        return summary


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ä½œè€…ä½œå“åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    parser.add_argument("--author", "-a", help="åé›†ã™ã‚‹ä½œè€…å")
    parser.add_argument("--all", action="store_true", help="å…¨ä½œè€…ã®ä½œå“ã‚’åé›†")
    parser.add_argument("--limit", "-l", type=int, help="å‡¦ç†ã™ã‚‹ä½œè€…æ•°ã®ä¸Šé™")
    parser.add_argument("--db", default="data/bungo_map.db", help="ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹")
    
    args = parser.parse_args()
    
    collector = AuthorWorksCollector(args.db)
    
    if args.author:
        # ç‰¹å®šä½œè€…ã®ä½œå“åé›†
        success, result = collector.collect_author_works(args.author)
        if success:
            print(f"âœ… æˆåŠŸ: {result}")
        else:
            print(f"âŒ å¤±æ•—: {result}")
    
    elif args.all:
        # å…¨ä½œè€…ã®ä½œå“åé›†
        summary = collector.collect_all_authors_works(args.limit)
        print(f"ğŸ“Š åé›†çµæœ: {summary}")
    
    else:
        print("--author ã¾ãŸã¯ --all ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
        parser.print_help()


if __name__ == "__main__":
    main() 