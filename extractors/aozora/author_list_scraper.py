#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é’ç©ºæ–‡åº«ä½œå®¶ãƒªã‚¹ãƒˆå°‚ç”¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
ä½œå®¶ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰è©³ç´°ãªä½œè€…æƒ…å ±ã‚’å–å¾—ã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ ¼ç´
"""

import requests
import re
import logging
import time
import sqlite3
from typing import List, Dict, Optional, Tuple
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from dataclasses import dataclass
import json
from datetime import datetime
import os
import argparse

logger = logging.getLogger(__name__)

@dataclass
class AuthorInfo:
    """ä½œè€…æƒ…å ±ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    name: str
    name_reading: Optional[str]
    works_count: int
    copyright_status: str  # 'expired', 'active', 'unknown'
    author_url: Optional[str]
    alias_info: Optional[str]  # åˆ¥åæƒ…å ±
    section: str  # ã‚ã€ã‹ã€ã•ç­‰ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³

class AuthorListScraper:
    """é’ç©ºæ–‡åº«ä½œå®¶ãƒªã‚¹ãƒˆå°‚ç”¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self, rate_limit: float = 1.0, db_path: str = "data/bungo_map.db"):
        """åˆæœŸåŒ–"""
        self.base_url = "https://www.aozora.gr.jp"
        self.author_list_url = "https://www.aozora.gr.jp/index_pages/person_all.html"
        self.rate_limit = rate_limit
        self.last_request_time = 0
        self.db_path = db_path
        
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'BungoMapBot/4.0 (Educational Research Purpose)'
        })
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥URLï¼ˆ50éŸ³é †ï¼‰
        self.section_urls = {
            'ã‚': f"{self.base_url}/index_pages/person_all_a.html",
            'ã‹': f"{self.base_url}/index_pages/person_all_ka.html",
            'ã•': f"{self.base_url}/index_pages/person_all_sa.html",
            'ãŸ': f"{self.base_url}/index_pages/person_all_ta.html",
            'ãª': f"{self.base_url}/index_pages/person_all_na.html",
            'ã¯': f"{self.base_url}/index_pages/person_all_ha.html",
            'ã¾': f"{self.base_url}/index_pages/person_all_ma.html",
            'ã‚„': f"{self.base_url}/index_pages/person_all_ya.html",
            'ã‚‰': f"{self.base_url}/index_pages/person_all_ra.html",
            'ã‚': f"{self.base_url}/index_pages/person_all_wa.html",
        }
        
        logger.info("ğŸ“š é’ç©ºæ–‡åº«ä½œå®¶ãƒªã‚¹ãƒˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼åˆæœŸåŒ–å®Œäº†")
    
    def _wait_for_rate_limit(self):
        """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’è€ƒæ…®ã—ã¦å¾…æ©Ÿ"""
        current_time = time.time()
        elapsed = current_time - self.last_request_time
        
        if elapsed < self.rate_limit:
            time.sleep(self.rate_limit - elapsed)
        
        self.last_request_time = time.time()
    
    def fetch_all_authors(self, update_database: bool = True) -> List[AuthorInfo]:
        """å…¨ä½œå®¶æƒ…å ±ã‚’å–å¾—"""
        print("ğŸ“š é’ç©ºæ–‡åº«å…¨ä½œå®¶æƒ…å ±å–å¾—é–‹å§‹")
        print("=" * 60)
        
        all_authors = []
        
        # ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‹ã‚‰å–å¾—
        main_authors = self._fetch_authors_from_url(self.author_list_url, "å…¨ä½“")
        all_authors.extend(main_authors)
        
        print(f"âœ… å…¨ä½œå®¶å–å¾—å®Œäº†: {len(all_authors)}å")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        if update_database and os.path.exists(self.db_path):
            print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é’ç©ºæ–‡åº«URLæ›´æ–°é–‹å§‹")
            self.update_database_urls(all_authors)
        
        return all_authors
    
    def update_database_urls(self, authors: List[AuthorInfo]) -> Dict[str, int]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®é’ç©ºæ–‡åº«URLã‚’æ›´æ–°"""
        try:
            # é’ç©ºæ–‡åº«ä½œè€…ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒ”ãƒ³ã‚°ä½œæˆ
            aozora_urls = {}
            for author in authors:
                if author.author_url:
                    aozora_urls[author.name] = author.author_url
            
            print(f"é’ç©ºæ–‡åº«ä½œè€…ãƒ‡ãƒ¼ã‚¿: {len(aozora_urls)}åã®URLæƒ…å ±")
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # é’ç©ºæ–‡åº«URLæœªè¨­å®šã®ä½œè€…ã‚’å–å¾—
            cursor.execute('''
                SELECT author_id, author_name 
                FROM authors 
                WHERE aozora_author_url IS NULL OR aozora_author_url = ''
                ORDER BY author_name
            ''')
            authors_without_url = cursor.fetchall()
            
            print(f"URLæœªè¨­å®šä½œè€…: {len(authors_without_url)}å")
            
            # ãƒãƒƒãƒãƒ³ã‚°ãƒ»æ›´æ–°å‡¦ç†
            updated_count = 0
            matched_count = 0
            
            for author_id, author_name in authors_without_url:
                # å®Œå…¨ä¸€è‡´ãƒã‚§ãƒƒã‚¯
                if author_name in aozora_urls:
                    url = aozora_urls[author_name]
                    cursor.execute(
                        'UPDATE authors SET aozora_author_url = ? WHERE author_id = ?',
                        (url, author_id)
                    )
                    updated_count += 1
                    matched_count += 1
                    print(f'âœ… å®Œå…¨ä¸€è‡´: {author_name} -> {url}')
                else:
                    # éƒ¨åˆ†ä¸€è‡´ãƒã‚§ãƒƒã‚¯ï¼ˆã‚¹ãƒšãƒ¼ã‚¹é™¤å»ãªã©ï¼‰
                    author_name_clean = author_name.replace(' ', '').replace('ã€€', '')
                    found = False
                    
                    for aozora_name, url in aozora_urls.items():
                        aozora_name_clean = aozora_name.replace(' ', '').replace('ã€€', '')
                        if author_name_clean == aozora_name_clean:
                            cursor.execute(
                                'UPDATE authors SET aozora_author_url = ? WHERE author_id = ?',
                                (url, author_id)
                            )
                            updated_count += 1
                            matched_count += 1
                            print(f'ğŸ”„ éƒ¨åˆ†ä¸€è‡´: {author_name} -> {aozora_name} -> {url}')
                            found = True
                            break
                    
                    if not found:
                        print(f'âŒ æœªç™ºè¦‹: {author_name}')
            
            # ã‚³ãƒŸãƒƒãƒˆ
            conn.commit()
            conn.close()
            
            print(f'\n=== ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°çµæœ ===')
            print(f'å‡¦ç†å¯¾è±¡: {len(authors_without_url)}å')
            print(f'ãƒãƒƒãƒãƒ³ã‚°æˆåŠŸ: {matched_count}å')
            print(f'DBæ›´æ–°: {updated_count}å')
            print(f'æœªç™ºè¦‹: {len(authors_without_url) - matched_count}å')
            print('âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é’ç©ºæ–‡åº«URLæ›´æ–°å®Œäº†')
            
            return {
                'processed': len(authors_without_url),
                'matched': matched_count,
                'updated': updated_count,
                'not_found': len(authors_without_url) - matched_count
            }
            
        except Exception as e:
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            return {'error': str(e)}
    
    def _fetch_authors_from_url(self, url: str, section: str) -> List[AuthorInfo]:
        """æŒ‡å®šURLã‹ã‚‰ä½œå®¶æƒ…å ±ã‚’å–å¾—"""
        self._wait_for_rate_limit()
        
        try:
            print(f"  ğŸ“– {section}ã‚»ã‚¯ã‚·ãƒ§ãƒ³å‡¦ç†ä¸­...")
            response = self.session.get(url)
            response.raise_for_status()
            
            # é’ç©ºæ–‡åº«ã®æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç†ã‚’æ”¹å–„
            # ã¾ãšShift_JISã§ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦ã‹ã‚‰UTF-8ã«å¤‰æ›
            if response.encoding is None or response.encoding.lower() in ['iso-8859-1', 'ascii']:
                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãƒã‚¤ãƒˆå†…å®¹ã‚’Shift_JISã¨ã—ã¦ãƒ‡ã‚³ãƒ¼ãƒ‰
                try:
                    content = response.content.decode('shift_jis')
                except UnicodeDecodeError:
                    # Shift_JISã§ãƒ‡ã‚³ãƒ¼ãƒ‰ã§ããªã„å ´åˆã¯EUC-JPã‚’è©¦ã™
                    try:
                        content = response.content.decode('euc-jp')
                    except UnicodeDecodeError:
                        # ãã‚Œã§ã‚‚ãƒ€ãƒ¡ãªå ´åˆã¯UTF-8ã‚’è©¦ã™
                        content = response.content.decode('utf-8', errors='ignore')
            else:
                content = response.text
            
            soup = BeautifulSoup(content, 'html.parser')
            authors = self._parse_author_list(soup, section)
            
            print(f"  âœ… {section}: {len(authors)}åå–å¾—")
            return authors
            
        except Exception as e:
            logger.error(f"âŒ {section}ã‚»ã‚¯ã‚·ãƒ§ãƒ³å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _parse_author_list(self, soup: BeautifulSoup, section: str) -> List[AuthorInfo]:
        """HTMLã‹ã‚‰ä½œå®¶ãƒªã‚¹ãƒˆã‚’è§£æ"""
        authors = []
        
        # ç›´æ¥olè¦ç´ å†…ã®liè¦ç´ ã‚’è§£æï¼ˆæ–‡å­—åŒ–ã‘ã‚’è€ƒæ…®ã—ã¦ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹ã§åˆ¤å®šï¼‰
        ol_elements = soup.find_all('ol')
        print(f"  ğŸ” æ¤œå‡ºã•ã‚ŒãŸolè¦ç´ æ•°: {len(ol_elements)}")
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³åã®ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆæ–‡å­—åŒ–ã‘å¯¾å¿œï¼‰
        section_mapping = {
            0: 'ã‚¢',  # æœ€åˆã®olãŒã€Œã‚¢ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³
            1: 'ã‚«',
            2: 'ã‚µ',
            3: 'ã‚¿',
            4: 'ãƒŠ',
            5: 'ãƒ',
            6: 'ãƒ',
            7: 'ãƒ¤',
            8: 'ãƒ©',
            9: 'ãƒ¯',
            10: 'ãã®ä»–'
        }
        
        for ol_index, ol in enumerate(ol_elements):
            li_elements = ol.find_all('li')
            current_section = section_mapping.get(ol_index, f"section_{ol_index+1}")
            
            print(f"    ol#{ol_index+1} ({current_section}): {len(li_elements)}å€‹ã®liè¦ç´ ")
            
            for li in li_elements:
                text = li.get_text().strip()
                # ä½œå®¶é …ç›®ã®åˆ¤å®šï¼šã€Œ(å…¬é–‹ä¸­ï¼šæ•°å­—)ã€ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å«ã‚€
                if 'å…¬é–‹ä¸­ï¼š' in text and ')' in text:
                    author_info = self._parse_author_item(li, current_section)
                    if author_info:
                        authors.append(author_info)
        
        return authors
    
    def _parse_author_item(self, li_element, section: str) -> Optional[AuthorInfo]:
        """å€‹åˆ¥ã®ä½œå®¶é …ç›®ã‚’è§£æ"""
        try:
            text = li_element.get_text()
            
            # åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼šã€Œä½œå®¶å (å…¬é–‹ä¸­ï¼šæ•°å­—)ã€
            # æ‹¡å¼µãƒ‘ã‚¿ãƒ¼ãƒ³ï¼šã€Œä½œå®¶å (å…¬é–‹ä¸­ï¼šæ•°å­—) ï¼Šè‘—ä½œæ¨©å­˜ç¶šï¼Šã€
            # åˆ¥åãƒ‘ã‚¿ãƒ¼ãƒ³ï¼šã€Œä½œå®¶å (å…¬é–‹ä¸­ï¼šæ•°å­—) (â†’åˆ¥å)ã€
            
            # ä½œå®¶åã¨ãƒªãƒ³ã‚¯ã‚’å–å¾—
            link = li_element.find('a')
            author_url = None
            if link and link.get('href'):
                href = link.get('href')
                # URLã‚’æ­£ã—ã„å½¢å¼ã«å¤‰æ›
                # ä¾‹: ../person/person74.html#sakuhin_list_1 -> https://www.aozora.gr.jp/index_pages/person74.html
                if 'person' in href:
                    # personç•ªå·ã‚’æŠ½å‡º
                    import re as url_re
                    person_match = url_re.search(r'person(\d+)\.html', href)
                    if person_match:
                        person_id = person_match.group(1)
                        author_url = f"https://www.aozora.gr.jp/index_pages/person{person_id}.html"
                    else:
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…ƒã®URLçµåˆ
                        author_url = urljoin(self.base_url, href)
                else:
                    author_url = urljoin(self.base_url, href)
            
            # ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
            # ãƒ‘ã‚¿ãƒ¼ãƒ³: åŸºæœ¬å½¢å¼
            match = re.match(r'(.+?)\s*\(å…¬é–‹ä¸­ï¼š(\d+)\)', text)
            if not match:
                return None
            
            author_name = match.group(1).strip()
            works_count = int(match.group(2))
            
            # è‘—ä½œæ¨©çŠ¶æ…‹ã‚’åˆ¤å®š
            copyright_status = 'expired'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            if 'ï¼Šè‘—ä½œæ¨©å­˜ç¶šï¼Š' in text:
                copyright_status = 'active'
            
            # åˆ¥åæƒ…å ±ã‚’æŠ½å‡º
            alias_info = None
            alias_match = re.search(r'\(â†’(.+?)\)', text)
            if alias_match:
                alias_info = alias_match.group(1).strip()
            
            # èª­ã¿ä»®åã‚’æ¨å®šï¼ˆç°¡æ˜“å®Ÿè£…ï¼‰
            name_reading = self._estimate_reading(author_name)
            
            return AuthorInfo(
                name=author_name,
                name_reading=name_reading,
                works_count=works_count,
                copyright_status=copyright_status,
                author_url=author_url,
                alias_info=alias_info,
                section=section
            )
            
        except Exception as e:
            logger.warning(f"ä½œå®¶é …ç›®è§£æã‚¨ãƒ©ãƒ¼: {li_element.get_text()[:50]} - {e}")
            return None
    
    def _estimate_reading(self, name: str) -> Optional[str]:
        """ä½œå®¶åã®èª­ã¿ä»®åã‚’æ¨å®šï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        # æœ‰åä½œå®¶ã®èª­ã¿ä»®åãƒãƒƒãƒ”ãƒ³ã‚°
        known_readings = {
            'å¤ç›® æ¼±çŸ³': 'ãªã¤ã‚ ãã†ã›ã',
            'èŠ¥å· ç«œä¹‹ä»‹': 'ã‚ããŸãŒã‚ ã‚Šã‚…ã†ã®ã™ã‘',
            'å¤ªå®° æ²»': 'ã ã–ã„ ãŠã•ã‚€',
            'å·ç«¯ åº·æˆ': 'ã‹ã‚ã°ãŸ ã‚„ã™ãªã‚Š',
            'ä¸‰å³¶ ç”±ç´€å¤«': 'ã¿ã—ã¾ ã‚†ããŠ',
            'æ£® é´å¤–': 'ã‚‚ã‚Š ãŠã†ãŒã„',
            'æ¨‹å£ ä¸€è‘‰': 'ã²ãã¡ ã„ã¡ã‚ˆã†',
            'å®®æ²¢ è³¢æ²»': 'ã¿ã‚„ã–ã‚ ã‘ã‚“ã˜',
            'è°·å´ æ½¤ä¸€éƒ': 'ãŸã«ã–ã ã˜ã‚…ã‚“ã„ã¡ã‚ã†',
            'å¿—è³€ ç›´å“‰': 'ã—ãŒ ãªãŠã‚„'
        }
        
        return known_readings.get(name)
    
    def save_authors_to_json(self, authors: List[AuthorInfo], file_path: str = "data/aozora_authors.json"):
        """ä½œå®¶æƒ…å ±ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        try:
            # AuthorInfoã‚’ãƒ‡ã‚£ã‚¯ã‚·ãƒ§ãƒŠãƒªã«å¤‰æ›
            authors_data = []
            for author in authors:
                authors_data.append({
                    'name': author.name,
                    'name_reading': author.name_reading,
                    'works_count': author.works_count,
                    'copyright_status': author.copyright_status,
                    'author_url': author.author_url,
                    'alias_info': author.alias_info,
                    'section': author.section,
                    'scraped_at': datetime.now().isoformat()
                })
            
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            
            # JSONä¿å­˜
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'authors': authors_data,
                    'total_count': len(authors_data),
                    'scraped_at': datetime.now().isoformat()
                }, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… ä½œå®¶æƒ…å ±ä¿å­˜å®Œäº†: {file_path} ({len(authors_data)}å)")
            
        except Exception as e:
            logger.error(f"âŒ ä½œå®¶æƒ…å ±ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def get_statistics(self, authors: List[AuthorInfo]) -> Dict[str, any]:
        """ä½œå®¶æƒ…å ±ã®çµ±è¨ˆã‚’å–å¾—"""
        total_authors = len(authors)
        total_works = sum(author.works_count for author in authors)
        
        # è‘—ä½œæ¨©çŠ¶æ…‹åˆ¥çµ±è¨ˆ
        copyright_active = len([a for a in authors if a.copyright_status == 'active'])
        copyright_expired = len([a for a in authors if a.copyright_status == 'expired'])
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥çµ±è¨ˆ
        section_stats = {}
        for author in authors:
            section = author.section
            if section not in section_stats:
                section_stats[section] = 0
            section_stats[section] += 1
        
        # ä½œå“æ•°ä¸Šä½ä½œå®¶
        top_authors = sorted(authors, key=lambda x: x.works_count, reverse=True)[:10]
        
        return {
            'total_authors': total_authors,
            'total_works': total_works,
            'copyright_active': copyright_active,
            'copyright_expired': copyright_expired,
            'section_stats': section_stats,
            'top_authors': [(a.name, a.works_count) for a in top_authors],
            'average_works_per_author': round(total_works / total_authors, 2) if total_authors > 0 else 0
        }
    
    def find_author_url(self, author_name: str) -> Optional[str]:
        """å˜ä¸€ä½œè€…ã®é’ç©ºæ–‡åº«URLã‚’æ¤œç´¢"""
        try:
            # JSONè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            json_path = "data/aozora_authors.json"
            if os.path.exists(json_path):
                with open(json_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ¤œç´¢
                for author in data.get('authors', []):
                    if author['name'] == author_name and author.get('author_url'):
                        return author['author_url']
                    
                    # éƒ¨åˆ†ä¸€è‡´ã‚‚è©¦è¡Œ
                    author_clean = author['name'].replace(' ', '').replace('ã€€', '')
                    name_clean = author_name.replace(' ', '').replace('ã€€', '')
                    if author_clean == name_clean and author.get('author_url'):
                        return author['author_url']
            
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã«ãªã‘ã‚Œã°Webã‹ã‚‰å–å¾—
            print(f"ğŸ” {author_name}ã®é’ç©ºæ–‡åº«URLæ¤œç´¢ä¸­...")
            authors = self.fetch_all_authors(update_database=False)
            
            for author in authors:
                if author.name == author_name and author.author_url:
                    return author.author_url
                    
                # éƒ¨åˆ†ä¸€è‡´ã‚‚è©¦è¡Œ
                author_clean = author.name.replace(' ', '').replace('ã€€', '')
                name_clean = author_name.replace(' ', '').replace('ã€€', '')
                if author_clean == name_clean and author.author_url:
                    return author.author_url
            
            return None
            
        except Exception as e:
            logger.error(f"âŒ ä½œè€…URLæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def update_single_author_url(self, author_name: str) -> bool:
        """å˜ä¸€ä½œè€…ã®é’ç©ºæ–‡åº«URLã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ›´æ–°"""
        try:
            # URLã‚’æ¤œç´¢
            author_url = self.find_author_url(author_name)
            
            if not author_url:
                print(f"âŒ {author_name}ã®é’ç©ºæ–‡åº«URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return False
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute(
                'UPDATE authors SET aozora_author_url = ? WHERE author_name = ?',
                (author_url, author_name)
            )
            
            if cursor.rowcount > 0:
                conn.commit()
                print(f"âœ… {author_name}ã®é’ç©ºæ–‡åº«URLæ›´æ–°: {author_url}")
                result = True
            else:
                print(f"âŒ {author_name}ãŒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                result = False
            
            conn.close()
            return result
            
        except Exception as e:
            logger.error(f"âŒ å˜ä¸€ä½œè€…URLæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            return False

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description='é’ç©ºæ–‡åº«ä½œå®¶ãƒªã‚¹ãƒˆå–å¾—ãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°')
    parser.add_argument('--update-db', action='store_true', 
                        help='ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®é’ç©ºæ–‡åº«URLã‚’è‡ªå‹•æ›´æ–°')
    parser.add_argument('--no-update-db', action='store_true', 
                        help='ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆJSONã®ã¿ä¿å­˜ï¼‰')
    parser.add_argument('--db-path', default='data/bungo_map.db', 
                        help='ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('--author', type=str, 
                        help='å˜ä¸€ä½œè€…ã®é’ç©ºæ–‡åº«URLæ¤œç´¢ãƒ»æ›´æ–°')
    
    args = parser.parse_args()
    
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼åˆæœŸåŒ–
    scraper = AuthorListScraper(rate_limit=1.0, db_path=args.db_path)
    
    # å˜ä¸€ä½œè€…ãƒ¢ãƒ¼ãƒ‰
    if args.author:
        print(f"ğŸ” å˜ä¸€ä½œè€…ãƒ¢ãƒ¼ãƒ‰: {args.author}")
        print("=" * 60)
        
        success = scraper.update_single_author_url(args.author)
        if success:
            print(f"âœ… {args.author}ã®é’ç©ºæ–‡åº«URLæ›´æ–°å®Œäº†")
        else:
            print(f"âŒ {args.author}ã®é’ç©ºæ–‡åº«URLæ›´æ–°å¤±æ•—")
        return
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°è¨­å®š
    update_database = True  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    if args.no_update_db:
        update_database = False
    elif args.update_db:
        update_database = True
    
    print("ğŸ—¾ é’ç©ºæ–‡åº«ä½œå®¶ãƒªã‚¹ãƒˆå–å¾—é–‹å§‹")
    print("=" * 60)
    
    if update_database:
        print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°: æœ‰åŠ¹")
    else:
        print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°: ç„¡åŠ¹ï¼ˆJSONã®ã¿ä¿å­˜ï¼‰")
    
    # å…¨ä½œå®¶æƒ…å ±å–å¾—
    authors = scraper.fetch_all_authors(update_database=update_database)
    
    if authors:
        # çµ±è¨ˆè¡¨ç¤º
        stats = scraper.get_statistics(authors)
        print(f"\nğŸ“Š å–å¾—çµ±è¨ˆ")
        print(f"ç·ä½œå®¶æ•°: {stats['total_authors']:,}å")
        print(f"ç·ä½œå“æ•°: {stats['total_works']:,}ä½œå“")
        print(f"è‘—ä½œæ¨©å­˜ç¶š: {stats['copyright_active']:,}å")
        print(f"è‘—ä½œæ¨©æº€äº†: {stats['copyright_expired']:,}å")
        print(f"å¹³å‡ä½œå“æ•°: {stats['average_works_per_author']}ä½œå“/ä½œå®¶")
        
        print(f"\nğŸ† ä½œå“æ•°ä¸Šä½10å:")
        for i, (name, count) in enumerate(stats['top_authors'], 1):
            print(f"  {i:2}. {name}: {count}ä½œå“")
        
        # JSONä¿å­˜
        scraper.save_authors_to_json(authors)
        
        print(f"\nâœ… ä½œå®¶ãƒªã‚¹ãƒˆå–å¾—å®Œäº†")
    else:
        print(f"âŒ ä½œå®¶æƒ…å ±å–å¾—å¤±æ•—")

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    main() 