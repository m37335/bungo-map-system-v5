#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºãƒ»å‡¦ç†ã®çµ±åˆã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³æ©Ÿèƒ½
é’ç©ºæ–‡åº«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã€ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã€åœ°åæŠ½å‡ºã®å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
"""

import asyncio
import logging
import time
from datetime import datetime
from typing import Dict, List, Any, Optional
import click
from pathlib import Path
import sys
import os

# ãƒ‘ã‚¹è¨­å®š
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from ..extractors.aozora_scraper import AozoraScraper
from ..extractors.text_processor import TextProcessor, ProcessingStats
from ..extractors.place_extractor import EnhancedPlaceExtractor
from ..database.manager import DatabaseManager
from ..core.cache import CacheManager
from ..quality.validator import QualityValidator

logger = logging.getLogger(__name__)

class DataProcessingPipeline:
    """ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, db_path: str = "data/db/bungo_v4.db"):
        """åˆæœŸåŒ–"""
        self.db_path = db_path
        self.db_manager = DatabaseManager()
        self.aozora_scraper = AozoraScraper(db_manager=self.db_manager)
        self.text_processor = TextProcessor()
        self.place_extractor = EnhancedPlaceExtractor()
        self.cache_manager = CacheManager()
        self.quality_validator = QualityValidator()
        
        # å¯¾è±¡ä½œå“ãƒªã‚¹ãƒˆï¼ˆç¢ºå®ŸãªURLä½¿ç”¨ï¼‰
        self.target_works = [
            {
                'author': 'å¤ç›®æ¼±çŸ³',
                'title': 'ã“ã“ã‚',
                'url': 'https://www.aozora.gr.jp/cards/000148/files/773_14560.html'
            },
            {
                'author': 'èŠ¥å·é¾ä¹‹ä»‹',
                'title': 'ç¾…ç”Ÿé–€',
                'url': 'https://www.aozora.gr.jp/cards/000879/files/127_15260.html'
            },
            {
                'author': 'å¤ªå®°æ²»',
                'title': 'èµ°ã‚Œãƒ¡ãƒ­ã‚¹',
                'url': 'https://www.aozora.gr.jp/cards/000035/files/1567_14913.html'
            },
            {
                'author': 'å®®æ²¢è³¢æ²»',
                'title': 'æ³¨æ–‡ã®å¤šã„æ–™ç†åº—',
                'url': 'https://www.aozora.gr.jp/cards/000081/files/43754_17659.html'
            },
            {
                'author': 'æ¨‹å£ä¸€è‘‰',
                'title': 'ãŸã‘ãã‚‰ã¹',
                'url': 'https://www.aozora.gr.jp/cards/000064/files/893_14763.html'
            }
        ]
        
        logger.info("ğŸ—¾ ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å®Œäº†")
    
    async def process_complete_flow(self, limit_sentences: int = 500) -> Dict[str, Any]:
        """å®Œå…¨ãƒ•ãƒ­ãƒ¼å‡¦ç†"""
        print(f"\nğŸš€ æ–‡è±ªã‚†ã‹ã‚Šåœ°å›³ã‚·ã‚¹ãƒ†ãƒ  - å®Œå…¨ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–‹å§‹")
        print("=" * 80)
        
        results = {
            'processed_works': [],
            'total_sentences': 0,
            'total_places': 0,
            'total_geocoded': 0,
            'errors': [],
            'start_time': datetime.now(),
            'end_time': None
        }
        
        try:
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
            await self._initialize_database()
            
            # å„ä½œå“ã‚’é †æ¬¡å‡¦ç†
            for i, work_info in enumerate(self.target_works, 1):
                print(f"\nğŸ“– {i}/{len(self.target_works)}: {work_info['author']} - {work_info['title']}")
                print("-" * 60)
                
                try:
                    work_result = await self._process_single_work(work_info, limit_sentences)
                    if work_result:
                        results['processed_works'].append(work_result)
                        results['total_sentences'] += work_result.get('sentences_count', 0)
                        results['total_places'] += work_result.get('places_count', 0)
                        results['total_geocoded'] += work_result.get('geocoded_count', 0)
                    else:
                        results['errors'].append(f"{work_info['author']} - {work_info['title']}")
                
                except Exception as e:
                    logger.error(f"âŒ ä½œå“å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    results['errors'].append(f"{work_info['author']} - {work_info['title']}: {str(e)}")
                
                time.sleep(1)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            
            results['end_time'] = datetime.now()
            
            # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            self._generate_final_report(results)
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ å®Œå…¨ãƒ•ãƒ­ãƒ¼å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            results['errors'].append(f"ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return results
    
    async def _initialize_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–"""
        print("ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ä¸­...")
        try:
            await self.db_manager.initialize_database()
            print("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–å¤±æ•—: {e}")
            raise
    
    async def _process_single_work(self, work_info: Dict[str, str], limit_sentences: int) -> Optional[Dict[str, Any]]:
        """å˜ä¸€ä½œå“ã®å®Œå…¨å‡¦ç†"""
        
        # 1. ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
        print(f"  ğŸ“„ ãƒ†ã‚­ã‚¹ãƒˆå–å¾—ä¸­...")
        raw_content = self.aozora_scraper.fetch_work_content(work_info['url'])
        if not raw_content:
            print(f"  âŒ ãƒ†ã‚­ã‚¹ãƒˆå–å¾—å¤±æ•—")
            return None
        
        print(f"  ğŸ“„ å–å¾—å®Œäº†: {len(raw_content):,}æ–‡å­—")
        
        # 2. ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†
        print(f"  ğŸ§¹ ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ...")
        cleaned_content, processing_stats = self.text_processor.clean_aozora_text(raw_content)
        
        # 3. å“è³ªæ¤œè¨¼
        quality_result = self.text_processor.validate_text_quality(cleaned_content)
        if not quality_result['is_valid']:
            print(f"  âš ï¸ å“è³ªæ¤œè¨¼è­¦å‘Š: {quality_result['errors']}")
        
        # 4. æ–‡åˆ†å‰²
        sentences = self.text_processor.split_into_sentences(cleaned_content)
        print(f"  ğŸ“ æ–‡åˆ†å‰²å®Œäº†: {len(sentences)}æ–‡")
        
        # 5. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ ¼ç´
        print(f"  ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ ¼ç´...")
        work_id = await self._store_work_in_database(work_info, cleaned_content, sentences)
        if not work_id:
            print(f"  âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ ¼ç´å¤±æ•—")
            return None
        
        print(f"  âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ ¼ç´å®Œäº†: work_id={work_id}")
        
        # 6. åœ°åæŠ½å‡ºï¼ˆåˆ¶é™ä»˜ãï¼‰
        print(f"  ğŸ—ºï¸ åœ°åæŠ½å‡ºå®Ÿè¡Œï¼ˆæœ€å¤§{limit_sentences}æ–‡ï¼‰...")
        limited_sentences = sentences[:limit_sentences]
        places_count = await self._extract_places_for_work(work_id, work_info, limited_sentences)
        print(f"  ğŸ—ºï¸ åœ°åæŠ½å‡ºå®Œäº†: {places_count}ä»¶")
        
        # 7. ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        print(f"  ğŸŒ ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Ÿè¡Œ...")
        geocoded_count = await self._geocode_places_for_work(work_id)
        print(f"  ğŸŒ ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œäº†: {geocoded_count}ä»¶")
        
        return {
            'work_id': work_id,
            'author': work_info['author'],
            'title': work_info['title'],
            'processing_stats': processing_stats,
            'quality_result': quality_result,
            'sentences_count': len(sentences),
            'places_count': places_count,
            'geocoded_count': geocoded_count
        }
    
    async def _store_work_in_database(self, work_info: Dict[str, str], content: str, sentences: List[str]) -> Optional[int]:
        """ä½œå“ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ ¼ç´"""
        try:
            # ä½œè€…ã®å–å¾—ã¾ãŸã¯ä½œæˆ
            author_data = {
                'author_name': work_info['author'],
                'source_system': 'v4.0',
                'verification_status': 'pending'
            }
            author = await self.db_manager.get_or_create_author(author_data)
            
            # ä½œå“ã®ä½œæˆ
            work_data = {
                'work_title': work_info['title'],
                'author_id': author.author_id,
                'aozora_url': work_info['url'],
                'content_length': len(content),
                'sentence_count': len(sentences),
                'source_system': 'v4.0',
                'processing_status': 'processing'
            }
            work = await self.db_manager.create_work(work_data)
            
            # ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ã®ä¸€æ‹¬ä½œæˆ
            sentence_data_list = []
            for i, sentence_text in enumerate(sentences):
                sentence_data = {
                    'sentence_text': sentence_text,
                    'work_id': work.work_id,
                    'author_id': author.author_id,
                    'sentence_length': len(sentence_text),
                    'position_in_work': i
                }
                sentence_data_list.append(sentence_data)
            
            await self.db_manager.create_sentences_batch(sentence_data_list)
            
            return work.work_id
            
        except Exception as e:
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ ¼ç´ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    async def _extract_places_for_work(self, work_id: int, work_info: Dict[str, str], sentences: List[str]) -> int:
        """ä½œå“ã®åœ°åæŠ½å‡º"""
        places_count = 0
        
        try:
            # å…¨æ–‡ã‚’ã¾ã¨ã‚ã¦å‡¦ç†ï¼ˆåŠ¹ç‡åŒ–ï¼‰
            full_text = ' '.join(sentences)
            
            # åœ°åæŠ½å‡ºå®Ÿè¡Œ
            extracted_places = await self.place_extractor.extract_places(
                full_text, 
                work_info['title'], 
                work_info['author']
            )
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ ¼ç´
            for place in extracted_places:
                try:
                    # åœ°åã®å–å¾—ã¾ãŸã¯ä½œæˆ
                    place_data = {
                        'place_name': place.name,
                        'canonical_name': place.name,
                        'place_type': self._map_place_category(place.category),
                        'confidence': place.confidence,
                        'source_system': 'v4.0',
                        'verification_status': 'auto'
                    }
                    place_obj = await self.db_manager.get_or_create_place(place_data)
                    
                    # ã‚»ãƒ³ãƒ†ãƒ³ã‚¹-åœ°åé–¢é€£ã®ä½œæˆ
                    # ç°¡ç•¥åŒ–: æœ€åˆã®æ–‡ã«é–¢é€£ä»˜ã‘
                    if sentences:
                        sentence_obj = await self.db_manager.get_sentence_by_text_and_work(
                            sentences[0], work_id
                        )
                        if sentence_obj:
                            relation_data = {
                                'sentence_id': sentence_obj.sentence_id,
                                'place_id': place_obj.place_id,
                                'extraction_method': place.source,
                                'confidence': place.confidence,
                                'matched_text': place.matched_text,
                                'verification_status': 'auto'
                            }
                            await self.db_manager.create_sentence_place(relation_data)
                            places_count += 1
                
                except Exception as e:
                    logger.warning(f"åœ°åæ ¼ç´ã‚¨ãƒ©ãƒ¼: {place.name} - {e}")
            
        except Exception as e:
            logger.error(f"âŒ åœ°åæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return places_count
    
    async def _geocode_places_for_work(self, work_id: int) -> int:
        """ä½œå“ã®åœ°åã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"""
        # åŸºæœ¬å®Ÿè£…ï¼ˆã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        try:
            geocoded_count = 0
            places = await self.db_manager.get_places_by_work(work_id)
            
            for place in places:
                if not place.latitude or not place.longitude:
                    # åŸºæœ¬çš„ãªã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆä»®å®Ÿè£…ï¼‰
                    # å®Ÿéš›ã®ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹ãŒå¿…è¦
                    # geocoded_data = await self.geocoding_service.geocode(place.place_name)
                    # if geocoded_data:
                    #     await self.db_manager.update_place_coordinates(
                    #         place.place_id, geocoded_data['lat'], geocoded_data['lng']
                    #     )
                    #     geocoded_count += 1
                    pass
            
            return geocoded_count
            
        except Exception as e:
            logger.error(f"âŒ ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
            return 0
    
    def _map_place_category(self, category: str) -> str:
        """åœ°åã‚«ãƒ†ã‚´ãƒªã®ãƒãƒƒãƒ”ãƒ³ã‚°"""
        mapping = {
            'å®Œå…¨åœ°å': 'å¸‚åŒºç”ºæ‘',
            'éƒ½é“åºœçœŒ': 'éƒ½é“åºœçœŒ',
            'å¸‚åŒºç”ºæ‘': 'å¸‚åŒºç”ºæ‘',
            'æœ‰ååœ°å': 'æœ‰ååœ°å',
            'è‡ªç„¶åœ°å': 'æœ‰ååœ°å',
            'æ­´å²åœ°å': 'æ­´å²åœ°å',
            'nlp_entity': 'æœ‰ååœ°å'
        }
        return mapping.get(category, 'æœ‰ååœ°å')
    
    def _generate_final_report(self, results: Dict[str, Any]):
        """æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print(f"\nğŸ“Š å‡¦ç†å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ")
        print("=" * 80)
        
        duration = results['end_time'] - results['start_time'] if results['end_time'] else None
        
        print(f"â±ï¸  å‡¦ç†æ™‚é–“: {duration}")
        print(f"ğŸ“– å‡¦ç†ä½œå“æ•°: {len(results['processed_works'])}/{len(self.target_works)}")
        print(f"ğŸ“ ç·ã‚»ãƒ³ãƒ†ãƒ³ã‚¹æ•°: {results['total_sentences']:,}")
        print(f"ğŸ—ºï¸ ç·åœ°åæ•°: {results['total_places']:,}")
        print(f"ğŸŒ ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ•°: {results['total_geocoded']:,}")
        
        if results['errors']:
            print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ ({len(results['errors'])}ä»¶):")
            for error in results['errors']:
                print(f"  - {error}")
        
        print(f"\nâœ… ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†")
        print("=" * 80)

# CLIã‚³ãƒãƒ³ãƒ‰å®šç¾©
@click.group()
def cli():
    """æ–‡è±ªã‚†ã‹ã‚Šåœ°å›³ã‚·ã‚¹ãƒ†ãƒ  ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚³ãƒãƒ³ãƒ‰"""
    pass

@cli.command()
@click.option('--limit-sentences', default=500, help='å‡¦ç†ã™ã‚‹æœ€å¤§ã‚»ãƒ³ãƒ†ãƒ³ã‚¹æ•°')
@click.option('--db-path', default='data/db/bungo_v4.db', help='ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹')
def process_all(limit_sentences: int, db_path: str):
    """å…¨ä½œå“ã®å®Œå…¨å‡¦ç†"""
    pipeline = DataProcessingPipeline(db_path)
    
    async def run():
        await pipeline.process_complete_flow(limit_sentences)
    
    asyncio.run(run())

@cli.command()
@click.argument('url')
@click.option('--author', required=True, help='ä½œè€…å')
@click.option('--title', required=True, help='ä½œå“å')
@click.option('--db-path', default='data/db/bungo_v4.db', help='ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹')
def process_single(url: str, author: str, title: str, db_path: str):
    """å˜ä¸€ä½œå“ã®å‡¦ç†"""
    pipeline = DataProcessingPipeline(db_path)
    work_info = {'url': url, 'author': author, 'title': title}
    
    async def run():
        result = await pipeline._process_single_work(work_info, 500)
        if result:
            print(f"âœ… å‡¦ç†å®Œäº†: {result}")
        else:
            print("âŒ å‡¦ç†å¤±æ•—")
    
    asyncio.run(run())

@cli.command()
@click.argument('text')
def test_extraction(text: str):
    """åœ°åæŠ½å‡ºãƒ†ã‚¹ãƒˆ"""
    async def run():
        extractor = EnhancedPlaceExtractor()
        places = await extractor.extract_places(text)
        
        print(f"ğŸ—ºï¸ æŠ½å‡ºçµæœ: {len(places)}ä»¶")
        for place in places:
            print(f"  - {place.name} (ä¿¡é ¼åº¦: {place.confidence:.2f}, ã‚½ãƒ¼ã‚¹: {place.source})")
    
    asyncio.run(run())

@cli.command()
@click.option('--force-refresh', is_flag=True, help='å¼·åˆ¶çš„ã«å…¨ä½œå®¶æƒ…å ±ã‚’æ›´æ–°')
@click.option('--section', help='ç‰¹å®šã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ã¿åŒæœŸï¼ˆã‚¢ã€ã‚«ã€ã‚µç­‰ï¼‰')
def sync_authors(force_refresh: bool, section: Optional[str]):
    """é’ç©ºæ–‡åº«ä½œå®¶ãƒªã‚¹ãƒˆã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«åŒæœŸ"""
    import asyncio
    from ..extractors.author_database_service import AuthorDatabaseService
    
    print("ğŸ”„ ä½œå®¶ãƒªã‚¹ãƒˆåŒæœŸé–‹å§‹")
    
    async def run_sync():
        service = AuthorDatabaseService()
        
        if section:
            print(f"ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ '{section}' ã®ä½œå®¶ã‚’åŒæœŸä¸­...")
            # ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥åŒæœŸã¯å°†æ¥ã®æ‹¡å¼µã¨ã—ã¦å®Ÿè£…
            print("âš ï¸ ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥åŒæœŸã¯ç¾åœ¨æœªå¯¾å¿œã§ã™ã€‚å…¨ä½“åŒæœŸã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
        
        result = await service.sync_all_authors(force_refresh=force_refresh)
        
        if result['success']:
            print("\nâœ… ä½œå®¶ãƒªã‚¹ãƒˆåŒæœŸå®Œäº†")
        else:
            print(f"\nâŒ ä½œå®¶ãƒªã‚¹ãƒˆåŒæœŸå¤±æ•—: {result.get('error', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')}")
            exit(1)
    
    asyncio.run(run_sync())

@cli.command()
@click.argument('search_term')
@click.option('--limit', default=20, help='æ¤œç´¢çµæœã®æœ€å¤§ä»¶æ•°')
@click.option('--section', help='æ¤œç´¢å¯¾è±¡ã‚»ã‚¯ã‚·ãƒ§ãƒ³')
def search_authors(search_term: str, limit: int, section: Optional[str]):
    """ä½œå®¶åæ¤œç´¢"""
    import asyncio
    from ..extractors.author_database_service import AuthorDatabaseService
    
    async def run_search():
        service = AuthorDatabaseService()
        
        if section:
            print(f"ğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ '{section}' ã§ '{search_term}' ã‚’æ¤œç´¢...")
            authors = await service.get_authors_by_section(section)
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            filtered_authors = [a for a in authors if search_term in a['name']][:limit]
        else:
            print(f"ğŸ“š '{search_term}' ã‚’æ¤œç´¢...")
            filtered_authors = await service.search_authors(search_term, limit)
        
        if filtered_authors:
            print(f"\nğŸ“Š æ¤œç´¢çµæœ: {len(filtered_authors)}å")
            print("=" * 80)
            
            for i, author in enumerate(filtered_authors, 1):
                name = author['name']
                kana = author.get('name_kana', '')
                works_count = author.get('works_count', 0)
                copyright_status = author.get('copyright_status', 'unknown')
                section = author.get('section', '')
                
                status_icon = "âš–ï¸" if copyright_status == "active" else "ğŸ“š"
                kana_text = f" ({kana})" if kana else ""
                
                print(f"{i:3}. {status_icon} {name}{kana_text}")
                print(f"     ä½œå“æ•°: {works_count}, ã‚»ã‚¯ã‚·ãƒ§ãƒ³: {section}")
                
                if author.get('aozora_url'):
                    print(f"     URL: {author['aozora_url']}")
                print()
        else:
            print(f"âŒ '{search_term}' ã«è©²å½“ã™ã‚‹ä½œå®¶ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    asyncio.run(run_search())

@cli.command()
@click.option('--section', help='ç‰¹å®šã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®çµ±è¨ˆ')
@click.option('--copyright', help='è‘—ä½œæ¨©çŠ¶æ…‹åˆ¥çµ±è¨ˆ')
def author_stats(section: Optional[str], copyright: Optional[str]):
    """ä½œå®¶çµ±è¨ˆè¡¨ç¤º"""
    import asyncio
    from ..extractors.author_database_service import AuthorDatabaseService
    
    async def run_stats():
        service = AuthorDatabaseService()
        stats = await service.get_author_statistics()
        
        if not stats:
            print("âŒ çµ±è¨ˆæƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        print("ğŸ“Š é’ç©ºæ–‡åº«ä½œå®¶çµ±è¨ˆ")
        print("=" * 60)
        
        # åŸºæœ¬çµ±è¨ˆ
        print(f"ç·ä½œå®¶æ•°: {stats.get('total_authors', 0):,}å")
        print(f"ç·ä½œå“æ•°: {stats.get('total_works', 0):,}ä½œå“")
        print(f"å¹³å‡ä½œå“æ•°: {stats.get('average_works_per_author', 0)}ä½œå“/ä½œå®¶")
        print(f"æœ€çµ‚åŒæœŸ: {stats.get('last_sync', 'N/A')}")
        
        # è‘—ä½œæ¨©çŠ¶æ…‹åˆ¥çµ±è¨ˆ
        copyright_stats = stats.get('copyright_stats', {})
        if copyright_stats:
            print(f"\nâš–ï¸ è‘—ä½œæ¨©çŠ¶æ…‹åˆ¥çµ±è¨ˆ:")
            for status, count in copyright_stats.items():
                status_name = {
                    'expired': 'è‘—ä½œæ¨©æº€äº†',
                    'active': 'è‘—ä½œæ¨©å­˜ç¶š',
                    'unknown': 'ä¸æ˜'
                }.get(status, status)
                print(f"  {status_name}: {count:,}å")
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥çµ±è¨ˆ
        section_stats = stats.get('section_stats', {})
        if section_stats and not section:
            print(f"\nğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥çµ±è¨ˆ:")
            for sec, count in sorted(section_stats.items()):
                print(f"  {sec}: {count:,}å")
        elif section and section in section_stats:
            print(f"\nğŸ“š ã‚»ã‚¯ã‚·ãƒ§ãƒ³ '{section}' çµ±è¨ˆ:")
            print(f"  ä½œå®¶æ•°: {section_stats[section]:,}å")
        
        # ä½œå“æ•°ä¸Šä½ä½œå®¶
        top_authors = stats.get('top_authors', [])
        if top_authors:
            print(f"\nğŸ† ä½œå“æ•°ä¸Šä½10å:")
            for i, (name, count) in enumerate(top_authors, 1):
                print(f"  {i:2}. {name}: {count}ä½œå“")
    
    asyncio.run(run_stats())

@cli.command()
def list_sections():
    """åˆ©ç”¨å¯èƒ½ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³ä¸€è¦§"""
    sections = {
        'ã‚¢': 'ã‚è¡Œï¼ˆã‚¢ã€œã‚ªï¼‰',
        'ã‚«': 'ã‹è¡Œï¼ˆã‚«ã€œã‚³ï¼‰',
        'ã‚µ': 'ã•è¡Œï¼ˆã‚µã€œã‚½ï¼‰',
        'ã‚¿': 'ãŸè¡Œï¼ˆã‚¿ã€œãƒˆï¼‰',
        'ãƒŠ': 'ãªè¡Œï¼ˆãƒŠã€œãƒï¼‰',
        'ãƒ': 'ã¯è¡Œï¼ˆãƒã€œãƒ›ï¼‰',
        'ãƒ': 'ã¾è¡Œï¼ˆãƒã€œãƒ¢ï¼‰',
        'ãƒ¤': 'ã‚„è¡Œï¼ˆãƒ¤ã€œãƒ¨ï¼‰',
        'ãƒ©': 'ã‚‰è¡Œï¼ˆãƒ©ã€œãƒ­ï¼‰',
        'ãƒ¯': 'ã‚è¡Œï¼ˆãƒ¯ã€œãƒ³ï¼‰',
        'ãã®ä»–': 'å¤–å›½äººä½œå®¶ç­‰'
    }
    
    print("ğŸ“š é’ç©ºæ–‡åº«ä½œå®¶ã‚»ã‚¯ã‚·ãƒ§ãƒ³ä¸€è¦§")
    print("=" * 40)
    
    for section, description in sections.items():
        print(f"  {section}: {description}")
    
    print(f"\nğŸ’¡ ä½¿ç”¨ä¾‹:")
    print(f"  bungo-map sync-authors --section ã‚¢")
    print(f"  bungo-map search-authors å¤ç›® --section ãƒŠ")
    print(f"  bungo-map author-stats --section ã‚¢")

if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO)
    cli()
