#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½œå®¶ãƒªã‚¹ãƒˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼å‹•ä½œãƒ†ã‚¹ãƒˆ
é’ç©ºæ–‡åº«ä½œå®¶ãƒªã‚¹ãƒˆå–å¾—ã¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ ¼ç´ã®ãƒ†ã‚¹ãƒˆ
"""

import asyncio
import logging
import sys
import os
from datetime import datetime

# ãƒ‘ã‚¹è¿½åŠ 
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# çµ¶å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«ä¿®æ­£
try:
    from extractors.author_list_scraper import AuthorListScraper, AuthorInfo
    from extractors.author_database_service import AuthorDatabaseService
    from database.manager import DatabaseManager
    from core.config import get_config
except ImportError:
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç›´æ¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    import importlib.util
    
    # AuthorListScraperã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    spec = importlib.util.spec_from_file_location(
        "author_list_scraper", 
        "extractors/author_list_scraper.py"
    )
    author_scraper_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(author_scraper_module)
    AuthorListScraper = author_scraper_module.AuthorListScraper
    AuthorInfo = author_scraper_module.AuthorInfo

def test_scraper_basic():
    """åŸºæœ¬çš„ãªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼å‹•ä½œãƒ†ã‚¹ãƒˆ"""
    print("ğŸ§ª ä½œå®¶ãƒªã‚¹ãƒˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼åŸºæœ¬ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    try:
        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼åˆæœŸåŒ–
        scraper = AuthorListScraper(rate_limit=2.0)  # ãƒ†ã‚¹ãƒˆç”¨ã«ä½é€Ÿè¨­å®š
        
        # å°‘æ•°ä½œå®¶æƒ…å ±å–å¾—ãƒ†ã‚¹ãƒˆï¼ˆå®Ÿéš›ã«ã‚µã‚¤ãƒˆã«ã‚¢ã‚¯ã‚»ã‚¹ï¼‰
        print("ğŸ“š ä½œå®¶æƒ…å ±å–å¾—ä¸­ï¼ˆãƒ†ã‚¹ãƒˆï¼‰...")
        print("âš ï¸ é’ç©ºæ–‡åº«ã‚µã‚¤ãƒˆã‹ã‚‰å®Ÿéš›ã«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã™...")
        
        authors = scraper.fetch_all_authors()
        
        if authors:
            print(f"âœ… ä½œå®¶æƒ…å ±å–å¾—æˆåŠŸ: {len(authors)}å")
            
            # çµ±è¨ˆè¡¨ç¤º
            stats = scraper.get_statistics(authors)
            print(f"\nğŸ“Š å–å¾—çµ±è¨ˆ:")
            print(f"  ç·ä½œå®¶æ•°: {stats['total_authors']:,}å")
            print(f"  ç·ä½œå“æ•°: {stats['total_works']:,}ä½œå“")
            print(f"  å¹³å‡ä½œå“æ•°: {stats['average_works_per_author']}ä½œå“/ä½œå®¶")
            print(f"  è‘—ä½œæ¨©å­˜ç¶š: {stats['copyright_active']:,}å")
            print(f"  è‘—ä½œæ¨©æº€äº†: {stats['copyright_expired']:,}å")
            
            # ã‚µãƒ³ãƒ—ãƒ«ä½œå®¶è¡¨ç¤º
            print(f"\nğŸ” ä½œå®¶ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€åˆã®5åï¼‰:")
            for i, author in enumerate(authors[:5], 1):
                status_icon = "âš–ï¸" if author.copyright_status == "active" else "ğŸ“š"
                print(f"  {i}. {status_icon} {author.name} (ä½œå“æ•°: {author.works_count})")
                if author.author_url:
                    print(f"     URL: {author.author_url}")
                if author.alias_info:
                    print(f"     åˆ¥å: {author.alias_info}")
            
            # ä½œå“æ•°ä¸Šä½ä½œå®¶
            print(f"\nğŸ† ä½œå“æ•°ä¸Šä½5å:")
            for i, (name, count) in enumerate(stats['top_authors'][:5], 1):
                print(f"  {i}. {name}: {count}ä½œå“")
            
            # JSONä¿å­˜ãƒ†ã‚¹ãƒˆ
            print(f"\nğŸ’¾ JSONä¿å­˜ãƒ†ã‚¹ãƒˆ...")
            scraper.save_authors_to_json(authors)
            
            return True
        else:
            print("âŒ ä½œå®¶æƒ…å ±å–å¾—å¤±æ•—")
            return False
            
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_database_integration():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ§ª ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    try:
        # ãƒ†ã‚¹ãƒˆç”¨è¨­å®š
        config = get_config()
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {config.database.database_url}")
        
        # ã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–
        service = AuthorDatabaseService()
        
        # å°‘æ•°ãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆ
        print("ğŸ“š ä½œå®¶ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åŒæœŸãƒ†ã‚¹ãƒˆ...")
        
        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼å˜ä½“ãƒ†ã‚¹ãƒˆ
        scraper = AuthorListScraper(rate_limit=2.0)
        authors_info = scraper.fetch_all_authors()
        
        if not authors_info:
            print("âŒ ä½œå®¶æƒ…å ±å–å¾—å¤±æ•—")
            return False
        
        # æœ€åˆã®10åã®ã¿ã§ãƒ†ã‚¹ãƒˆ
        test_authors = authors_info[:10]
        print(f"ğŸ”¬ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_authors)}å")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åŒæœŸãƒ†ã‚¹ãƒˆ
        sync_result = await service._sync_authors_to_database(test_authors, force_refresh=True)
        
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åŒæœŸå®Œäº†:")
        print(f"  æ–°è¦ä½œå®¶: {sync_result['new_count']}å")
        print(f"  æ›´æ–°ä½œå®¶: {sync_result['updated_count']}å")
        
        # æ¤œç´¢ãƒ†ã‚¹ãƒˆ
        print(f"\nğŸ” æ¤œç´¢ãƒ†ã‚¹ãƒˆ...")
        if test_authors:
            search_name = test_authors[0].name.split()[0]  # å§“ã§æ¤œç´¢
            search_results = await service.search_authors(search_name, limit=5)
            
            print(f"'{search_name}' ã®æ¤œç´¢çµæœ: {len(search_results)}å")
            for result in search_results:
                print(f"  - {result['name']} (ä½œå“æ•°: {result['works_count']})")
        
        # çµ±è¨ˆãƒ†ã‚¹ãƒˆ
        print(f"\nğŸ“Š çµ±è¨ˆãƒ†ã‚¹ãƒˆ...")
        stats = await service.get_author_statistics()
        if stats:
            print(f"  ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…ä½œå®¶æ•°: {stats.get('total_authors', 0)}å")
            print(f"  ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…ä½œå“æ•°: {stats.get('total_works', 0)}ä½œå“")
        
        return True
        
    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_author_parsing():
    """ä½œå®¶é …ç›®è§£æãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ§ª ä½œå®¶é …ç›®è§£æãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    test_cases = [
        "èŠ¥å· ç«œä¹‹ä»‹ (å…¬é–‹ä¸­ï¼š379)",
        "é’æœ¨ æ „ç³ (å…¬é–‹ä¸­ï¼š1) ï¼Šè‘—ä½œæ¨©å­˜ç¶šï¼Š",
        "èŠ¥å· ç´—ç¹” (å…¬é–‹ä¸­ï¼š5) (â†’é–“æ‰€ ç´—ç¹”)",
        "ã‚¢ãƒ¼ãƒ´ã‚£ãƒ³ã‚° ãƒ¯ã‚·ãƒ³ãƒˆãƒ³ (å…¬é–‹ä¸­ï¼š16)",
        "ãƒ¦ã‚´ãƒ¼ ãƒ´ã‚£ã‚¯ãƒˆãƒ« (å…¬é–‹ä¸­ï¼š7)"
    ]
    
    scraper = AuthorListScraper()
    
    print("ğŸ” è§£æãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹:")
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n{i}. ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹: '{test_case}'")
        
        # ç°¡æ˜“HTMLè¦ç´ ä½œæˆ
        from bs4 import BeautifulSoup
        html = f"<li>{test_case}</li>"
        soup = BeautifulSoup(html, 'html.parser')
        li_element = soup.find('li')
        
        # è§£æå®Ÿè¡Œ
        author_info = scraper._parse_author_item(li_element, "ãƒ†ã‚¹ãƒˆ")
        
        if author_info:
            print(f"   âœ… è§£ææˆåŠŸ:")
            print(f"      åå‰: {author_info.name}")
            print(f"      ä½œå“æ•°: {author_info.works_count}")
            print(f"      è‘—ä½œæ¨©: {author_info.copyright_status}")
            if author_info.alias_info:
                print(f"      åˆ¥å: {author_info.alias_info}")
        else:
            print(f"   âŒ è§£æå¤±æ•—")
    
    return True

def test_simple_scraping():
    """ç°¡æ˜“ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ†ã‚¹ãƒˆï¼ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šç¢ºèªï¼‰"""
    print("\nğŸ§ª ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    try:
        import requests
        
        # é’ç©ºæ–‡åº«ã®åŸºæœ¬æ¥ç¶šãƒ†ã‚¹ãƒˆ
        url = "https://www.aozora.gr.jp/index_pages/person_all.html"
        print(f"ğŸ“¡ æ¥ç¶šãƒ†ã‚¹ãƒˆ: {url}")
        
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        
        print(f"âœ… æ¥ç¶šæˆåŠŸ:")
        print(f"  ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}")
        print(f"  ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚µã‚¤ã‚º: {len(response.content):,} ãƒã‚¤ãƒˆ")
        print(f"  ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {response.encoding}")
        
        # HTMLãƒ‘ãƒ¼ã‚¹ç°¡æ˜“ãƒ†ã‚¹ãƒˆ
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ä½œå®¶æ•°ã®æ¦‚ç®—
        ol_elements = soup.find_all('ol')
        li_count = sum(len(ol.find_all('li')) for ol in ol_elements)
        
        print(f"  æ¤œå‡ºã•ã‚ŒãŸé …ç›®æ•°: {li_count}é …ç›®")
        
        return True
        
    except Exception as e:
        print(f"âŒ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False

def main():
    """ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    print("ğŸ—¾ é’ç©ºæ–‡åº«ä½œå®¶ãƒªã‚¹ãƒˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ç·åˆãƒ†ã‚¹ãƒˆ")
    print("=" * 80)
    print(f"ãƒ†ã‚¹ãƒˆé–‹å§‹æ™‚åˆ»: {datetime.now()}")
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    test_results = []
    
    # 1. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šãƒ†ã‚¹ãƒˆ
    test_results.append(("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶š", test_simple_scraping()))
    
    # 2. è§£æãƒ­ã‚¸ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
    test_results.append(("é …ç›®è§£æ", test_author_parsing()))
    
    # 3. åŸºæœ¬ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ãƒ†ã‚¹ãƒˆï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼‰
    print(f"\nâš ï¸ å®Ÿéš›ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™ã‹ï¼Ÿ")
    print(f"   ã“ã®ãƒ†ã‚¹ãƒˆã¯é’ç©ºæ–‡åº«ã‚µã‚¤ãƒˆã«å®Ÿéš›ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¾ã™ã€‚")
    response = input("å®Ÿè¡Œã™ã‚‹å ´åˆã¯ 'y' ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ")
    
    if response.lower() == 'y':
        test_results.append(("åŸºæœ¬ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼", test_scraper_basic()))
    else:
        print("â­ï¸ å®Ÿéš›ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ†ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")
        test_results.append(("åŸºæœ¬ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼", True))  # ã‚¹ã‚­ãƒƒãƒ—ã¨ã—ã¦æˆåŠŸæ‰±ã„
    
    # çµæœãƒ¬ãƒãƒ¼ãƒˆ
    print(f"\n" + "=" * 80)
    print("ğŸ ãƒ†ã‚¹ãƒˆçµæœãƒ¬ãƒãƒ¼ãƒˆ")
    print("=" * 80)
    
    success_count = 0
    for test_name, result in test_results:
        status = "âœ… æˆåŠŸ" if result else "âŒ å¤±æ•—"
        print(f"{test_name}: {status}")
        if result:
            success_count += 1
    
    print(f"\nğŸ“Š ç·åˆçµæœ: {success_count}/{len(test_results)} ãƒ†ã‚¹ãƒˆæˆåŠŸ")
    
    if success_count == len(test_results):
        print("ğŸ‰ å…¨ãƒ†ã‚¹ãƒˆæˆåŠŸï¼ä½œå®¶ãƒªã‚¹ãƒˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã¯æ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™ã€‚")
        return 0
    else:
        print("âš ï¸ ä¸€éƒ¨ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return 1

if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    exit_code = main()
    sys.exit(exit_code) 